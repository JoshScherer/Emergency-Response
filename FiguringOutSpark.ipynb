{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODZQCRtRhVSwBs95Y3ET73",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshScherer/Emergency-Response/blob/josh-dev/FiguringOutSpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kCEi2D06C0u_"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install spark. we are using the one that uses hadoop as the underlying scheduler.\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!tar xf  spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "\n",
        "#Provides findspark.init() to make pyspark importable as a regular library.\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.2.4-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 pyspark-shell'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBCqGm0GC8y0",
        "outputId": "393716ce-9a9c-45d7-b87a-461d68461e14"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 294136\n",
            "drwxr-xr-x  1 root root      4096 Apr 28 13:35 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Apr  9 21:17 spark-3.2.4-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 301183180 Apr  9 21:35 spark-3.2.4-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip nashville-tweets-2019-01-28.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG1JNMZ3ESBM",
        "outputId": "db57f666-9f22-49e3-e944-ee54f177b2d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  nashville-tweets-2019-01-28.zip\n",
            "  inflating: nashville-tweets-2019-01-28  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -3 nashville-tweets-2019-01-28.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg0WLNbMDVDG",
        "outputId": "8e8420b4-b683-4342-a8e9-d9d14876a5c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PK\u0003\u0004\u0014\u0000\b\u0000\b\u0000\u000bYyR\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000�\u0011~\u0001\u001b\u0000 \u0000nashville-tweets-2019-01-28UT\r\u0000\u0007v�\\`u�\\`��a`ux\u000b\u0000\u0001\u0004�\u0001\u0000\u0000\u0004\u0014\u0000\u0000\u0000�iw�8�.���\u0015HrW�\u0019\u001c�\u0000A\u0002��};�<&�㊝�N��EI�Ę\"\u0015�������\rj )J�dY���T9�\bN�\u0006�<{���� v�����_�\u0012m��l\u0006~\u001c��A\u001c�\u0011|�\u000f<\u000f��Áߴc�\u0005_�m/r�;�\u0007���M�.�پ%�F���EzA�~���aܵI7���n�\u001aW�A�����o�>��\t�p���\t�؎\u0007Q��\u001d:}ox\u001d\u0007���.ޜj\u0016ӄFMiR.uɄ\u000e����ט6�\u001a��2�\u0014\u001a�L8Զo��ͽ���n�:�\b�|9���_\u0005>~�?�L��\f]X\u0012nk�'l�Mu���\u0005(�_uWx�\u0010;e�\u0014�3j���_�v����\u000eÃ�oG����%�&�l���g9��ns���\u001a�l�\"��M?\u000b=�}�H�+���;N���/|�h�k\u0004�zC��kG���L~\u001f��c=���IO9?���rZ�p\u001c_k2�wn\u001c;!\bA��\u0001Z؞W�A�\u001c�W�\u0011�\u0016�W��\r�J��Zn������}�YI_4+Z���\n",
            "�zf�صj�n�oDx�^G]8�s����m9�u�\u001d�\u001a��a��s��K�V��yۯ|�w^f��&�d\u001a�LX#9p�\u0018��g�\u001dG}��O�Ԕ\u0014�q<�����\u000e>s���g���Vh�/7�p�z�I{jd�7à?>\u0001���Wv\u0002�l�\u0018\u0006\n",
            "B<�\u0013԰��O�Tw�\u0007`��k��_���zX�:�D5� l�>,\\���9 p��uO\r:58�)L3���(\u0005��������\u0015:���>~�����;�\u000b�VՅ����_\u0012�eh�\u0007����_����9i\u0007!q/�������e~}[�:��Tjv�g\u0005.�ס�+��\u001f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark pyspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tibabgyADCxv",
        "outputId": "90ed21f4-acd4-404b-e129-88fb0f75d8b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOCAL\n",
        "%%file local_1_count.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import json\n",
        "from operator import add\n",
        "#import other things as required\n",
        "\n",
        "#create spark context. This is very important. Do this similarly for the other parts\n",
        "# Note to read a file directly from s3 into an rdd you may have to do something like this\n",
        "\n",
        "def checkjson(entry):\n",
        "  try:\n",
        "    json.loads(entry)\n",
        "    #if load succeeded. We use correct as the key\n",
        "    return \"correct\", 1\n",
        "  except:\n",
        "    #there was an error in loading. We use incorrect as the key\n",
        "    return \"incorrect\", 1\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # replace this line with the s3 pass when testing over EMR\n",
        "  conf = SparkConf().setAppName('local_1_count').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "\n",
        "  try:\n",
        "    #@todo: fix the path as required\n",
        "    tweets=sc.textFile('/content/nashville-tweets-2019-01-28')\n",
        "    # review the page rank example for how to use the map operation\n",
        "    # review word count for reduce and add\n",
        "    # see how we use map to parse each row\n",
        "    counts = tweets.map(lambda tweet: checkjson(tweet)).reduceByKey(add)\n",
        "    print(counts.take(2))\n",
        "\n",
        "    # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "    output = counts.sortBy(lambda a: a[1], ascending=False)\n",
        "\n",
        "    # @todo: the s3 version will have to save it to correct s3 path\n",
        "    output.repartition(1).saveAsTextFile(\"local_1_count.out\")\n",
        "    \n",
        "\n",
        "  finally:\n",
        "    # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "    #finally is used to make sure the context is stopped even with errors\n",
        "    sc.stop()\n",
        "  \n",
        "\n",
        " \n",
        "  \n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6864B_IDQa8",
        "outputId": "8b872fb9-3817-486f-c7ae-454ccce20306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing local_1_count.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 local_1_count.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWGlE464FgVe",
        "outputId": "6ca63577-835a-4650-f70f-c827f436bbe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-dce22bc7-e519-41a8-b87c-92cc415d9e8c;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.4.7/spark-streaming-kafka-0-8_2.11-2.4.7.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7!spark-streaming-kafka-0-8_2.11.jar (60ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.kafka#kafka_2.11;0.8.2.1!kafka_2.11.jar (220ms)\n",
            "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
            "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (11ms)\n",
            "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.2/scala-xml_2.11-1.0.2.jar ...\n",
            "\t[SUCCESSFUL ] org.scala-lang.modules#scala-xml_2.11;1.0.2!scala-xml_2.11.jar(bundle) (54ms)\n",
            "downloading https://repo1.maven.org/maven2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar ...\n",
            "\t[SUCCESSFUL ] com.yammer.metrics#metrics-core;2.2.0!metrics-core.jar (16ms)\n",
            "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar ...\n",
            "\t[SUCCESSFUL ] org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0!scala-parser-combinators_2.11.jar(bundle) (34ms)\n",
            "downloading https://repo1.maven.org/maven2/com/101tec/zkclient/0.3/zkclient-0.3.jar ...\n",
            "\t[SUCCESSFUL ] com.101tec#zkclient;0.3!zkclient.jar (15ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;0.8.2.1!kafka-clients.jar (25ms)\n",
            "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...\n",
            "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (12ms)\n",
            "downloading https://repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar ...\n",
            "\t[SUCCESSFUL ] log4j#log4j;1.2.17!log4j.jar(bundle) (68ms)\n",
            "downloading https://repo1.maven.org/maven2/net/jpountz/lz4/lz4/1.2.0/lz4-1.2.0.jar ...\n",
            "\t[SUCCESSFUL ] net.jpountz.lz4#lz4;1.2.0!lz4.jar (18ms)\n",
            "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.7.5/snappy-java-1.1.7.5.jar ...\n",
            "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.7.5!snappy-java.jar(bundle) (117ms)\n",
            ":: resolution report :: resolve 3486ms :: artifacts dl 669ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   12  |   12  |   0   ||   12  |   12  |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-dce22bc7-e519-41a8-b87c-92cc415d9e8c\n",
            "\tconfs: [default]\n",
            "\t12 artifacts copied, 0 already retrieved (8282kB/59ms)\n",
            "23/04/30 02:55:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/30 02:55:35 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/04/30 02:55:35 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 02:55:35 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/30 02:55:35 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 02:55:35 INFO SparkContext: Submitted application: local_1_count\n",
            "23/04/30 02:55:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/30 02:55:35 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/30 02:55:35 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/30 02:55:35 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/30 02:55:35 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/30 02:55:35 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/30 02:55:35 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/30 02:55:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/30 02:55:36 INFO Utils: Successfully started service 'sparkDriver' on port 42663.\n",
            "23/04/30 02:55:36 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/30 02:55:36 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/30 02:55:36 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/30 02:55:36 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/30 02:55:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/30 02:55:36 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-211e57e0-e4d7-4c6b-aa38-2d6fd4110cfd\n",
            "23/04/30 02:55:36 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/30 02:55:36 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/30 02:55:36 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/30 02:55:37 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://fe7812865801:4040\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://fe7812865801:42663/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://fe7812865801:42663/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://fe7812865801:42663/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://fe7812865801:42663/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://fe7812865801:42663/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://fe7812865801:42663/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://fe7812865801:42663/jars/com.101tec_zkclient-0.3.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://fe7812865801:42663/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://fe7812865801:42663/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://fe7812865801:42663/jars/log4j_log4j-1.2.17.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://fe7812865801:42663/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://fe7812865801:42663/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/log4j_log4j-1.2.17.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 02:55:37 INFO Executor: Starting executor ID driver on host fe7812865801\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/log4j_log4j-1.2.17.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO TransportClientFactory: Successfully created connection to fe7812865801/172.28.0.12:42663 after 41 ms (0 ms spent in bootstraps)\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp454044287111138420.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp454044287111138420.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp3647916134763500826.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp3647916134763500826.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp5471250395108677831.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp5471250395108677831.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp970250618888269534.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp970250618888269534.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp8674847155725214929.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp8674847155725214929.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp7380504686912578160.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp7380504686912578160.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/log4j_log4j-1.2.17.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/log4j_log4j-1.2.17.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp8145504063903493070.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp8145504063903493070.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/log4j_log4j-1.2.17.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp7084789055581084624.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp7084789055581084624.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp2434420460375898848.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp2434420460375898848.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp6250372548388709988.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp6250372548388709988.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/com.101tec_zkclient-0.3.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp3691715182997485759.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp3691715182997485759.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp2049087260221762115.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp2049087260221762115.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/30 02:55:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33917.\n",
            "23/04/30 02:55:37 INFO NettyBlockTransferService: Server created on fe7812865801:33917\n",
            "23/04/30 02:55:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/30 02:55:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fe7812865801, 33917, None)\n",
            "23/04/30 02:55:37 INFO BlockManagerMasterEndpoint: Registering block manager fe7812865801:33917 with 366.3 MiB RAM, BlockManagerId(driver, fe7812865801, 33917, None)\n",
            "23/04/30 02:55:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fe7812865801, 33917, None)\n",
            "23/04/30 02:55:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fe7812865801, 33917, None)\n",
            "23/04/30 02:55:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.3 KiB, free 366.0 MiB)\n",
            "23/04/30 02:55:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fe7812865801:33917 (size: 32.0 KiB, free: 366.3 MiB)\n",
            "23/04/30 02:55:39 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 02:55:40 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/30 02:55:40 INFO SparkContext: Starting job: runJob at PythonRDD.scala:166\n",
            "23/04/30 02:55:40 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /content/local_1_count.py:37) as input to shuffle 0\n",
            "23/04/30 02:55:40 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:166) with 1 output partitions\n",
            "23/04/30 02:55:40 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at PythonRDD.scala:166)\n",
            "23/04/30 02:55:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
            "23/04/30 02:55:40 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
            "23/04/30 02:55:41 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/local_1_count.py:37), which has no missing parents\n",
            "23/04/30 02:55:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.2 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fe7812865801:33917 (size: 7.5 KiB, free: 366.3 MiB)\n",
            "23/04/30 02:55:41 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:41 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/local_1_count.py:37) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/30 02:55:41 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "23/04/30 02:55:41 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (fe7812865801, executor driver, partition 0, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:41 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (fe7812865801, executor driver, partition 1, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:41 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "23/04/30 02:55:41 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/30 02:55:42 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:0+12519625\n",
            "23/04/30 02:55:42 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:12519625+12519626\n",
            "23/04/30 02:55:46 INFO PythonRunner: Times: total = 2872, boot = 1417, init = 578, finish = 877\n",
            "23/04/30 02:55:46 INFO PythonRunner: Times: total = 2821, boot = 1433, init = 591, finish = 797\n",
            "23/04/30 02:55:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1612 bytes result sent to driver\n",
            "23/04/30 02:55:46 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1655 bytes result sent to driver\n",
            "23/04/30 02:55:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5020 ms on fe7812865801 (executor driver) (1/2)\n",
            "23/04/30 02:55:46 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 4958 ms on fe7812865801 (executor driver) (2/2)\n",
            "23/04/30 02:55:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:46 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 52053\n",
            "23/04/30 02:55:46 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /content/local_1_count.py:37) finished in 5.613 s\n",
            "23/04/30 02:55:46 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 02:55:46 INFO DAGScheduler: running: Set()\n",
            "23/04/30 02:55:46 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
            "23/04/30 02:55:46 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 02:55:46 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at RDD at PythonRDD.scala:53), which has no missing parents\n",
            "23/04/30 02:55:46 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.2 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:46 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:46 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on fe7812865801:33917 (size: 6.1 KiB, free: 366.3 MiB)\n",
            "23/04/30 02:55:46 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[6] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 02:55:46 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/04/30 02:55:46 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (fe7812865801, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:46 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)\n",
            "23/04/30 02:55:46 INFO ShuffleBlockFetcherIterator: Getting 2 (160.0 B) non-empty blocks including 2 (160.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 27 ms\n",
            "23/04/30 02:55:46 INFO PythonRunner: Times: total = 76, boot = -955, init = 1031, finish = 0\n",
            "23/04/30 02:55:46 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 1625 bytes result sent to driver\n",
            "23/04/30 02:55:46 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 200 ms on fe7812865801 (executor driver) (1/1)\n",
            "23/04/30 02:55:46 INFO DAGScheduler: ResultStage 1 (runJob at PythonRDD.scala:166) finished in 0.224 s\n",
            "23/04/30 02:55:46 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:46 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 02:55:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "23/04/30 02:55:46 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:166, took 6.105414 s\n",
            "23/04/30 02:55:47 INFO SparkContext: Starting job: runJob at PythonRDD.scala:166\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Got job 1 (runJob at PythonRDD.scala:166) with 1 output partitions\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Final stage: ResultStage 3 (runJob at PythonRDD.scala:166)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[7] at RDD at PythonRDD.scala:53), which has no missing parents\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.2 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:47 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on fe7812865801:33917 (size: 6.1 KiB, free: 366.2 MiB)\n",
            "23/04/30 02:55:47 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (PythonRDD[7] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(1))\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (fe7812865801, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:47 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 02:55:47 INFO PythonRunner: Times: total = 74, boot = -1144, init = 1217, finish = 1\n",
            "23/04/30 02:55:47 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1572 bytes result sent to driver\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 104 ms on fe7812865801 (executor driver) (1/1)\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:47 INFO DAGScheduler: ResultStage 3 (runJob at PythonRDD.scala:166) finished in 0.126 s\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Job 1 finished: runJob at PythonRDD.scala:166, took 0.138441 s\n",
            "[('correct', 6294)]\n",
            "23/04/30 02:55:47 INFO SparkContext: Starting job: sortBy at /content/local_1_count.py:41\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Got job 2 (sortBy at /content/local_1_count.py:41) with 2 output partitions\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Final stage: ResultStage 5 (sortBy at /content/local_1_count.py:41)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting ResultStage 5 (PythonRDD[8] at sortBy at /content/local_1_count.py:41), which has no missing parents\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 11.7 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:47 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on fe7812865801:33917 (size: 6.6 KiB, free: 366.2 MiB)\n",
            "23/04/30 02:55:47 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 5 (PythonRDD[8] at sortBy at /content/local_1_count.py:41) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (fe7812865801, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 5) (fe7812865801, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:47 INFO Executor: Running task 1.0 in stage 5.0 (TID 5)\n",
            "23/04/30 02:55:47 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Getting 2 (160.0 B) non-empty blocks including 2 (160.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms\n",
            "23/04/30 02:55:47 INFO PythonRunner: Times: total = 138, boot = -329, init = 467, finish = 0\n",
            "23/04/30 02:55:47 INFO Executor: Finished task 1.0 in stage 5.0 (TID 5). 1612 bytes result sent to driver\n",
            "23/04/30 02:55:47 INFO PythonRunner: Times: total = 136, boot = -126, init = 262, finish = 0\n",
            "23/04/30 02:55:47 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 1612 bytes result sent to driver\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 5) in 187 ms on fe7812865801 (executor driver) (1/2)\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 188 ms on fe7812865801 (executor driver) (2/2)\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:47 INFO DAGScheduler: ResultStage 5 (sortBy at /content/local_1_count.py:41) finished in 0.211 s\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Job 2 finished: sortBy at /content/local_1_count.py:41, took 0.219877 s\n",
            "23/04/30 02:55:47 INFO SparkContext: Starting job: sortBy at /content/local_1_count.py:41\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Got job 3 (sortBy at /content/local_1_count.py:41) with 2 output partitions\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Final stage: ResultStage 7 (sortBy at /content/local_1_count.py:41)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting ResultStage 7 (PythonRDD[9] at sortBy at /content/local_1_count.py:41), which has no missing parents\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 11.0 KiB, free 365.8 MiB)\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 365.8 MiB)\n",
            "23/04/30 02:55:47 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on fe7812865801:33917 (size: 6.4 KiB, free: 366.2 MiB)\n",
            "23/04/30 02:55:47 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 7 (PythonRDD[9] at sortBy at /content/local_1_count.py:41) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Adding task set 7.0 with 2 tasks resource profile 0\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (fe7812865801, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 7) (fe7812865801, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:47 INFO Executor: Running task 1.0 in stage 7.0 (TID 7)\n",
            "23/04/30 02:55:47 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Getting 2 (160.0 B) non-empty blocks including 2 (160.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms\n",
            "23/04/30 02:55:47 INFO PythonRunner: Times: total = 135, boot = -58, init = 193, finish = 0\n",
            "23/04/30 02:55:47 INFO PythonRunner: Times: total = 128, boot = -84, init = 212, finish = 0\n",
            "23/04/30 02:55:47 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 1613 bytes result sent to driver\n",
            "23/04/30 02:55:47 INFO Executor: Finished task 1.0 in stage 7.0 (TID 7). 1572 bytes result sent to driver\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 181 ms on fe7812865801 (executor driver) (1/2)\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 7) in 182 ms on fe7812865801 (executor driver) (2/2)\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:47 INFO DAGScheduler: ResultStage 7 (sortBy at /content/local_1_count.py:41) finished in 0.197 s\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Job 3 finished: sortBy at /content/local_1_count.py:41, took 0.205789 s\n",
            "23/04/30 02:55:47 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "23/04/30 02:55:47 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/30 02:55:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 02:55:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 02:55:47 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Registering RDD 11 (sortBy at /content/local_1_count.py:41) as input to shuffle 2\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Registering RDD 15 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Got job 4 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Final stage: ResultStage 11 (runJob at SparkHadoopWriter.scala:83)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 10)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting ShuffleMapStage 9 (PairwiseRDD[11] at sortBy at /content/local_1_count.py:41), which has no missing parents\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 11.8 KiB, free 365.8 MiB)\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 365.8 MiB)\n",
            "23/04/30 02:55:47 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on fe7812865801:33917 (size: 7.2 KiB, free: 366.2 MiB)\n",
            "23/04/30 02:55:47 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (PairwiseRDD[11] at sortBy at /content/local_1_count.py:41) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Adding task set 9.0 with 2 tasks resource profile 0\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 8) (fe7812865801, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 9) (fe7812865801, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:47 INFO Executor: Running task 0.0 in stage 9.0 (TID 8)\n",
            "23/04/30 02:55:47 INFO Executor: Running task 1.0 in stage 9.0 (TID 9)\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Getting 2 (160.0 B) non-empty blocks including 2 (160.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms\n",
            "23/04/30 02:55:48 INFO PythonRunner: Times: total = 132, boot = -258, init = 390, finish = 0\n",
            "23/04/30 02:55:48 INFO Executor: Finished task 0.0 in stage 9.0 (TID 8). 1784 bytes result sent to driver\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 8) in 173 ms on fe7812865801 (executor driver) (1/2)\n",
            "23/04/30 02:55:48 INFO PythonRunner: Times: total = 170, boot = -290, init = 460, finish = 0\n",
            "23/04/30 02:55:48 INFO Executor: Finished task 1.0 in stage 9.0 (TID 9). 1655 bytes result sent to driver\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 9) in 214 ms on fe7812865801 (executor driver) (2/2)\n",
            "23/04/30 02:55:48 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:48 INFO DAGScheduler: ShuffleMapStage 9 (sortBy at /content/local_1_count.py:41) finished in 0.229 s\n",
            "23/04/30 02:55:48 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 02:55:48 INFO DAGScheduler: running: Set()\n",
            "23/04/30 02:55:48 INFO DAGScheduler: waiting: Set(ShuffleMapStage 10, ResultStage 11)\n",
            "23/04/30 02:55:48 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 02:55:48 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[15] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 02:55:48 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 10.2 KiB, free 365.8 MiB)\n",
            "23/04/30 02:55:48 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 365.8 MiB)\n",
            "23/04/30 02:55:48 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on fe7812865801:33917 (size: 6.0 KiB, free: 366.2 MiB)\n",
            "23/04/30 02:55:48 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[15] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/30 02:55:48 INFO TaskSchedulerImpl: Adding task set 10.0 with 2 tasks resource profile 0\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 10) (fe7812865801, executor driver, partition 1, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 11) (fe7812865801, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:48 INFO Executor: Running task 1.0 in stage 10.0 (TID 10)\n",
            "23/04/30 02:55:48 INFO Executor: Running task 0.0 in stage 10.0 (TID 11)\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms\n",
            "23/04/30 02:55:48 INFO PythonRunner: Times: total = 129, boot = -107, init = 236, finish = 0\n",
            "23/04/30 02:55:48 INFO Executor: Finished task 0.0 in stage 10.0 (TID 11). 1654 bytes result sent to driver\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 11) in 182 ms on fe7812865801 (executor driver) (1/2)\n",
            "23/04/30 02:55:48 INFO PythonRunner: Times: total = 151, boot = -56, init = 207, finish = 0\n",
            "23/04/30 02:55:48 INFO Executor: Finished task 1.0 in stage 10.0 (TID 10). 1783 bytes result sent to driver\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 10) in 218 ms on fe7812865801 (executor driver) (2/2)\n",
            "23/04/30 02:55:48 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:48 INFO DAGScheduler: ShuffleMapStage 10 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.250 s\n",
            "23/04/30 02:55:48 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 02:55:48 INFO DAGScheduler: running: Set()\n",
            "23/04/30 02:55:48 INFO DAGScheduler: waiting: Set(ResultStage 11)\n",
            "23/04/30 02:55:48 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 02:55:48 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[21] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 02:55:48 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 105.9 KiB, free 365.7 MiB)\n",
            "23/04/30 02:55:48 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 39.7 KiB, free 365.7 MiB)\n",
            "23/04/30 02:55:48 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on fe7812865801:33917 (size: 39.7 KiB, free: 366.2 MiB)\n",
            "23/04/30 02:55:48 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[21] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 02:55:48 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 12) (fe7812865801, executor driver, partition 0, NODE_LOCAL, 4547 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:48 INFO Executor: Running task 0.0 in stage 11.0 (TID 12)\n",
            "23/04/30 02:55:48 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/30 02:55:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 02:55:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms\n",
            "23/04/30 02:55:48 INFO PythonRunner: Times: total = 145, boot = -213, init = 358, finish = 0\n",
            "23/04/30 02:55:48 INFO FileOutputCommitter: Saved output of task 'attempt_202304300255471325538644976917902_0021_m_000000_0' to file:/content/local_1_count.out/_temporary/0/task_202304300255471325538644976917902_0021_m_000000\n",
            "23/04/30 02:55:48 INFO SparkHadoopMapRedUtil: attempt_202304300255471325538644976917902_0021_m_000000_0: Committed\n",
            "23/04/30 02:55:48 INFO Executor: Finished task 0.0 in stage 11.0 (TID 12). 1952 bytes result sent to driver\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 12) in 315 ms on fe7812865801 (executor driver) (1/1)\n",
            "23/04/30 02:55:48 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:48 INFO DAGScheduler: ResultStage 11 (runJob at SparkHadoopWriter.scala:83) finished in 0.369 s\n",
            "23/04/30 02:55:48 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 02:55:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
            "23/04/30 02:55:48 INFO DAGScheduler: Job 4 finished: runJob at SparkHadoopWriter.scala:83, took 0.884339 s\n",
            "23/04/30 02:55:48 INFO SparkHadoopWriter: Start to commit write Job job_202304300255471325538644976917902_0021.\n",
            "23/04/30 02:55:48 INFO SparkHadoopWriter: Write Job job_202304300255471325538644976917902_0021 committed. Elapsed time: 18 ms.\n",
            "23/04/30 02:55:48 INFO SparkUI: Stopped Spark web UI at http://fe7812865801:4040\n",
            "23/04/30 02:55:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/30 02:55:48 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/30 02:55:48 INFO BlockManager: BlockManager stopped\n",
            "23/04/30 02:55:48 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/30 02:55:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/30 02:55:48 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/30 02:55:49 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/30 02:55:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a\n",
            "23/04/30 02:55:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-73213582-d3dc-4f67-b966-404ac352b724\n",
            "23/04/30 02:55:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/pyspark-3bfb6939-d3f0-4839-98e2-c8d3755da6ac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test1(lines):\n",
        "  if '6294' in lines[0] and 'correct' in lines[0]:\n",
        "    print(\"passed\")\n",
        "  else:\n",
        "    print(\"failed\")\n",
        "\n",
        "# test local execution results\n",
        "with open('local_1_count.out/part-00000') as f:\n",
        "  lines = f.readlines()\n",
        "  print(lines)\n",
        "  test1(lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKQKQItEQRIB",
        "outputId": "ceb843f9-bfc4-47ee-da62-60e5f28af425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"('correct', 6294)\\n\"]\n",
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We have verified that the old homework question works with our EMR and S3 buckets...\n",
        "\n",
        "### Now, let's move on to trying to read in a basic parquet file"
      ],
      "metadata": {
        "id": "k9-oFWK1RHEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New day, new attempts\n",
        "\n",
        "THESE ARE STARTING FROM SCRATCH... NO PREVIOUS CODE WAS USED"
      ],
      "metadata": {
        "id": "iyKGPQMNUbls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3\n",
        "import boto3, json\n",
        "\n",
        "credentials = {\n",
        "    'region_name': 'us-east-1',\n",
        "    'aws_access_key_id': 'ASIAYELUFHJMURE4TF2W',\n",
        "    'aws_secret_access_key': 'D3nKroBer2njIx0GWVs7jywex0tHusXmBmcu5Cy8',\n",
        "    'aws_session_token': 'FwoGZXIvYXdzELX//////////wEaDMxb9NYrFGfv1HiPOSLMAYytcd0aipS3xMmUymMz7o1cdXHLj59D7CO9NPLnAOa6a0zyNB3vE/tCMFbEwB9YffuAW6+XclMnbfwza91I+J08hDMcLsB5/TSJuEmUt/mXnaNf59JfLMC02xGz3P3jx8e5ZIVC1C+AN/ZJXrGQyJwP8OgnLhnP9pd4iYeQtHJjwjhP2XwKpeDWYNefvt8v8ZG24Qd86htiw8q9f8YUTfasAOh6uRd1Q2Hh+au7I7wPHGK3zFblYK/x/aCqfOsjE1XHJPFuSF0cye/VpyjRrcCiBjItNo07vadXO93/LrHEu7Mu2Eqhx76fPulAe9/M1DBLvQJgzWd0cfOqyq0JmDFt'\n",
        "}\n",
        "\n",
        "session = boto3.session.Session(**credentials)\n",
        "s3 = session.client('s3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlkj5_A4WXUN",
        "outputId": "d618ea66-ab59-4c1b-d0dd-578d9349b190"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.123-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.123 (from boto3)\n",
            "  Downloading botocore-1.29.123-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3)\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.123->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.123->boto3) (1.26.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.123->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.26.123 botocore-1.29.123 jmespath-1.0.1 s3transfer-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The following works, meaning that we can store everything locally"
      ],
      "metadata": {
        "id": "QcUZkznPdiBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First I want to run a test to see if we can download all of the data locally\n",
        "num_years = 13\n",
        "num_months = 12\n",
        "\n",
        "years_list = [2010 + i for i in range(num_years)]\n",
        "months_list = [i for i in range(1, 13)]\n",
        "print(years_list)\n",
        "print(months_list)\n",
        "\n",
        "full_path = 'weather_tn.parquet/year=2021/month=1/part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet'\n",
        "for year in years_list:\n",
        "  for month in months_list:\n",
        "    local_name =  str(year) + '-' + str(month) +'-weather.parquet'\n",
        "    s3.download_file('cs4266-finalproject', full_path, local_name)\n",
        "    print(\"Downloaded\", local_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzCYUgMvUdsW",
        "outputId": "e41525cc-15fa-42ec-88b8-01601205175c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
            "Downloaded 2010-1-weather.parquet\n",
            "Downloaded 2010-2-weather.parquet\n",
            "Downloaded 2010-3-weather.parquet\n",
            "Downloaded 2010-4-weather.parquet\n",
            "Downloaded 2010-5-weather.parquet\n",
            "Downloaded 2010-6-weather.parquet\n",
            "Downloaded 2010-7-weather.parquet\n",
            "Downloaded 2010-8-weather.parquet\n",
            "Downloaded 2010-9-weather.parquet\n",
            "Downloaded 2010-10-weather.parquet\n",
            "Downloaded 2010-11-weather.parquet\n",
            "Downloaded 2010-12-weather.parquet\n",
            "Downloaded 2011-1-weather.parquet\n",
            "Downloaded 2011-2-weather.parquet\n",
            "Downloaded 2011-3-weather.parquet\n",
            "Downloaded 2011-4-weather.parquet\n",
            "Downloaded 2011-5-weather.parquet\n",
            "Downloaded 2011-6-weather.parquet\n",
            "Downloaded 2011-7-weather.parquet\n",
            "Downloaded 2011-8-weather.parquet\n",
            "Downloaded 2011-9-weather.parquet\n",
            "Downloaded 2011-10-weather.parquet\n",
            "Downloaded 2011-11-weather.parquet\n",
            "Downloaded 2011-12-weather.parquet\n",
            "Downloaded 2012-1-weather.parquet\n",
            "Downloaded 2012-2-weather.parquet\n",
            "Downloaded 2012-3-weather.parquet\n",
            "Downloaded 2012-4-weather.parquet\n",
            "Downloaded 2012-5-weather.parquet\n",
            "Downloaded 2012-6-weather.parquet\n",
            "Downloaded 2012-7-weather.parquet\n",
            "Downloaded 2012-8-weather.parquet\n",
            "Downloaded 2012-9-weather.parquet\n",
            "Downloaded 2012-10-weather.parquet\n",
            "Downloaded 2012-11-weather.parquet\n",
            "Downloaded 2012-12-weather.parquet\n",
            "Downloaded 2013-1-weather.parquet\n",
            "Downloaded 2013-2-weather.parquet\n",
            "Downloaded 2013-3-weather.parquet\n",
            "Downloaded 2013-4-weather.parquet\n",
            "Downloaded 2013-5-weather.parquet\n",
            "Downloaded 2013-6-weather.parquet\n",
            "Downloaded 2013-7-weather.parquet\n",
            "Downloaded 2013-8-weather.parquet\n",
            "Downloaded 2013-9-weather.parquet\n",
            "Downloaded 2013-10-weather.parquet\n",
            "Downloaded 2013-11-weather.parquet\n",
            "Downloaded 2013-12-weather.parquet\n",
            "Downloaded 2014-1-weather.parquet\n",
            "Downloaded 2014-2-weather.parquet\n",
            "Downloaded 2014-3-weather.parquet\n",
            "Downloaded 2014-4-weather.parquet\n",
            "Downloaded 2014-5-weather.parquet\n",
            "Downloaded 2014-6-weather.parquet\n",
            "Downloaded 2014-7-weather.parquet\n",
            "Downloaded 2014-8-weather.parquet\n",
            "Downloaded 2014-9-weather.parquet\n",
            "Downloaded 2014-10-weather.parquet\n",
            "Downloaded 2014-11-weather.parquet\n",
            "Downloaded 2014-12-weather.parquet\n",
            "Downloaded 2015-1-weather.parquet\n",
            "Downloaded 2015-2-weather.parquet\n",
            "Downloaded 2015-3-weather.parquet\n",
            "Downloaded 2015-4-weather.parquet\n",
            "Downloaded 2015-5-weather.parquet\n",
            "Downloaded 2015-6-weather.parquet\n",
            "Downloaded 2015-7-weather.parquet\n",
            "Downloaded 2015-8-weather.parquet\n",
            "Downloaded 2015-9-weather.parquet\n",
            "Downloaded 2015-10-weather.parquet\n",
            "Downloaded 2015-11-weather.parquet\n",
            "Downloaded 2015-12-weather.parquet\n",
            "Downloaded 2016-1-weather.parquet\n",
            "Downloaded 2016-2-weather.parquet\n",
            "Downloaded 2016-3-weather.parquet\n",
            "Downloaded 2016-4-weather.parquet\n",
            "Downloaded 2016-5-weather.parquet\n",
            "Downloaded 2016-6-weather.parquet\n",
            "Downloaded 2016-7-weather.parquet\n",
            "Downloaded 2016-8-weather.parquet\n",
            "Downloaded 2016-9-weather.parquet\n",
            "Downloaded 2016-10-weather.parquet\n",
            "Downloaded 2016-11-weather.parquet\n",
            "Downloaded 2016-12-weather.parquet\n",
            "Downloaded 2017-1-weather.parquet\n",
            "Downloaded 2017-2-weather.parquet\n",
            "Downloaded 2017-3-weather.parquet\n",
            "Downloaded 2017-4-weather.parquet\n",
            "Downloaded 2017-5-weather.parquet\n",
            "Downloaded 2017-6-weather.parquet\n",
            "Downloaded 2017-7-weather.parquet\n",
            "Downloaded 2017-8-weather.parquet\n",
            "Downloaded 2017-9-weather.parquet\n",
            "Downloaded 2017-10-weather.parquet\n",
            "Downloaded 2017-11-weather.parquet\n",
            "Downloaded 2017-12-weather.parquet\n",
            "Downloaded 2018-1-weather.parquet\n",
            "Downloaded 2018-2-weather.parquet\n",
            "Downloaded 2018-3-weather.parquet\n",
            "Downloaded 2018-4-weather.parquet\n",
            "Downloaded 2018-5-weather.parquet\n",
            "Downloaded 2018-6-weather.parquet\n",
            "Downloaded 2018-7-weather.parquet\n",
            "Downloaded 2018-8-weather.parquet\n",
            "Downloaded 2018-9-weather.parquet\n",
            "Downloaded 2018-10-weather.parquet\n",
            "Downloaded 2018-11-weather.parquet\n",
            "Downloaded 2018-12-weather.parquet\n",
            "Downloaded 2019-1-weather.parquet\n",
            "Downloaded 2019-2-weather.parquet\n",
            "Downloaded 2019-3-weather.parquet\n",
            "Downloaded 2019-4-weather.parquet\n",
            "Downloaded 2019-5-weather.parquet\n",
            "Downloaded 2019-6-weather.parquet\n",
            "Downloaded 2019-7-weather.parquet\n",
            "Downloaded 2019-8-weather.parquet\n",
            "Downloaded 2019-9-weather.parquet\n",
            "Downloaded 2019-10-weather.parquet\n",
            "Downloaded 2019-11-weather.parquet\n",
            "Downloaded 2019-12-weather.parquet\n",
            "Downloaded 2020-1-weather.parquet\n",
            "Downloaded 2020-2-weather.parquet\n",
            "Downloaded 2020-3-weather.parquet\n",
            "Downloaded 2020-4-weather.parquet\n",
            "Downloaded 2020-5-weather.parquet\n",
            "Downloaded 2020-6-weather.parquet\n",
            "Downloaded 2020-7-weather.parquet\n",
            "Downloaded 2020-8-weather.parquet\n",
            "Downloaded 2020-9-weather.parquet\n",
            "Downloaded 2020-10-weather.parquet\n",
            "Downloaded 2020-11-weather.parquet\n",
            "Downloaded 2020-12-weather.parquet\n",
            "Downloaded 2021-1-weather.parquet\n",
            "Downloaded 2021-2-weather.parquet\n",
            "Downloaded 2021-3-weather.parquet\n",
            "Downloaded 2021-4-weather.parquet\n",
            "Downloaded 2021-5-weather.parquet\n",
            "Downloaded 2021-6-weather.parquet\n",
            "Downloaded 2021-7-weather.parquet\n",
            "Downloaded 2021-8-weather.parquet\n",
            "Downloaded 2021-9-weather.parquet\n",
            "Downloaded 2021-10-weather.parquet\n",
            "Downloaded 2021-11-weather.parquet\n",
            "Downloaded 2021-12-weather.parquet\n",
            "Downloaded 2022-1-weather.parquet\n",
            "Downloaded 2022-2-weather.parquet\n",
            "Downloaded 2022-3-weather.parquet\n",
            "Downloaded 2022-4-weather.parquet\n",
            "Downloaded 2022-5-weather.parquet\n",
            "Downloaded 2022-6-weather.parquet\n",
            "Downloaded 2022-7-weather.parquet\n",
            "Downloaded 2022-8-weather.parquet\n",
            "Downloaded 2022-9-weather.parquet\n",
            "Downloaded 2022-10-weather.parquet\n",
            "Downloaded 2022-11-weather.parquet\n",
            "Downloaded 2022-12-weather.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now we want to see if we can join incidents and weather bucketed by hour\n",
        "\n",
        "Start by downloading weather for January of 2021"
      ],
      "metadata": {
        "id": "ku7acXZ0d7ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_path = 'weather_tn.parquet/year=2021/month=1/part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet'\n",
        "\n",
        "local_name = '2021-1-weather.parquet'\n",
        "s3.download_file('cs4266-finalproject', full_path, local_name)\n",
        "print(\"Downloaded\", local_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNVlAxdUZMw3",
        "outputId": "baefa4cd-8493-40ee-9fc6-45266ce8deb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 2021-1-weather.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now download the incidents file"
      ],
      "metadata": {
        "id": "ZIcGeigNeDDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_path = 'nfd_incidents_xd_seg.parquet'\n",
        "\n",
        "local_name = 'incidents.parquet'\n",
        "s3.download_file('cs4266-finalproject', full_path, local_name)\n",
        "print(\"Downloaded\", local_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMz3sS3fd51H",
        "outputId": "dbf69fb9-005e-4ba3-fe37-ee038eef031b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded incidents.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "608AcUfueRZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple test on weather data"
      ],
      "metadata": {
        "id": "6_ngEQwyjFIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file localtest2.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('localtest2').getOrCreate()\n",
        "\n",
        "try:\n",
        "  # read in parquet file\n",
        "  # part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n",
        "  tweets_df = spark.read.parquet('2021-1-weather.parquet')\n",
        "  tweets_df.createOrReplaceTempView(\"weather\")\n",
        "\n",
        "  # run SQL query\n",
        "  counts = spark.sql('SELECT COUNT(*) FROM weather')\n",
        "  print(counts.show())\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = counts#.sort(\"count\", ascending=False)\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"localtest2.out\")\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmFaqk7feiba",
        "outputId": "5ceca35e-7128-47df-d6c0-71b05985e565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting localtest2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 localtest2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNrEC-J6fBqY",
        "outputId": "93ed157b-778b-49b4-807f-a78069f7a684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c35c0565-c994-46ae-8a08-7cd28938a725;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 914ms :: artifacts dl 59ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-c35c0565-c994-46ae-8a08-7cd28938a725\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/26ms)\n",
            "23/04/30 19:01:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/30 19:01:56 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/04/30 19:01:56 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 19:01:56 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/30 19:01:56 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 19:01:56 INFO SparkContext: Submitted application: localtest2\n",
            "23/04/30 19:01:56 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/30 19:01:56 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/30 19:01:56 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/30 19:01:56 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/30 19:01:56 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/30 19:01:56 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/30 19:01:56 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/30 19:01:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/30 19:01:57 INFO Utils: Successfully started service 'sparkDriver' on port 38317.\n",
            "23/04/30 19:01:57 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/30 19:01:57 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/30 19:01:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/30 19:01:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/30 19:01:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/30 19:01:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1aa6f82a-d68e-4561-9899-c586822bb6b8\n",
            "23/04/30 19:01:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/30 19:01:57 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/30 19:01:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/30 19:01:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://145f628fbc4b:4040\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://145f628fbc4b:38317/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://145f628fbc4b:38317/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://145f628fbc4b:38317/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://145f628fbc4b:38317/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://145f628fbc4b:38317/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://145f628fbc4b:38317/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://145f628fbc4b:38317/jars/com.101tec_zkclient-0.3.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://145f628fbc4b:38317/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://145f628fbc4b:38317/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://145f628fbc4b:38317/jars/log4j_log4j-1.2.17.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://145f628fbc4b:38317/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://145f628fbc4b:38317/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/log4j_log4j-1.2.17.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 19:01:58 INFO Executor: Starting executor ID driver on host 145f628fbc4b\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/log4j_log4j-1.2.17.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO TransportClientFactory: Successfully created connection to 145f628fbc4b/172.28.0.12:38317 after 71 ms (0 ms spent in bootstraps)\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp7540423681872739871.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp7540423681872739871.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp6755531432535173981.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp6755531432535173981.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/com.101tec_zkclient-0.3.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp5743219548581484238.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp5743219548581484238.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp4272695134485183491.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp4272695134485183491.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp5634625261822589726.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp5634625261822589726.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp407738289051001962.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp407738289051001962.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp595870644550565025.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp595870644550565025.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp3507976221332203627.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp3507976221332203627.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp5911555324453383978.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp5911555324453383978.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/log4j_log4j-1.2.17.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/log4j_log4j-1.2.17.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp1765298295228190302.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp1765298295228190302.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/log4j_log4j-1.2.17.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp7904445938667965601.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp7904445938667965601.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp384757528685471959.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp384757528685471959.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/30 19:01:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46421.\n",
            "23/04/30 19:01:59 INFO NettyBlockTransferService: Server created on 145f628fbc4b:46421\n",
            "23/04/30 19:01:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/30 19:01:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 145f628fbc4b, 46421, None)\n",
            "23/04/30 19:01:59 INFO BlockManagerMasterEndpoint: Registering block manager 145f628fbc4b:46421 with 366.3 MiB RAM, BlockManagerId(driver, 145f628fbc4b, 46421, None)\n",
            "23/04/30 19:01:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 145f628fbc4b, 46421, None)\n",
            "23/04/30 19:01:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 145f628fbc4b, 46421, None)\n",
            "23/04/30 19:02:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/04/30 19:02:00 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/04/30 19:02:02 INFO InMemoryFileIndex: It took 56 ms to list leaf files for 1 paths.\n",
            "23/04/30 19:02:03 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 19:02:03 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:02:03 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:02:03 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 19:02:03 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:02:03 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:02:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/04/30 19:02:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/04/30 19:02:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 145f628fbc4b:46421 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 19:02:03 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:02:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:02:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:02:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4584 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:02:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/30 19:02:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3118 bytes result sent to driver\n",
            "23/04/30 19:02:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1114 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:02:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:02:05 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.460 s\n",
            "23/04/30 19:02:05 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 19:02:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/30 19:02:05 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.551314 s\n",
            "23/04/30 19:02:05 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 145f628fbc4b:46421 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 19:02:08 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "23/04/30 19:02:09 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/30 19:02:09 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "23/04/30 19:02:09 INFO FileSourceStrategy: Output Data Schema: struct<>\n",
            "23/04/30 19:02:09 INFO CodeGenerator: Code generated in 251.496979 ms\n",
            "23/04/30 19:02:09 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 351.9 KiB, free 366.0 MiB)\n",
            "23/04/30 19:02:09 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.5 KiB, free 365.9 MiB)\n",
            "23/04/30 19:02:09 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 145f628fbc4b:46421 (size: 35.5 KiB, free: 366.3 MiB)\n",
            "23/04/30 19:02:09 INFO SparkContext: Created broadcast 1 from showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 19:02:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/30 19:02:09 INFO DAGScheduler: Registering RDD 5 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/30 19:02:09 INFO DAGScheduler: Got map stage job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:02:09 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:02:09 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 19:02:09 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:02:09 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:02:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 15.2 KiB, free 365.9 MiB)\n",
            "23/04/30 19:02:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 365.9 MiB)\n",
            "23/04/30 19:02:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 145f628fbc4b:46421 (size: 7.2 KiB, free: 366.3 MiB)\n",
            "23/04/30 19:02:09 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:02:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:02:09 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:02:09 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4852 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:02:09 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/30 19:02:10 INFO FileScanRDD: Reading File path: file:///content/2021-1-weather.parquet, range: 0-3369951, partition values: [empty row]\n",
            "23/04/30 19:02:10 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2167 bytes result sent to driver\n",
            "23/04/30 19:02:10 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 430 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:02:10 INFO DAGScheduler: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.535 s\n",
            "23/04/30 19:02:10 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 19:02:10 INFO DAGScheduler: running: Set()\n",
            "23/04/30 19:02:10 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 19:02:10 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 19:02:10 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:02:10 INFO CodeGenerator: Code generated in 51.919664 ms\n",
            "23/04/30 19:02:10 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 19:02:10 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:02:10 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:02:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/30 19:02:10 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:02:10 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:02:10 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.1 KiB, free 365.9 MiB)\n",
            "23/04/30 19:02:10 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 365.9 MiB)\n",
            "23/04/30 19:02:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 145f628fbc4b:46421 (size: 5.5 KiB, free: 366.3 MiB)\n",
            "23/04/30 19:02:10 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:02:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:02:10 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:02:10 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:02:10 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
            "23/04/30 19:02:10 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 19:02:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 22 ms\n",
            "23/04/30 19:02:10 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 2657 bytes result sent to driver\n",
            "23/04/30 19:02:10 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 104 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:02:10 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:02:10 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.138 s\n",
            "23/04/30 19:02:10 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 19:02:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/30 19:02:10 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.164881 s\n",
            "23/04/30 19:02:10 INFO CodeGenerator: Code generated in 33.796646 ms\n",
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|  144336|\n",
            "+--------+\n",
            "\n",
            "None\n",
            "23/04/30 19:02:10 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/30 19:02:10 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "23/04/30 19:02:10 INFO FileSourceStrategy: Output Data Schema: struct<>\n",
            "23/04/30 19:02:11 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 351.9 KiB, free 365.5 MiB)\n",
            "23/04/30 19:02:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.5 KiB, free 365.5 MiB)\n",
            "23/04/30 19:02:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 145f628fbc4b:46421 (size: 35.5 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:02:11 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 19:02:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Registering RDD 12 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Got map stage job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[12] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:02:11 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.2 KiB, free 365.5 MiB)\n",
            "23/04/30 19:02:11 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 365.5 MiB)\n",
            "23/04/30 19:02:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 145f628fbc4b:46421 (size: 7.2 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:02:11 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[12] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:02:11 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:02:11 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4852 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:02:11 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)\n",
            "23/04/30 19:02:11 INFO FileScanRDD: Reading File path: file:///content/2021-1-weather.parquet, range: 0-3369951, partition values: [empty row]\n",
            "23/04/30 19:02:11 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 2124 bytes result sent to driver\n",
            "23/04/30 19:02:11 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 64 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:02:11 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:02:11 INFO DAGScheduler: ShuffleMapStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 0.086 s\n",
            "23/04/30 19:02:11 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 19:02:11 INFO DAGScheduler: running: Set()\n",
            "23/04/30 19:02:11 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 19:02:11 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 19:02:11 INFO CodeGenerator: Code generated in 36.552302 ms\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Registering RDD 15 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Got map stage job 4 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:02:11 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 11.8 KiB, free 365.5 MiB)\n",
            "23/04/30 19:02:11 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 365.5 MiB)\n",
            "23/04/30 19:02:11 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 145f628fbc4b:46421 (size: 6.0 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:02:11 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:02:11 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:02:11 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:02:11 INFO Executor: Running task 0.0 in stage 6.0 (TID 4)\n",
            "23/04/30 19:02:11 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 19:02:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms\n",
            "23/04/30 19:02:11 INFO Executor: Finished task 0.0 in stage 6.0 (TID 4). 2839 bytes result sent to driver\n",
            "23/04/30 19:02:11 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 24 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:02:11 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:02:11 INFO DAGScheduler: ShuffleMapStage 6 (csv at NativeMethodAccessorImpl.java:0) finished in 0.049 s\n",
            "23/04/30 19:02:11 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 19:02:11 INFO DAGScheduler: running: Set()\n",
            "23/04/30 19:02:11 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 19:02:11 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 19:02:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 19:02:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 19:02:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 19:02:11 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Got job 5 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Final stage: ResultStage 9 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Submitting ResultStage 9 (ShuffledRowRDD[16] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:02:11 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 211.2 KiB, free 365.3 MiB)\n",
            "23/04/30 19:02:11 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 75.5 KiB, free 365.2 MiB)\n",
            "23/04/30 19:02:11 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 145f628fbc4b:46421 (size: 75.5 KiB, free: 366.1 MiB)\n",
            "23/04/30 19:02:11 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (ShuffledRowRDD[16] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:02:11 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:02:11 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 5) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:02:11 INFO Executor: Running task 0.0 in stage 9.0 (TID 5)\n",
            "23/04/30 19:02:11 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 19:02:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 19:02:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 19:02:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 19:02:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 19:02:11 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 145f628fbc4b:46421 in memory (size: 6.0 KiB, free: 366.1 MiB)\n",
            "23/04/30 19:02:11 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 145f628fbc4b:46421 in memory (size: 5.5 KiB, free: 366.1 MiB)\n",
            "23/04/30 19:02:11 INFO FileOutputCommitter: Saved output of task 'attempt_202304301902111307220367332770486_0009_m_000000_5' to file:/content/localtest2.out/_temporary/0/task_202304301902111307220367332770486_0009_m_000000\n",
            "23/04/30 19:02:11 INFO SparkHadoopMapRedUtil: attempt_202304301902111307220367332770486_0009_m_000000_5: Committed\n",
            "23/04/30 19:02:12 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 145f628fbc4b:46421 in memory (size: 35.5 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:02:12 INFO Executor: Finished task 0.0 in stage 9.0 (TID 5). 3526 bytes result sent to driver\n",
            "23/04/30 19:02:12 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 5) in 464 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:02:12 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:02:12 INFO DAGScheduler: ResultStage 9 (csv at NativeMethodAccessorImpl.java:0) finished in 0.534 s\n",
            "23/04/30 19:02:12 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 19:02:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "23/04/30 19:02:12 INFO DAGScheduler: Job 5 finished: csv at NativeMethodAccessorImpl.java:0, took 0.542768 s\n",
            "23/04/30 19:02:12 INFO FileFormatWriter: Start to commit write Job e7e557cd-c9d8-4cb4-b347-da4455ba4b84.\n",
            "23/04/30 19:02:12 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 145f628fbc4b:46421 in memory (size: 7.2 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:02:12 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 145f628fbc4b:46421 in memory (size: 7.2 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:02:12 INFO FileFormatWriter: Write Job e7e557cd-c9d8-4cb4-b347-da4455ba4b84 committed. Elapsed time: 42 ms.\n",
            "23/04/30 19:02:12 INFO FileFormatWriter: Finished processing stats for write job e7e557cd-c9d8-4cb4-b347-da4455ba4b84.\n",
            "23/04/30 19:02:12 INFO SparkUI: Stopped Spark web UI at http://145f628fbc4b:4040\n",
            "23/04/30 19:02:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/30 19:02:12 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/30 19:02:12 INFO BlockManager: BlockManager stopped\n",
            "23/04/30 19:02:12 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/30 19:02:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/30 19:02:12 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/30 19:02:12 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/30 19:02:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/pyspark-68eb5c2d-aeae-4ab8-b3ad-78a509093c84\n",
            "23/04/30 19:02:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-910119cc-f1c7-4539-81b4-966157ff2d23\n",
            "23/04/30 19:02:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('localtest2.out/part-00000-053821a9-4ee2-439d-8640-ddc2605a4b05-c000.csv') as f:\n",
        "  lines = f.readlines()\n",
        "  print(lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIzcQg0meuwG",
        "outputId": "94de2dd6-e1db-4675-9e98-3cc72b81829c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['144336\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r localtest2.out"
      ],
      "metadata": {
        "id": "8cVLNIWlevZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now let's test a basic query on incidents with spark"
      ],
      "metadata": {
        "id": "dd_Xa6EDjhzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file localtest2.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('localtest2').getOrCreate()\n",
        "\n",
        "try:\n",
        "  # read in parquet file\n",
        "  # part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n",
        "  incidents_df = spark.read.parquet('incidents.parquet')\n",
        "  incidents_df.createOrReplaceTempView(\"incidents\")\n",
        "\n",
        "  # run SQL query\n",
        "  counts = spark.sql('SELECT AVG(incidents.latitude), AVG(incidents.longitude) FROM incidents')\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = counts\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"localtest2.out\")\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaDRnhPgjgTM",
        "outputId": "f6bfb7d9-d23b-4700-82ea-8366503e0916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting localtest2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 localtest2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Viyg67znfmH",
        "outputId": "30ae21a8-f873-4e45-866e-789a7e4e6d1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2ececbd4-d2c7-4511-8a6e-c4ba0a6f61ae;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 873ms :: artifacts dl 55ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-2ececbd4-d2c7-4511-8a6e-c4ba0a6f61ae\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/24ms)\n",
            "23/04/30 19:27:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/30 19:27:52 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/04/30 19:27:52 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 19:27:52 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/30 19:27:52 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 19:27:52 INFO SparkContext: Submitted application: localtest2\n",
            "23/04/30 19:27:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/30 19:27:52 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/30 19:27:52 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/30 19:27:52 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/30 19:27:52 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/30 19:27:52 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/30 19:27:52 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/30 19:27:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/30 19:27:52 INFO Utils: Successfully started service 'sparkDriver' on port 41867.\n",
            "23/04/30 19:27:52 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/30 19:27:53 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/30 19:27:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/30 19:27:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/30 19:27:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/30 19:27:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-83a718a4-7ca2-4aa7-8e4a-c6b69ddf4507\n",
            "23/04/30 19:27:53 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/30 19:27:53 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/30 19:27:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/30 19:27:53 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://145f628fbc4b:4040\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://145f628fbc4b:41867/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://145f628fbc4b:41867/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://145f628fbc4b:41867/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://145f628fbc4b:41867/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://145f628fbc4b:41867/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://145f628fbc4b:41867/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://145f628fbc4b:41867/jars/com.101tec_zkclient-0.3.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://145f628fbc4b:41867/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://145f628fbc4b:41867/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://145f628fbc4b:41867/jars/log4j_log4j-1.2.17.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://145f628fbc4b:41867/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://145f628fbc4b:41867/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/log4j_log4j-1.2.17.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 19:27:54 INFO Executor: Starting executor ID driver on host 145f628fbc4b\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/log4j_log4j-1.2.17.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO TransportClientFactory: Successfully created connection to 145f628fbc4b/172.28.0.12:41867 after 78 ms (0 ms spent in bootstraps)\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp7141776552641971921.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp7141776552641971921.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp4148912846098409607.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp4148912846098409607.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp1950556471998117991.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp1950556471998117991.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp827791910842979561.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp827791910842979561.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp8628034061007495248.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp8628034061007495248.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp980947152333789592.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp980947152333789592.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/com.101tec_zkclient-0.3.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp9215188553534959149.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp9215188553534959149.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp1892133265186533642.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp1892133265186533642.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp993965001569242677.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp993965001569242677.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp1300540902745574788.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp1300540902745574788.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp584258419599847395.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp584258419599847395.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/log4j_log4j-1.2.17.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/log4j_log4j-1.2.17.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp5166325385106947539.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp5166325385106947539.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/log4j_log4j-1.2.17.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/30 19:27:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40335.\n",
            "23/04/30 19:27:54 INFO NettyBlockTransferService: Server created on 145f628fbc4b:40335\n",
            "23/04/30 19:27:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/30 19:27:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 145f628fbc4b, 40335, None)\n",
            "23/04/30 19:27:54 INFO BlockManagerMasterEndpoint: Registering block manager 145f628fbc4b:40335 with 366.3 MiB RAM, BlockManagerId(driver, 145f628fbc4b, 40335, None)\n",
            "23/04/30 19:27:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 145f628fbc4b, 40335, None)\n",
            "23/04/30 19:27:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 145f628fbc4b, 40335, None)\n",
            "23/04/30 19:27:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/04/30 19:27:55 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/04/30 19:27:57 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.\n",
            "23/04/30 19:27:58 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 19:27:58 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:27:58 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:27:58 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 19:27:58 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:27:58 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:27:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/04/30 19:27:59 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/04/30 19:27:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 145f628fbc4b:40335 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 19:27:59 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:27:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:27:59 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:27:59 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4579 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:27:59 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/30 19:28:01 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2231 bytes result sent to driver\n",
            "23/04/30 19:28:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2186 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:28:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:28:01 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.787 s\n",
            "23/04/30 19:28:01 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 19:28:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/30 19:28:01 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.997514 s\n",
            "23/04/30 19:28:02 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 145f628fbc4b:40335 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 19:28:05 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/30 19:28:05 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "23/04/30 19:28:05 INFO FileSourceStrategy: Output Data Schema: struct<latitude: double, longitude: double>\n",
            "23/04/30 19:28:06 INFO CodeGenerator: Code generated in 336.123365 ms\n",
            "23/04/30 19:28:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 352.5 KiB, free 366.0 MiB)\n",
            "23/04/30 19:28:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.5 KiB, free 365.9 MiB)\n",
            "23/04/30 19:28:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 145f628fbc4b:40335 (size: 35.5 KiB, free: 366.3 MiB)\n",
            "23/04/30 19:28:06 INFO SparkContext: Created broadcast 1 from csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 19:28:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/30 19:28:06 INFO DAGScheduler: Registering RDD 5 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/30 19:28:06 INFO DAGScheduler: Got map stage job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:28:06 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:28:06 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 19:28:06 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:28:06 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:28:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 20.1 KiB, free 365.9 MiB)\n",
            "23/04/30 19:28:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 365.9 MiB)\n",
            "23/04/30 19:28:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 145f628fbc4b:40335 (size: 8.8 KiB, free: 366.3 MiB)\n",
            "23/04/30 19:28:07 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:28:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:28:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:28:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4847 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:28:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/30 19:28:07 INFO FileScanRDD: Reading File path: file:///content/incidents.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/30 19:28:07 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/04/30 19:28:07 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2167 bytes result sent to driver\n",
            "23/04/30 19:28:07 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 740 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:28:07 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:28:07 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 0.830 s\n",
            "23/04/30 19:28:07 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 19:28:07 INFO DAGScheduler: running: Set()\n",
            "23/04/30 19:28:07 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 19:28:07 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 19:28:07 INFO CodeGenerator: Code generated in 43.53524 ms\n",
            "23/04/30 19:28:07 INFO DAGScheduler: Registering RDD 8 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/30 19:28:07 INFO DAGScheduler: Got map stage job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:28:07 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:28:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/30 19:28:07 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:28:07 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[8] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:28:07 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 365.9 MiB)\n",
            "23/04/30 19:28:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 365.9 MiB)\n",
            "23/04/30 19:28:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 145f628fbc4b:40335 (size: 7.3 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:28:07 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:28:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[8] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:28:07 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:28:07 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:28:07 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
            "23/04/30 19:28:08 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 19:28:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 34 ms\n",
            "23/04/30 19:28:08 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 2839 bytes result sent to driver\n",
            "23/04/30 19:28:08 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 115 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:28:08 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:28:08 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 0.170 s\n",
            "23/04/30 19:28:08 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 19:28:08 INFO DAGScheduler: running: Set()\n",
            "23/04/30 19:28:08 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 19:28:08 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 19:28:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 19:28:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 19:28:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 19:28:08 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 19:28:08 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:28:08 INFO DAGScheduler: Final stage: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:28:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
            "23/04/30 19:28:08 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:28:08 INFO DAGScheduler: Submitting ResultStage 6 (ShuffledRowRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:28:08 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)\n",
            "23/04/30 19:28:08 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 75.5 KiB, free 365.6 MiB)\n",
            "23/04/30 19:28:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 145f628fbc4b:40335 (size: 75.5 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:28:08 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:28:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (ShuffledRowRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:28:08 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:28:08 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 3) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:28:08 INFO Executor: Running task 0.0 in stage 6.0 (TID 3)\n",
            "23/04/30 19:28:08 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 19:28:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "23/04/30 19:28:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 19:28:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 19:28:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 19:28:08 INFO FileOutputCommitter: Saved output of task 'attempt_202304301928086813574559748560989_0006_m_000000_3' to file:/content/localtest2.out/_temporary/0/task_202304301928086813574559748560989_0006_m_000000\n",
            "23/04/30 19:28:08 INFO SparkHadoopMapRedUtil: attempt_202304301928086813574559748560989_0006_m_000000_3: Committed\n",
            "23/04/30 19:28:08 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 145f628fbc4b:40335 in memory (size: 8.8 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:28:08 INFO Executor: Finished task 0.0 in stage 6.0 (TID 3). 3526 bytes result sent to driver\n",
            "23/04/30 19:28:08 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 3) in 319 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:28:08 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:28:08 INFO DAGScheduler: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0) finished in 0.364 s\n",
            "23/04/30 19:28:08 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 19:28:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "23/04/30 19:28:08 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 0.382930 s\n",
            "23/04/30 19:28:08 INFO FileFormatWriter: Start to commit write Job b598bb27-ce27-47b4-9a67-20e7977b1788.\n",
            "23/04/30 19:28:08 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 145f628fbc4b:40335 in memory (size: 7.3 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:28:08 INFO FileFormatWriter: Write Job b598bb27-ce27-47b4-9a67-20e7977b1788 committed. Elapsed time: 27 ms.\n",
            "23/04/30 19:28:08 INFO FileFormatWriter: Finished processing stats for write job b598bb27-ce27-47b4-9a67-20e7977b1788.\n",
            "23/04/30 19:28:08 INFO SparkUI: Stopped Spark web UI at http://145f628fbc4b:4040\n",
            "23/04/30 19:28:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/30 19:28:08 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/30 19:28:08 INFO BlockManager: BlockManager stopped\n",
            "23/04/30 19:28:08 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/30 19:28:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/30 19:28:08 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/30 19:28:08 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/30 19:28:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2\n",
            "23/04/30 19:28:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/pyspark-cd5ee225-d03c-4b41-bf5a-ce528258209c\n",
            "23/04/30 19:28:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-15ca1184-f4d2-4383-9cc5-adc8e07f6b43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/localtest2.out/part-00000-fd44653f-34a7-4bd8-837e-c374b301f17e-c000.csv') as f:\n",
        "  lines = f.readlines()\n",
        "  print(lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8eqcQAKnIRu",
        "outputId": "0041aa9b-7b05-4b00-a52a-b843dddbaae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['36.151854061679494,-86.74292086389275\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r localtest2.out"
      ],
      "metadata": {
        "id": "96oBkNtNoHSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now we know that both of our datasets can interact with Spark\n",
        "\n",
        "### Let's now try to extract the hour from the incident data to make sure that we can do it in Spark"
      ],
      "metadata": {
        "id": "PqFst6cAnwmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file localtest2.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('localtest2').getOrCreate()\n",
        "\n",
        "try:\n",
        "  # read in parquet file\n",
        "  # part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n",
        "  incidents_df = spark.read.parquet('incidents.parquet')\n",
        "  incidents_df.createOrReplaceTempView(\"incidents\")\n",
        "\n",
        "  # run SQL query\n",
        "  # OLD QUERY: 'SELECT HOUR(FROM_UNIXTIME(time_utc)), COUNT(incident_id) FROM \"incidents\" GROUP BY HOUR(FROM_UNIXTIME(time_utc)) ORDER BY COUNT(incident_id) DESC;'\n",
        "  counts = spark.sql('SELECT HOUR(time_local), COUNT(incident_id) FROM incidents GROUP BY HOUR(time_local) ORDER BY COUNT(incident_id) DESC;')\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = counts\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"localtest2.out\")\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prZo9ownoB3J",
        "outputId": "e473fcbb-adf7-4a64-89c7-156dde695c5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting localtest2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 localtest2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0tWfj7Hqrx8",
        "outputId": "c96317a8-c464-40ab-a7ed-2fbbc6505565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-abbaf799-0461-4afe-a7e7-aef961b92116;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 826ms :: artifacts dl 27ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-abbaf799-0461-4afe-a7e7-aef961b92116\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/24ms)\n",
            "23/04/30 22:04:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/30 22:04:55 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/04/30 22:04:56 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 22:04:56 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/30 22:04:56 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 22:04:56 INFO SparkContext: Submitted application: localtest2\n",
            "23/04/30 22:04:56 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/30 22:04:56 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/30 22:04:56 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/30 22:04:56 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/30 22:04:56 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/30 22:04:56 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/30 22:04:56 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/30 22:04:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/30 22:04:56 INFO Utils: Successfully started service 'sparkDriver' on port 35263.\n",
            "23/04/30 22:04:56 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/30 22:04:56 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/30 22:04:56 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/30 22:04:56 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/30 22:04:56 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/30 22:04:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c36bee47-13c1-470c-ba5d-58f0a48a8510\n",
            "23/04/30 22:04:56 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/30 22:04:56 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/30 22:04:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/30 22:04:57 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://145f628fbc4b:4040\n",
            "23/04/30 22:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://145f628fbc4b:35263/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://145f628fbc4b:35263/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://145f628fbc4b:35263/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://145f628fbc4b:35263/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://145f628fbc4b:35263/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://145f628fbc4b:35263/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://145f628fbc4b:35263/jars/com.101tec_zkclient-0.3.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://145f628fbc4b:35263/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://145f628fbc4b:35263/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://145f628fbc4b:35263/jars/log4j_log4j-1.2.17.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://145f628fbc4b:35263/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://145f628fbc4b:35263/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 22:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 22:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 22:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 22:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 22:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 22:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 22:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 22:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 22:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/log4j_log4j-1.2.17.jar\n",
            "23/04/30 22:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 22:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 22:04:57 INFO Executor: Starting executor ID driver on host 145f628fbc4b\n",
            "23/04/30 22:04:57 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:57 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/log4j_log4j-1.2.17.jar\n",
            "23/04/30 22:04:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 22:04:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 22:04:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 22:04:58 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 22:04:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 22:04:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 22:04:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 22:04:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 22:04:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 22:04:58 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 22:04:58 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 22:04:58 INFO Executor: Fetching spark://145f628fbc4b:35263/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO TransportClientFactory: Successfully created connection to 145f628fbc4b/172.28.0.12:35263 after 69 ms (0 ms spent in bootstraps)\n",
            "23/04/30 22:04:58 INFO Utils: Fetching spark://145f628fbc4b:35263/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp8096977016212171189.tmp\n",
            "23/04/30 22:04:58 INFO Utils: /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp8096977016212171189.tmp has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 22:04:58 INFO Executor: Adding file:/tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/30 22:04:58 INFO Executor: Fetching spark://145f628fbc4b:35263/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: Fetching spark://145f628fbc4b:35263/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp8243151944586017489.tmp\n",
            "23/04/30 22:04:58 INFO Utils: /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp8243151944586017489.tmp has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 22:04:58 INFO Executor: Adding file:/tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/30 22:04:58 INFO Executor: Fetching spark://145f628fbc4b:35263/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: Fetching spark://145f628fbc4b:35263/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp6904389983586492357.tmp\n",
            "23/04/30 22:04:58 INFO Utils: /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp6904389983586492357.tmp has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 22:04:58 INFO Executor: Adding file:/tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/30 22:04:58 INFO Executor: Fetching spark://145f628fbc4b:35263/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: Fetching spark://145f628fbc4b:35263/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp6890590801127671261.tmp\n",
            "23/04/30 22:04:58 INFO Utils: /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp6890590801127671261.tmp has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 22:04:58 INFO Executor: Adding file:/tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/30 22:04:58 INFO Executor: Fetching spark://145f628fbc4b:35263/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: Fetching spark://145f628fbc4b:35263/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp7642617798774556713.tmp\n",
            "23/04/30 22:04:58 INFO Utils: /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp7642617798774556713.tmp has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 22:04:58 INFO Executor: Adding file:/tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/30 22:04:58 INFO Executor: Fetching spark://145f628fbc4b:35263/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: Fetching spark://145f628fbc4b:35263/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp1754408803861192756.tmp\n",
            "23/04/30 22:04:58 INFO Utils: /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp1754408803861192756.tmp has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 22:04:58 INFO Executor: Adding file:/tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/30 22:04:58 INFO Executor: Fetching spark://145f628fbc4b:35263/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: Fetching spark://145f628fbc4b:35263/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp8404324463058665888.tmp\n",
            "23/04/30 22:04:58 INFO Utils: /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp8404324463058665888.tmp has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 22:04:58 INFO Executor: Adding file:/tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/30 22:04:58 INFO Executor: Fetching spark://145f628fbc4b:35263/jars/log4j_log4j-1.2.17.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: Fetching spark://145f628fbc4b:35263/jars/log4j_log4j-1.2.17.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp8125764615064827199.tmp\n",
            "23/04/30 22:04:58 INFO Utils: /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp8125764615064827199.tmp has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/log4j_log4j-1.2.17.jar\n",
            "23/04/30 22:04:58 INFO Executor: Adding file:/tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/30 22:04:58 INFO Executor: Fetching spark://145f628fbc4b:35263/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: Fetching spark://145f628fbc4b:35263/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp1068538292721467440.tmp\n",
            "23/04/30 22:04:58 INFO Utils: /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp1068538292721467440.tmp has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 22:04:58 INFO Executor: Adding file:/tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/30 22:04:58 INFO Executor: Fetching spark://145f628fbc4b:35263/jars/com.101tec_zkclient-0.3.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: Fetching spark://145f628fbc4b:35263/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp3903172649580866134.tmp\n",
            "23/04/30 22:04:58 INFO Utils: /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp3903172649580866134.tmp has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 22:04:58 INFO Executor: Adding file:/tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/30 22:04:58 INFO Executor: Fetching spark://145f628fbc4b:35263/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: Fetching spark://145f628fbc4b:35263/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp1887509670139846082.tmp\n",
            "23/04/30 22:04:58 INFO Utils: /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp1887509670139846082.tmp has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 22:04:58 INFO Executor: Adding file:/tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/30 22:04:58 INFO Executor: Fetching spark://145f628fbc4b:35263/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682892295963\n",
            "23/04/30 22:04:58 INFO Utils: Fetching spark://145f628fbc4b:35263/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp5925092807090875513.tmp\n",
            "23/04/30 22:04:58 INFO Utils: /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/fetchFileTemp5925092807090875513.tmp has been previously copied to /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 22:04:58 INFO Executor: Adding file:/tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/userFiles-4b0831a6-5f75-4645-bab7-e48b8791ab10/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/30 22:04:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36423.\n",
            "23/04/30 22:04:58 INFO NettyBlockTransferService: Server created on 145f628fbc4b:36423\n",
            "23/04/30 22:04:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/30 22:04:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 145f628fbc4b, 36423, None)\n",
            "23/04/30 22:04:58 INFO BlockManagerMasterEndpoint: Registering block manager 145f628fbc4b:36423 with 366.3 MiB RAM, BlockManagerId(driver, 145f628fbc4b, 36423, None)\n",
            "23/04/30 22:04:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 145f628fbc4b, 36423, None)\n",
            "23/04/30 22:04:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 145f628fbc4b, 36423, None)\n",
            "23/04/30 22:04:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/04/30 22:04:59 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/04/30 22:05:01 INFO InMemoryFileIndex: It took 103 ms to list leaf files for 1 paths.\n",
            "23/04/30 22:05:02 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 22:05:02 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 22:05:02 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 22:05:02 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 22:05:02 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 22:05:02 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 22:05:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/04/30 22:05:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.5 KiB, free 366.2 MiB)\n",
            "23/04/30 22:05:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 145f628fbc4b:36423 (size: 37.5 KiB, free: 366.3 MiB)\n",
            "23/04/30 22:05:02 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 22:05:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 22:05:02 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/30 22:05:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4579 bytes) taskResourceAssignments Map()\n",
            "23/04/30 22:05:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/30 22:05:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2274 bytes result sent to driver\n",
            "23/04/30 22:05:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1143 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 22:05:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/30 22:05:04 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.812 s\n",
            "23/04/30 22:05:04 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 22:05:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/30 22:05:04 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.968861 s\n",
            "23/04/30 22:05:05 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 145f628fbc4b:36423 in memory (size: 37.5 KiB, free: 366.3 MiB)\n",
            "23/04/30 22:05:08 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/30 22:05:08 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "23/04/30 22:05:08 INFO FileSourceStrategy: Output Data Schema: struct<time_local: timestamp, Incident_ID: int>\n",
            "23/04/30 22:05:08 INFO CodeGenerator: Code generated in 302.607469 ms\n",
            "23/04/30 22:05:08 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 352.5 KiB, free 366.0 MiB)\n",
            "23/04/30 22:05:08 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 365.9 MiB)\n",
            "23/04/30 22:05:08 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 145f628fbc4b:36423 (size: 35.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 22:05:08 INFO SparkContext: Created broadcast 1 from csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 22:05:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/30 22:05:09 INFO DAGScheduler: Registering RDD 5 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/30 22:05:09 INFO DAGScheduler: Got map stage job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 22:05:09 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 22:05:09 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 22:05:09 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 22:05:09 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 22:05:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 37.4 KiB, free 365.9 MiB)\n",
            "23/04/30 22:05:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 365.9 MiB)\n",
            "23/04/30 22:05:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 145f628fbc4b:36423 (size: 17.1 KiB, free: 366.2 MiB)\n",
            "23/04/30 22:05:09 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 22:05:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 22:05:09 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/04/30 22:05:09 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4847 bytes) taskResourceAssignments Map()\n",
            "23/04/30 22:05:09 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/30 22:05:09 INFO CodeGenerator: Code generated in 51.431019 ms\n",
            "23/04/30 22:05:09 INFO CodeGenerator: Code generated in 27.815058 ms\n",
            "23/04/30 22:05:09 INFO CodeGenerator: Code generated in 11.897637 ms\n",
            "23/04/30 22:05:09 INFO CodeGenerator: Code generated in 11.934453 ms\n",
            "23/04/30 22:05:09 INFO FileScanRDD: Reading File path: file:///content/incidents.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/30 22:05:09 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/04/30 22:05:10 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 3013 bytes result sent to driver\n",
            "23/04/30 22:05:10 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1185 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 22:05:10 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/30 22:05:10 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 1.227 s\n",
            "23/04/30 22:05:10 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 22:05:10 INFO DAGScheduler: running: Set()\n",
            "23/04/30 22:05:10 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 22:05:10 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 22:05:10 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/30 22:05:10 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/30 22:05:10 INFO CodeGenerator: Code generated in 55.198767 ms\n",
            "23/04/30 22:05:10 INFO CodeGenerator: Code generated in 30.502635 ms\n",
            "23/04/30 22:05:10 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 22:05:10 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 22:05:10 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 22:05:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/30 22:05:10 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 22:05:10 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 22:05:10 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 36.8 KiB, free 365.8 MiB)\n",
            "23/04/30 22:05:10 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 365.8 MiB)\n",
            "23/04/30 22:05:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 145f628fbc4b:36423 (size: 17.4 KiB, free: 366.2 MiB)\n",
            "23/04/30 22:05:10 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 22:05:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 22:05:10 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/30 22:05:10 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/30 22:05:10 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
            "23/04/30 22:05:10 INFO ShuffleBlockFetcherIterator: Getting 1 (1474.0 B) non-empty blocks including 1 (1474.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 22:05:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms\n",
            "23/04/30 22:05:10 INFO CodeGenerator: Code generated in 21.933345 ms\n",
            "23/04/30 22:05:10 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 4936 bytes result sent to driver\n",
            "23/04/30 22:05:10 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 151 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 22:05:10 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/30 22:05:10 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 0.176 s\n",
            "23/04/30 22:05:10 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 22:05:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/30 22:05:10 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.211388 s\n",
            "23/04/30 22:05:10 INFO DAGScheduler: Registering RDD 11 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/30 22:05:10 INFO DAGScheduler: Got map stage job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 22:05:10 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 22:05:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
            "23/04/30 22:05:10 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 22:05:10 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[11] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 22:05:10 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 37.7 KiB, free 365.8 MiB)\n",
            "23/04/30 22:05:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 17.7 KiB, free 365.8 MiB)\n",
            "23/04/30 22:05:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 145f628fbc4b:36423 (size: 17.7 KiB, free: 366.2 MiB)\n",
            "23/04/30 22:05:11 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 22:05:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[11] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 22:05:11 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "23/04/30 22:05:11 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/30 22:05:11 INFO Executor: Running task 0.0 in stage 5.0 (TID 3)\n",
            "23/04/30 22:05:11 INFO ShuffleBlockFetcherIterator: Getting 1 (1474.0 B) non-empty blocks including 1 (1474.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 22:05:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 22:05:11 INFO Executor: Finished task 0.0 in stage 5.0 (TID 3). 4199 bytes result sent to driver\n",
            "23/04/30 22:05:11 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 87 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 22:05:11 INFO DAGScheduler: ShuffleMapStage 5 (csv at NativeMethodAccessorImpl.java:0) finished in 0.117 s\n",
            "23/04/30 22:05:11 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 22:05:11 INFO DAGScheduler: running: Set()\n",
            "23/04/30 22:05:11 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 22:05:11 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 22:05:11 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "23/04/30 22:05:11 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/30 22:05:11 INFO CodeGenerator: Code generated in 33.795548 ms\n",
            "23/04/30 22:05:11 INFO DAGScheduler: Registering RDD 14 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
            "23/04/30 22:05:11 INFO DAGScheduler: Got map stage job 4 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 22:05:11 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 22:05:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
            "23/04/30 22:05:11 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 22:05:11 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[14] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 22:05:11 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 35.3 KiB, free 365.7 MiB)\n",
            "23/04/30 22:05:11 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 365.7 MiB)\n",
            "23/04/30 22:05:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 145f628fbc4b:36423 (size: 16.8 KiB, free: 366.2 MiB)\n",
            "23/04/30 22:05:11 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 22:05:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[14] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 22:05:11 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "23/04/30 22:05:11 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 4) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/30 22:05:11 INFO Executor: Running task 0.0 in stage 8.0 (TID 4)\n",
            "23/04/30 22:05:11 INFO ShuffleBlockFetcherIterator: Getting 1 (1578.0 B) non-empty blocks including 1 (1578.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 22:05:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
            "23/04/30 22:05:11 INFO CodeGenerator: Code generated in 13.206722 ms\n",
            "23/04/30 22:05:11 INFO CodeGenerator: Code generated in 21.171797 ms\n",
            "23/04/30 22:05:11 INFO Executor: Finished task 0.0 in stage 8.0 (TID 4). 5085 bytes result sent to driver\n",
            "23/04/30 22:05:11 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 4) in 105 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 22:05:11 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "23/04/30 22:05:11 INFO DAGScheduler: ShuffleMapStage 8 (csv at NativeMethodAccessorImpl.java:0) finished in 0.130 s\n",
            "23/04/30 22:05:11 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 22:05:11 INFO DAGScheduler: running: Set()\n",
            "23/04/30 22:05:11 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 22:05:11 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 22:05:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 22:05:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 22:05:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 22:05:11 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 22:05:11 INFO DAGScheduler: Got job 5 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 22:05:11 INFO DAGScheduler: Final stage: ResultStage 12 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 22:05:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\n",
            "23/04/30 22:05:11 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 22:05:11 INFO DAGScheduler: Submitting ResultStage 12 (ShuffledRowRDD[15] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 22:05:11 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 211.3 KiB, free 365.5 MiB)\n",
            "23/04/30 22:05:11 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 75.6 KiB, free 365.4 MiB)\n",
            "23/04/30 22:05:11 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 145f628fbc4b:36423 (size: 75.6 KiB, free: 366.1 MiB)\n",
            "23/04/30 22:05:11 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 22:05:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (ShuffledRowRDD[15] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 22:05:11 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
            "23/04/30 22:05:11 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 5) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/30 22:05:11 INFO Executor: Running task 0.0 in stage 12.0 (TID 5)\n",
            "23/04/30 22:05:11 INFO ShuffleBlockFetcherIterator: Getting 1 (368.0 B) non-empty blocks including 1 (368.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 22:05:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 22:05:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 22:05:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 22:05:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 22:05:11 INFO FileOutputCommitter: Saved output of task 'attempt_202304302205115099684577206333748_0012_m_000000_5' to file:/content/localtest2.out/_temporary/0/task_202304302205115099684577206333748_0012_m_000000\n",
            "23/04/30 22:05:11 INFO SparkHadoopMapRedUtil: attempt_202304302205115099684577206333748_0012_m_000000_5: Committed\n",
            "23/04/30 22:05:11 INFO Executor: Finished task 0.0 in stage 12.0 (TID 5). 3483 bytes result sent to driver\n",
            "23/04/30 22:05:11 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 5) in 167 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 22:05:11 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
            "23/04/30 22:05:11 INFO DAGScheduler: ResultStage 12 (csv at NativeMethodAccessorImpl.java:0) finished in 0.220 s\n",
            "23/04/30 22:05:11 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 22:05:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
            "23/04/30 22:05:11 INFO DAGScheduler: Job 5 finished: csv at NativeMethodAccessorImpl.java:0, took 0.231515 s\n",
            "23/04/30 22:05:11 INFO FileFormatWriter: Start to commit write Job 3f805cf8-e490-4058-a6c1-ec3fd0c4ef1a.\n",
            "23/04/30 22:05:11 INFO FileFormatWriter: Write Job 3f805cf8-e490-4058-a6c1-ec3fd0c4ef1a committed. Elapsed time: 19 ms.\n",
            "23/04/30 22:05:11 INFO FileFormatWriter: Finished processing stats for write job 3f805cf8-e490-4058-a6c1-ec3fd0c4ef1a.\n",
            "23/04/30 22:05:11 INFO SparkUI: Stopped Spark web UI at http://145f628fbc4b:4040\n",
            "23/04/30 22:05:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/30 22:05:11 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/30 22:05:11 INFO BlockManager: BlockManager stopped\n",
            "23/04/30 22:05:11 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/30 22:05:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/30 22:05:11 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/30 22:05:12 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/30 22:05:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-52373944-8adf-4ddb-b074-5a30778e01a3\n",
            "23/04/30 22:05:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb/pyspark-bd4c8121-a67b-41b8-8b87-538b0d65b503\n",
            "23/04/30 22:05:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-240307ff-d771-4ec8-895f-474484a943eb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/localtest2.out/part-00000-22e40c7b-e802-4fa3-bfbe-108bd3cc0e73-c000.csv') as f:\n",
        "  lines = f.readlines()\n",
        "  print(lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dbhvt5LpsFaa",
        "outputId": "d2ad4309-eb42-4a1b-a730-8e495700752c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['17,2303\\n', '16,2081\\n', '15,1984\\n', '18,1874\\n', '14,1670\\n', '13,1613\\n', '12,1609\\n', '19,1554\\n', '11,1403\\n', '20,1307\\n', '10,1231\\n', '21,1197\\n', '7,1190\\n', '8,1163\\n', '9,1105\\n', '22,1032\\n', '6,931\\n', '23,878\\n', '0,661\\n', '5,659\\n', '2,638\\n', '1,631\\n', '3,582\\n', '4,469\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set(style='ticks')\n",
        "time_hist = pd.read_csv('/content/localtest2.out/part-00000-22e40c7b-e802-4fa3-bfbe-108bd3cc0e73-c000.csv', header=None, names=['_col0', '_col1'])\n",
        "time_hist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "id": "ECZZVI64sRH2",
        "outputId": "dafc7157-a8e9-4d30-a452-f1db1c5f7cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    _col0  _col1\n",
              "0      17   2303\n",
              "1      16   2081\n",
              "2      15   1984\n",
              "3      18   1874\n",
              "4      14   1670\n",
              "5      13   1613\n",
              "6      12   1609\n",
              "7      19   1554\n",
              "8      11   1403\n",
              "9      20   1307\n",
              "10     10   1231\n",
              "11     21   1197\n",
              "12      7   1190\n",
              "13      8   1163\n",
              "14      9   1105\n",
              "15     22   1032\n",
              "16      6    931\n",
              "17     23    878\n",
              "18      0    661\n",
              "19      5    659\n",
              "20      2    638\n",
              "21      1    631\n",
              "22      3    582\n",
              "23      4    469"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8fc3f97-d246-4290-8f5e-a031d272f02b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_col0</th>\n",
              "      <th>_col1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17</td>\n",
              "      <td>2303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16</td>\n",
              "      <td>2081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>15</td>\n",
              "      <td>1984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18</td>\n",
              "      <td>1874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14</td>\n",
              "      <td>1670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>13</td>\n",
              "      <td>1613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>12</td>\n",
              "      <td>1609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>19</td>\n",
              "      <td>1554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>11</td>\n",
              "      <td>1403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>20</td>\n",
              "      <td>1307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>1231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>21</td>\n",
              "      <td>1197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>7</td>\n",
              "      <td>1190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>8</td>\n",
              "      <td>1163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>9</td>\n",
              "      <td>1105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>22</td>\n",
              "      <td>1032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6</td>\n",
              "      <td>931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>23</td>\n",
              "      <td>878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>5</td>\n",
              "      <td>659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2</td>\n",
              "      <td>638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1</td>\n",
              "      <td>631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3</td>\n",
              "      <td>582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>4</td>\n",
              "      <td>469</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8fc3f97-d246-4290-8f5e-a031d272f02b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c8fc3f97-d246-4290-8f5e-a031d272f02b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c8fc3f97-d246-4290-8f5e-a031d272f02b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(11.7, 8.27)\n",
        "sns.barplot(data=time_hist, x=\"_col0\", y=\"_col1\", color='steelblue', order=list(range(0, 24))).set(\n",
        "    title='Distribution of Incidents by Hour of Day', xlabel='Hour of Day', ylabel='Total Count')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "id": "tFW5Jivfsqdv",
        "outputId": "c67c640a-ea53-43ae-d7b5-28939c4bfdba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0.5, 1.0, 'Distribution of Incidents by Hour of Day'),\n",
              " Text(0.5, 0, 'Hour of Day'),\n",
              " Text(0, 0.5, 'Total Count')]"
            ]
          },
          "metadata": {},
          "execution_count": 155
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1170x827 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Great, we can clearly extract the hours from each...\n",
        "\n",
        "Now, we're going to need to extract ONLY ONE element of the weather from a given bucket... let's see if we can grab the first weather measurement from January 2021, since that should be easier and we'll know we did it correctly if we get a single result\n",
        "\n",
        "Note: Right off the bat, I think that LIMIT 1 will be the route that we should take"
      ],
      "metadata": {
        "id": "iVpkGFUVu6py"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WHEN WE JOIN: NEED TO USE UTC TIME INSTEAD OF LOCAL FOR INCIDENTS...\n",
        "\n",
        "### Then, when we plot, we can use the same incident's local time to convert"
      ],
      "metadata": {
        "id": "lLsFa2UNxiS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file localtest2.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('localtest2').getOrCreate()\n",
        "\n",
        "try:\n",
        "  # read in parquet file\n",
        "  # part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n",
        "  incidents_df = spark.read.parquet('2021-1-weather.parquet')\n",
        "  incidents_df.createOrReplaceTempView(\"weather\")\n",
        "\n",
        "  # run SQL query\n",
        "  # OLD QUERY: 'SELECT HOUR(FROM_UNIXTIME(time_utc)), COUNT(incident_id) FROM \"incidents\" GROUP BY HOUR(FROM_UNIXTIME(time_utc)) ORDER BY COUNT(incident_id) DESC;'\n",
        "  counts = spark.sql('SELECT DISTINCT HOUR(weather.timestamp_local) AS hour, station_id, precip, temp FROM weather WHERE station_id = \"KBNA\" ORDER BY hour ASC LIMIT 1;')\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = counts\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"localtest2.out\")\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv7GH9sDuXv0",
        "outputId": "2fa36378-364a-4d2f-a9eb-4d51de247707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting localtest2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 localtest2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X79_FHyVymQT",
        "outputId": "66a01120-9295-4781-b1c6-be584739d0cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-32320c2b-35f3-40ce-83ab-27c7eaddde2e;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 913ms :: artifacts dl 47ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-32320c2b-35f3-40ce-83ab-27c7eaddde2e\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/13ms)\n",
            "23/04/30 20:20:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/30 20:20:07 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/04/30 20:20:07 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 20:20:07 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/30 20:20:07 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 20:20:07 INFO SparkContext: Submitted application: localtest2\n",
            "23/04/30 20:20:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/30 20:20:07 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/30 20:20:07 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/30 20:20:07 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/30 20:20:07 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/30 20:20:07 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/30 20:20:07 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/30 20:20:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/30 20:20:07 INFO Utils: Successfully started service 'sparkDriver' on port 45401.\n",
            "23/04/30 20:20:07 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/30 20:20:07 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/30 20:20:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/30 20:20:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/30 20:20:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/30 20:20:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-05e759f6-fd51-4417-a4bd-da75b90b36bf\n",
            "23/04/30 20:20:08 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/30 20:20:08 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/30 20:20:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/30 20:20:08 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://145f628fbc4b:4040\n",
            "23/04/30 20:20:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://145f628fbc4b:45401/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://145f628fbc4b:45401/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://145f628fbc4b:45401/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://145f628fbc4b:45401/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://145f628fbc4b:45401/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://145f628fbc4b:45401/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://145f628fbc4b:45401/jars/com.101tec_zkclient-0.3.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://145f628fbc4b:45401/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://145f628fbc4b:45401/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://145f628fbc4b:45401/jars/log4j_log4j-1.2.17.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://145f628fbc4b:45401/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://145f628fbc4b:45401/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 20:20:08 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 20:20:08 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 20:20:08 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 20:20:08 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 20:20:08 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 20:20:08 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 20:20:08 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 20:20:08 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 20:20:08 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/log4j_log4j-1.2.17.jar\n",
            "23/04/30 20:20:08 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 20:20:08 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:08 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 20:20:09 INFO Executor: Starting executor ID driver on host 145f628fbc4b\n",
            "23/04/30 20:20:09 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:09 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/log4j_log4j-1.2.17.jar\n",
            "23/04/30 20:20:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:09 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 20:20:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:09 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 20:20:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:09 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 20:20:09 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:09 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 20:20:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:09 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 20:20:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:09 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 20:20:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:09 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 20:20:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:09 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 20:20:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:09 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 20:20:09 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:09 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 20:20:09 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:09 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 20:20:09 INFO Executor: Fetching spark://145f628fbc4b:45401/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:09 INFO TransportClientFactory: Successfully created connection to 145f628fbc4b/172.28.0.12:45401 after 144 ms (0 ms spent in bootstraps)\n",
            "23/04/30 20:20:09 INFO Utils: Fetching spark://145f628fbc4b:45401/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp9206845340186801733.tmp\n",
            "23/04/30 20:20:09 INFO Utils: /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp9206845340186801733.tmp has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 20:20:09 INFO Executor: Adding file:/tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/30 20:20:09 INFO Executor: Fetching spark://145f628fbc4b:45401/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:09 INFO Utils: Fetching spark://145f628fbc4b:45401/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp8215610495149902390.tmp\n",
            "23/04/30 20:20:10 INFO Utils: /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp8215610495149902390.tmp has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 20:20:10 INFO Executor: Adding file:/tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/30 20:20:10 INFO Executor: Fetching spark://145f628fbc4b:45401/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:10 INFO Utils: Fetching spark://145f628fbc4b:45401/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp7412037709768844471.tmp\n",
            "23/04/30 20:20:10 INFO Utils: /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp7412037709768844471.tmp has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 20:20:10 INFO Executor: Adding file:/tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/30 20:20:10 INFO Executor: Fetching spark://145f628fbc4b:45401/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:10 INFO Utils: Fetching spark://145f628fbc4b:45401/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp7836614385587376716.tmp\n",
            "23/04/30 20:20:10 INFO Utils: /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp7836614385587376716.tmp has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 20:20:10 INFO Executor: Adding file:/tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/30 20:20:10 INFO Executor: Fetching spark://145f628fbc4b:45401/jars/log4j_log4j-1.2.17.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:10 INFO Utils: Fetching spark://145f628fbc4b:45401/jars/log4j_log4j-1.2.17.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp2899421509921534059.tmp\n",
            "23/04/30 20:20:10 INFO Utils: /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp2899421509921534059.tmp has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/log4j_log4j-1.2.17.jar\n",
            "23/04/30 20:20:10 INFO Executor: Adding file:/tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/30 20:20:10 INFO Executor: Fetching spark://145f628fbc4b:45401/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:10 INFO Utils: Fetching spark://145f628fbc4b:45401/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp830441982611730193.tmp\n",
            "23/04/30 20:20:10 INFO Utils: /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp830441982611730193.tmp has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 20:20:10 INFO Executor: Adding file:/tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/30 20:20:10 INFO Executor: Fetching spark://145f628fbc4b:45401/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:10 INFO Utils: Fetching spark://145f628fbc4b:45401/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp6180909920012028806.tmp\n",
            "23/04/30 20:20:10 INFO Utils: /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp6180909920012028806.tmp has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 20:20:10 INFO Executor: Adding file:/tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/30 20:20:10 INFO Executor: Fetching spark://145f628fbc4b:45401/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:10 INFO Utils: Fetching spark://145f628fbc4b:45401/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp300071966468185747.tmp\n",
            "23/04/30 20:20:10 INFO Utils: /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp300071966468185747.tmp has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 20:20:10 INFO Executor: Adding file:/tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/30 20:20:10 INFO Executor: Fetching spark://145f628fbc4b:45401/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:10 INFO Utils: Fetching spark://145f628fbc4b:45401/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp4413962279972951175.tmp\n",
            "23/04/30 20:20:10 INFO Utils: /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp4413962279972951175.tmp has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 20:20:10 INFO Executor: Adding file:/tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/30 20:20:10 INFO Executor: Fetching spark://145f628fbc4b:45401/jars/com.101tec_zkclient-0.3.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:10 INFO Utils: Fetching spark://145f628fbc4b:45401/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp5162829217779237024.tmp\n",
            "23/04/30 20:20:10 INFO Utils: /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp5162829217779237024.tmp has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 20:20:10 INFO Executor: Adding file:/tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/30 20:20:10 INFO Executor: Fetching spark://145f628fbc4b:45401/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:10 INFO Utils: Fetching spark://145f628fbc4b:45401/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp134738024763430764.tmp\n",
            "23/04/30 20:20:10 INFO Utils: /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp134738024763430764.tmp has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 20:20:10 INFO Executor: Adding file:/tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/30 20:20:10 INFO Executor: Fetching spark://145f628fbc4b:45401/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682886007024\n",
            "23/04/30 20:20:10 INFO Utils: Fetching spark://145f628fbc4b:45401/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp2223996520091047567.tmp\n",
            "23/04/30 20:20:10 INFO Utils: /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/fetchFileTemp2223996520091047567.tmp has been previously copied to /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 20:20:10 INFO Executor: Adding file:/tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/userFiles-4ed14884-089a-496d-8131-c823aa7300a1/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/30 20:20:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44957.\n",
            "23/04/30 20:20:10 INFO NettyBlockTransferService: Server created on 145f628fbc4b:44957\n",
            "23/04/30 20:20:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/30 20:20:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 145f628fbc4b, 44957, None)\n",
            "23/04/30 20:20:10 INFO BlockManagerMasterEndpoint: Registering block manager 145f628fbc4b:44957 with 366.3 MiB RAM, BlockManagerId(driver, 145f628fbc4b, 44957, None)\n",
            "23/04/30 20:20:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 145f628fbc4b, 44957, None)\n",
            "23/04/30 20:20:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 145f628fbc4b, 44957, None)\n",
            "23/04/30 20:20:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/04/30 20:20:11 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/04/30 20:20:13 INFO InMemoryFileIndex: It took 45 ms to list leaf files for 1 paths.\n",
            "23/04/30 20:20:14 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 20:20:14 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 20:20:14 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 20:20:14 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 20:20:14 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 20:20:14 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 20:20:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/04/30 20:20:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/04/30 20:20:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 145f628fbc4b:44957 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 20:20:14 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 20:20:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 20:20:14 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/30 20:20:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4584 bytes) taskResourceAssignments Map()\n",
            "23/04/30 20:20:14 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/30 20:20:15 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3161 bytes result sent to driver\n",
            "23/04/30 20:20:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1238 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 20:20:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/30 20:20:15 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.626 s\n",
            "23/04/30 20:20:15 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 20:20:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/30 20:20:15 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.734150 s\n",
            "23/04/30 20:20:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 145f628fbc4b:44957 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 20:20:18 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "23/04/30 20:20:19 INFO FileSourceStrategy: Pushed Filters: IsNotNull(station_id),EqualTo(station_id,KBNA)\n",
            "23/04/30 20:20:19 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(station_id#0),(station_id#0 = KBNA)\n",
            "23/04/30 20:20:19 INFO FileSourceStrategy: Output Data Schema: struct<station_id: string, timestamp_local: timestamp, temp: double, precip: double ... 2 more fields>\n",
            "23/04/30 20:20:19 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/30 20:20:20 INFO CodeGenerator: Code generated in 342.201933 ms\n",
            "23/04/30 20:20:20 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 353.0 KiB, free 366.0 MiB)\n",
            "23/04/30 20:20:20 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 365.9 MiB)\n",
            "23/04/30 20:20:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 145f628fbc4b:44957 (size: 35.7 KiB, free: 366.3 MiB)\n",
            "23/04/30 20:20:20 INFO SparkContext: Created broadcast 1 from csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 20:20:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/30 20:20:20 INFO DAGScheduler: Registering RDD 5 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/30 20:20:20 INFO DAGScheduler: Got map stage job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 20:20:20 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 20:20:20 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 20:20:20 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 20:20:20 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 20:20:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 35.5 KiB, free 365.9 MiB)\n",
            "23/04/30 20:20:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.8 KiB, free 365.9 MiB)\n",
            "23/04/30 20:20:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 145f628fbc4b:44957 (size: 15.8 KiB, free: 366.2 MiB)\n",
            "23/04/30 20:20:20 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 20:20:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 20:20:20 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/04/30 20:20:20 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4852 bytes) taskResourceAssignments Map()\n",
            "23/04/30 20:20:20 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/30 20:20:21 INFO CodeGenerator: Code generated in 67.684371 ms\n",
            "23/04/30 20:20:21 INFO CodeGenerator: Code generated in 33.460718 ms\n",
            "23/04/30 20:20:21 INFO CodeGenerator: Code generated in 33.005145 ms\n",
            "23/04/30 20:20:21 INFO FileScanRDD: Reading File path: file:///content/2021-1-weather.parquet, range: 0-3369951, partition values: [empty row]\n",
            "23/04/30 20:20:21 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/04/30 20:20:21 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/04/30 20:20:22 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 3069 bytes result sent to driver\n",
            "23/04/30 20:20:22 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1345 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 20:20:22 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/30 20:20:22 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 1.396 s\n",
            "23/04/30 20:20:22 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 20:20:22 INFO DAGScheduler: running: Set()\n",
            "23/04/30 20:20:22 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 20:20:22 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 20:20:22 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/30 20:20:22 INFO CodeGenerator: Code generated in 37.650773 ms\n",
            "23/04/30 20:20:22 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/30 20:20:22 INFO CodeGenerator: Code generated in 62.513427 ms\n",
            "23/04/30 20:20:22 INFO DAGScheduler: Registering RDD 9 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/30 20:20:22 INFO DAGScheduler: Got map stage job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 20:20:22 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 20:20:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/30 20:20:22 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 20:20:22 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 20:20:22 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 39.4 KiB, free 365.8 MiB)\n",
            "23/04/30 20:20:22 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 18.3 KiB, free 365.8 MiB)\n",
            "23/04/30 20:20:22 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 145f628fbc4b:44957 (size: 18.3 KiB, free: 366.2 MiB)\n",
            "23/04/30 20:20:22 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 20:20:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 20:20:22 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/30 20:20:22 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/30 20:20:22 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
            "23/04/30 20:20:22 INFO ShuffleBlockFetcherIterator: Getting 1 (20.8 KiB) non-empty blocks including 1 (20.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 20:20:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms\n",
            "23/04/30 20:20:22 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 4792 bytes result sent to driver\n",
            "23/04/30 20:20:22 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 213 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 20:20:22 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/30 20:20:22 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 0.241 s\n",
            "23/04/30 20:20:22 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 20:20:22 INFO DAGScheduler: running: Set()\n",
            "23/04/30 20:20:22 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 20:20:22 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 20:20:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 20:20:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 20:20:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 20:20:22 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 20:20:22 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 20:20:22 INFO DAGScheduler: Final stage: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 20:20:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
            "23/04/30 20:20:22 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 20:20:22 INFO DAGScheduler: Submitting ResultStage 6 (ShuffledRowRDD[10] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 20:20:22 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 211.7 KiB, free 365.6 MiB)\n",
            "23/04/30 20:20:22 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 75.7 KiB, free 365.5 MiB)\n",
            "23/04/30 20:20:22 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 145f628fbc4b:44957 (size: 75.7 KiB, free: 366.2 MiB)\n",
            "23/04/30 20:20:22 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 20:20:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (ShuffledRowRDD[10] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 20:20:22 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "23/04/30 20:20:22 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 3) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/30 20:20:22 INFO Executor: Running task 0.0 in stage 6.0 (TID 3)\n",
            "23/04/30 20:20:23 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 20:20:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 20:20:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 20:20:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 20:20:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 20:20:23 INFO FileOutputCommitter: Saved output of task 'attempt_202304302020222932915803922522449_0006_m_000000_3' to file:/content/localtest2.out/_temporary/0/task_202304302020222932915803922522449_0006_m_000000\n",
            "23/04/30 20:20:23 INFO SparkHadoopMapRedUtil: attempt_202304302020222932915803922522449_0006_m_000000_3: Committed\n",
            "23/04/30 20:20:23 INFO Executor: Finished task 0.0 in stage 6.0 (TID 3). 3483 bytes result sent to driver\n",
            "23/04/30 20:20:23 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 3) in 246 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 20:20:23 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "23/04/30 20:20:23 INFO DAGScheduler: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0) finished in 0.315 s\n",
            "23/04/30 20:20:23 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 20:20:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "23/04/30 20:20:23 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 0.332214 s\n",
            "23/04/30 20:20:23 INFO FileFormatWriter: Start to commit write Job a9cf7100-dc45-4110-91fd-2e20b80f85a1.\n",
            "23/04/30 20:20:23 INFO FileFormatWriter: Write Job a9cf7100-dc45-4110-91fd-2e20b80f85a1 committed. Elapsed time: 43 ms.\n",
            "23/04/30 20:20:23 INFO FileFormatWriter: Finished processing stats for write job a9cf7100-dc45-4110-91fd-2e20b80f85a1.\n",
            "23/04/30 20:20:23 INFO SparkUI: Stopped Spark web UI at http://145f628fbc4b:4040\n",
            "23/04/30 20:20:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/30 20:20:23 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/30 20:20:23 INFO BlockManager: BlockManager stopped\n",
            "23/04/30 20:20:23 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/30 20:20:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/30 20:20:23 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/30 20:20:23 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/30 20:20:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311\n",
            "23/04/30 20:20:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-94cc7734-75b9-4d10-9ee7-94c162a70311/pyspark-b9527243-23a6-452c-a3a5-1e96f409accb\n",
            "23/04/30 20:20:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-4dca40b5-a3b5-4a2c-b40a-3b337829a7bb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/localtest2.out/part-00000-5936ab53-064f-46d1-b33a-01bd825b52f4-c000.csv') as f:\n",
        "  lines = f.readlines()\n",
        "  time_hist = pd.read_csv('/content/localtest2.out/part-00000-5936ab53-064f-46d1-b33a-01bd825b52f4-c000.csv', header=None, names=['hour', 'station', 'precip', 'temp'])\n",
        "time_hist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "aDmXy_u7z_RF",
        "outputId": "9a88c9d0-5319-4cc0-cb36-51bcfcfbf73d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   hour station  precip  temp\n",
              "0     0    KBNA     0.0  -3.3"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-74832303-f4ea-42eb-859a-17cea7d90712\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hour</th>\n",
              "      <th>station</th>\n",
              "      <th>precip</th>\n",
              "      <th>temp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-3.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-74832303-f4ea-42eb-859a-17cea7d90712')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-74832303-f4ea-42eb-859a-17cea7d90712 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-74832303-f4ea-42eb-859a-17cea7d90712');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now that we can extract the first measurement of the month, let's see if we can extract the first measurement of each hour of the day"
      ],
      "metadata": {
        "id": "O5tNhsAX1pYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file localtest2.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('localtest2').getOrCreate()\n",
        "\n",
        "try:\n",
        "  # read in parquet file\n",
        "  # part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n",
        "  incidents_df = spark.read.parquet('2021-1-weather.parquet')\n",
        "  incidents_df.createOrReplaceTempView(\"weather\")\n",
        "\n",
        "  # run SQL query\n",
        "\n",
        "  \"\"\"\n",
        "  From: https://sparkbyexamples.com/pyspark/pyspark-select-first-row-of-each-group/#:~:text=Using%20the%20PySpark%20filter(),first%20row%20of%20each%20group.\n",
        "  SELECT Name, Department, Salary FROM (\n",
        "     SELECT *, row_number() OVER (\n",
        "       PARTITION BY department \n",
        "       ORDER BY salary\n",
        "     ) as rn\n",
        "     FROM EMP) tmp \n",
        "  WHERE rn = 1\")\n",
        "  \"\"\"\n",
        "\n",
        "  query=\"\"\"\n",
        "  SELECT HOUR(timestamp_local), station_id, precip, temp FROM(\n",
        "    SELECT *, row_number() OVER (\n",
        "      PARTITION BY HOUR(timestamp_local)\n",
        "      ORDER BY timestamp_local\n",
        "    ) as rn\n",
        "    FROM weather \n",
        "    WHERE station_id='KBNA') tmp\n",
        "  WHERE rn = 1\n",
        "  \"\"\"\n",
        "\n",
        "  counts = spark.sql(query)\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = counts\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"localtest2.out\")\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXV1ATdXBAji",
        "outputId": "53d38c3d-5b1b-49bb-8586-87613b9d141d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting localtest2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 localtest2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftBw4wUz2lO_",
        "outputId": "dc02528b-aa4d-47c2-f90b-f3b01346e836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c1926338-975a-4126-91b4-22c679c4ce74;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 914ms :: artifacts dl 40ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-c1926338-975a-4126-91b4-22c679c4ce74\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/14ms)\n",
            "23/04/30 21:52:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/30 21:52:59 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/04/30 21:52:59 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 21:52:59 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/30 21:52:59 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 21:52:59 INFO SparkContext: Submitted application: localtest2\n",
            "23/04/30 21:52:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/30 21:52:59 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/30 21:52:59 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/30 21:53:00 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/30 21:53:00 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/30 21:53:00 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/30 21:53:00 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/30 21:53:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/30 21:53:00 INFO Utils: Successfully started service 'sparkDriver' on port 38101.\n",
            "23/04/30 21:53:00 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/30 21:53:01 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/30 21:53:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/30 21:53:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/30 21:53:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/30 21:53:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e99edf4c-a762-4999-9f20-b40578a4fd4e\n",
            "23/04/30 21:53:01 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/30 21:53:01 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/30 21:53:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/30 21:53:02 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://145f628fbc4b:4040\n",
            "23/04/30 21:53:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://145f628fbc4b:38101/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://145f628fbc4b:38101/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://145f628fbc4b:38101/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://145f628fbc4b:38101/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://145f628fbc4b:38101/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://145f628fbc4b:38101/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://145f628fbc4b:38101/jars/com.101tec_zkclient-0.3.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://145f628fbc4b:38101/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://145f628fbc4b:38101/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://145f628fbc4b:38101/jars/log4j_log4j-1.2.17.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://145f628fbc4b:38101/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://145f628fbc4b:38101/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 21:53:02 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 21:53:02 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 21:53:02 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 21:53:02 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 21:53:02 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 21:53:02 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 21:53:02 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 21:53:02 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 21:53:02 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/log4j_log4j-1.2.17.jar\n",
            "23/04/30 21:53:02 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 21:53:02 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:02 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 21:53:03 INFO Executor: Starting executor ID driver on host 145f628fbc4b\n",
            "23/04/30 21:53:03 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/log4j_log4j-1.2.17.jar\n",
            "23/04/30 21:53:03 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 21:53:03 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 21:53:03 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 21:53:03 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 21:53:03 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 21:53:03 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 21:53:03 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 21:53:03 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 21:53:03 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 21:53:03 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 21:53:03 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 21:53:03 INFO Executor: Fetching spark://145f628fbc4b:38101/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO TransportClientFactory: Successfully created connection to 145f628fbc4b/172.28.0.12:38101 after 63 ms (0 ms spent in bootstraps)\n",
            "23/04/30 21:53:03 INFO Utils: Fetching spark://145f628fbc4b:38101/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp3563490180008850054.tmp\n",
            "23/04/30 21:53:03 INFO Utils: /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp3563490180008850054.tmp has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 21:53:03 INFO Executor: Adding file:/tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/30 21:53:03 INFO Executor: Fetching spark://145f628fbc4b:38101/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: Fetching spark://145f628fbc4b:38101/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp2794404234851486172.tmp\n",
            "23/04/30 21:53:03 INFO Utils: /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp2794404234851486172.tmp has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 21:53:03 INFO Executor: Adding file:/tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/30 21:53:03 INFO Executor: Fetching spark://145f628fbc4b:38101/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: Fetching spark://145f628fbc4b:38101/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp5575449184270668959.tmp\n",
            "23/04/30 21:53:03 INFO Utils: /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp5575449184270668959.tmp has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 21:53:03 INFO Executor: Adding file:/tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/30 21:53:03 INFO Executor: Fetching spark://145f628fbc4b:38101/jars/com.101tec_zkclient-0.3.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: Fetching spark://145f628fbc4b:38101/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp430809542951653265.tmp\n",
            "23/04/30 21:53:03 INFO Utils: /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp430809542951653265.tmp has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 21:53:03 INFO Executor: Adding file:/tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/30 21:53:03 INFO Executor: Fetching spark://145f628fbc4b:38101/jars/log4j_log4j-1.2.17.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: Fetching spark://145f628fbc4b:38101/jars/log4j_log4j-1.2.17.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp2223567915662525074.tmp\n",
            "23/04/30 21:53:03 INFO Utils: /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp2223567915662525074.tmp has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/log4j_log4j-1.2.17.jar\n",
            "23/04/30 21:53:03 INFO Executor: Adding file:/tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/30 21:53:03 INFO Executor: Fetching spark://145f628fbc4b:38101/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: Fetching spark://145f628fbc4b:38101/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp1960689451045629691.tmp\n",
            "23/04/30 21:53:03 INFO Utils: /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp1960689451045629691.tmp has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 21:53:03 INFO Executor: Adding file:/tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/30 21:53:03 INFO Executor: Fetching spark://145f628fbc4b:38101/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: Fetching spark://145f628fbc4b:38101/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp7419555371871859435.tmp\n",
            "23/04/30 21:53:03 INFO Utils: /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp7419555371871859435.tmp has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 21:53:03 INFO Executor: Adding file:/tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/30 21:53:03 INFO Executor: Fetching spark://145f628fbc4b:38101/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: Fetching spark://145f628fbc4b:38101/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp8925542017578013701.tmp\n",
            "23/04/30 21:53:03 INFO Utils: /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp8925542017578013701.tmp has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 21:53:03 INFO Executor: Adding file:/tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/30 21:53:03 INFO Executor: Fetching spark://145f628fbc4b:38101/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: Fetching spark://145f628fbc4b:38101/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp2160072263304933057.tmp\n",
            "23/04/30 21:53:03 INFO Utils: /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp2160072263304933057.tmp has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 21:53:03 INFO Executor: Adding file:/tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/30 21:53:03 INFO Executor: Fetching spark://145f628fbc4b:38101/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: Fetching spark://145f628fbc4b:38101/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp7883330532127757485.tmp\n",
            "23/04/30 21:53:03 INFO Utils: /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp7883330532127757485.tmp has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 21:53:03 INFO Executor: Adding file:/tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/30 21:53:03 INFO Executor: Fetching spark://145f628fbc4b:38101/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: Fetching spark://145f628fbc4b:38101/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp8948167139580414856.tmp\n",
            "23/04/30 21:53:03 INFO Utils: /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp8948167139580414856.tmp has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 21:53:03 INFO Executor: Adding file:/tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/30 21:53:03 INFO Executor: Fetching spark://145f628fbc4b:38101/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682891579672\n",
            "23/04/30 21:53:03 INFO Utils: Fetching spark://145f628fbc4b:38101/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp556562625653073350.tmp\n",
            "23/04/30 21:53:03 INFO Utils: /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/fetchFileTemp556562625653073350.tmp has been previously copied to /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 21:53:03 INFO Executor: Adding file:/tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/userFiles-e6f4bee3-8597-4145-a979-6dec5af7ea1c/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/30 21:53:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43629.\n",
            "23/04/30 21:53:03 INFO NettyBlockTransferService: Server created on 145f628fbc4b:43629\n",
            "23/04/30 21:53:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/30 21:53:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 145f628fbc4b, 43629, None)\n",
            "23/04/30 21:53:03 INFO BlockManagerMasterEndpoint: Registering block manager 145f628fbc4b:43629 with 366.3 MiB RAM, BlockManagerId(driver, 145f628fbc4b, 43629, None)\n",
            "23/04/30 21:53:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 145f628fbc4b, 43629, None)\n",
            "23/04/30 21:53:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 145f628fbc4b, 43629, None)\n",
            "23/04/30 21:53:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/04/30 21:53:04 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/04/30 21:53:05 INFO InMemoryFileIndex: It took 51 ms to list leaf files for 1 paths.\n",
            "23/04/30 21:53:06 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 21:53:06 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 21:53:06 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 21:53:06 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 21:53:06 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 21:53:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 21:53:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/04/30 21:53:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/04/30 21:53:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 145f628fbc4b:43629 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 21:53:06 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 21:53:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 21:53:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/30 21:53:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4584 bytes) taskResourceAssignments Map()\n",
            "23/04/30 21:53:07 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/30 21:53:08 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3161 bytes result sent to driver\n",
            "23/04/30 21:53:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1114 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 21:53:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/30 21:53:08 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.397 s\n",
            "23/04/30 21:53:08 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 21:53:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/30 21:53:08 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.475130 s\n",
            "23/04/30 21:53:09 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 145f628fbc4b:43629 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 21:53:11 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "23/04/30 21:53:12 INFO FileSourceStrategy: Pushed Filters: IsNotNull(station_id),EqualTo(station_id,KBNA)\n",
            "23/04/30 21:53:12 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(station_id#0),(station_id#0 = KBNA)\n",
            "23/04/30 21:53:12 INFO FileSourceStrategy: Output Data Schema: struct<station_id: string, timestamp_local: timestamp, temp: double, precip: double ... 2 more fields>\n",
            "23/04/30 21:53:12 INFO CodeGenerator: Code generated in 248.875723 ms\n",
            "23/04/30 21:53:13 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 353.0 KiB, free 366.0 MiB)\n",
            "23/04/30 21:53:13 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 365.9 MiB)\n",
            "23/04/30 21:53:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 145f628fbc4b:43629 (size: 35.7 KiB, free: 366.3 MiB)\n",
            "23/04/30 21:53:13 INFO SparkContext: Created broadcast 1 from csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 21:53:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/30 21:53:13 INFO DAGScheduler: Registering RDD 5 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/30 21:53:13 INFO DAGScheduler: Got map stage job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 21:53:13 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 21:53:13 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 21:53:13 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 21:53:13 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 21:53:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 18.7 KiB, free 365.9 MiB)\n",
            "23/04/30 21:53:13 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 365.9 MiB)\n",
            "23/04/30 21:53:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 145f628fbc4b:43629 (size: 8.2 KiB, free: 366.3 MiB)\n",
            "23/04/30 21:53:13 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 21:53:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 21:53:13 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/04/30 21:53:13 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4852 bytes) taskResourceAssignments Map()\n",
            "23/04/30 21:53:13 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/30 21:53:13 INFO CodeGenerator: Code generated in 81.692519 ms\n",
            "23/04/30 21:53:13 INFO FileScanRDD: Reading File path: file:///content/2021-1-weather.parquet, range: 0-3369951, partition values: [empty row]\n",
            "23/04/30 21:53:13 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/04/30 21:53:14 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/04/30 21:53:15 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2327 bytes result sent to driver\n",
            "23/04/30 21:53:15 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2095 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 21:53:15 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 2.225 s\n",
            "23/04/30 21:53:15 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 21:53:15 INFO DAGScheduler: running: Set()\n",
            "23/04/30 21:53:15 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 21:53:15 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 21:53:15 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/30 21:53:15 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/30 21:53:15 INFO CodeGenerator: Code generated in 82.855889 ms\n",
            "23/04/30 21:53:15 INFO CodeGenerator: Code generated in 53.678654 ms\n",
            "23/04/30 21:53:16 INFO DAGScheduler: Registering RDD 10 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/30 21:53:16 INFO DAGScheduler: Got map stage job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 21:53:16 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 21:53:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/30 21:53:16 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 21:53:16 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 21:53:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 36.0 KiB, free 365.9 MiB)\n",
            "23/04/30 21:53:16 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 365.8 MiB)\n",
            "23/04/30 21:53:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 145f628fbc4b:43629 (size: 16.9 KiB, free: 366.2 MiB)\n",
            "23/04/30 21:53:16 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 21:53:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 21:53:16 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/30 21:53:16 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/30 21:53:16 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
            "23/04/30 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 1 (11.8 KiB) non-empty blocks including 1 (11.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 36 ms\n",
            "23/04/30 21:53:16 INFO CodeGenerator: Code generated in 63.756247 ms\n",
            "23/04/30 21:53:16 INFO CodeGenerator: Code generated in 34.982563 ms\n",
            "23/04/30 21:53:16 INFO CodeGenerator: Code generated in 45.799486 ms\n",
            "23/04/30 21:53:16 INFO CodeGenerator: Code generated in 14.767997 ms\n",
            "23/04/30 21:53:16 INFO CodeGenerator: Code generated in 19.287102 ms\n",
            "23/04/30 21:53:16 INFO CodeGenerator: Code generated in 22.897849 ms\n",
            "23/04/30 21:53:16 INFO CodeGenerator: Code generated in 37.945173 ms\n",
            "23/04/30 21:53:16 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 3750 bytes result sent to driver\n",
            "23/04/30 21:53:16 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 754 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 21:53:16 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 0.830 s\n",
            "23/04/30 21:53:16 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 21:53:16 INFO DAGScheduler: running: Set()\n",
            "23/04/30 21:53:16 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 21:53:16 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 21:53:16 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/30 21:53:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 21:53:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 21:53:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 21:53:17 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 21:53:17 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 21:53:17 INFO DAGScheduler: Final stage: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 21:53:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
            "23/04/30 21:53:17 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 21:53:17 INFO DAGScheduler: Submitting ResultStage 6 (ShuffledRowRDD[11] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 21:53:17 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 211.8 KiB, free 365.6 MiB)\n",
            "23/04/30 21:53:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 75.8 KiB, free 365.6 MiB)\n",
            "23/04/30 21:53:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 145f628fbc4b:43629 (size: 75.8 KiB, free: 366.2 MiB)\n",
            "23/04/30 21:53:17 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 21:53:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (ShuffledRowRDD[11] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 21:53:17 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "23/04/30 21:53:17 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 3) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/30 21:53:17 INFO Executor: Running task 0.0 in stage 6.0 (TID 3)\n",
            "23/04/30 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 1 (445.0 B) non-empty blocks including 1 (445.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "23/04/30 21:53:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 21:53:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 21:53:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 21:53:17 INFO FileOutputCommitter: Saved output of task 'attempt_202304302153178684447064121539331_0006_m_000000_3' to file:/content/localtest2.out/_temporary/0/task_202304302153178684447064121539331_0006_m_000000\n",
            "23/04/30 21:53:17 INFO SparkHadoopMapRedUtil: attempt_202304302153178684447064121539331_0006_m_000000_3: Committed\n",
            "23/04/30 21:53:17 INFO Executor: Finished task 0.0 in stage 6.0 (TID 3). 3483 bytes result sent to driver\n",
            "23/04/30 21:53:17 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 3) in 300 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 21:53:17 INFO DAGScheduler: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0) finished in 0.368 s\n",
            "23/04/30 21:53:17 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 21:53:17 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "23/04/30 21:53:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "23/04/30 21:53:17 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 0.388068 s\n",
            "23/04/30 21:53:17 INFO FileFormatWriter: Start to commit write Job de320312-e754-4a3d-b1eb-8c59b2e6abcb.\n",
            "23/04/30 21:53:17 INFO FileFormatWriter: Write Job de320312-e754-4a3d-b1eb-8c59b2e6abcb committed. Elapsed time: 38 ms.\n",
            "23/04/30 21:53:17 INFO FileFormatWriter: Finished processing stats for write job de320312-e754-4a3d-b1eb-8c59b2e6abcb.\n",
            "23/04/30 21:53:17 INFO SparkUI: Stopped Spark web UI at http://145f628fbc4b:4040\n",
            "23/04/30 21:53:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/30 21:53:17 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/30 21:53:17 INFO BlockManager: BlockManager stopped\n",
            "23/04/30 21:53:17 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/30 21:53:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/30 21:53:17 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/30 21:53:17 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/30 21:53:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-66c94611-347e-4c51-b0b0-e3766d3192b6\n",
            "23/04/30 21:53:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150/pyspark-5e90eb68-40ac-426a-9114-a98e38a43f33\n",
            "23/04/30 21:53:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-e387c9aa-34b4-40c2-9059-037aea8f9150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/localtest2.out/part-00000-09b6b5ab-a8c2-4c39-8994-a43fbdf68415-c000.csv') as f:\n",
        "  lines = f.readlines()\n",
        "  time_hist = pd.read_csv('/content/localtest2.out/part-00000-09b6b5ab-a8c2-4c39-8994-a43fbdf68415-c000.csv', header=None, names=['hour', 'station', 'precip', 'temp'])\n",
        "time_hist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "id": "4hOZIleDCLkE",
        "outputId": "f5a87c87-7cda-47ff-9ee4-24e54ce769b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    hour station  precip  temp\n",
              "0      0    KBNA     1.5   7.2\n",
              "1      1    KBNA     0.8   7.2\n",
              "2      2    KBNA     0.8   7.8\n",
              "3      3    KBNA     4.6   8.3\n",
              "4      4    KBNA     1.3   8.9\n",
              "5      5    KBNA     0.5   9.4\n",
              "6      6    KBNA     0.0  10.0\n",
              "7      7    KBNA     1.3  10.6\n",
              "8      8    KBNA     0.5  15.6\n",
              "9      9    KBNA     3.6  17.2\n",
              "10    10    KBNA     0.3  20.0\n",
              "11    11    KBNA     0.3  20.6\n",
              "12    12    KBNA     0.0  22.8\n",
              "13    13    KBNA     0.0  23.3\n",
              "14    14    KBNA     0.0  23.9\n",
              "15    15    KBNA     0.0  23.3\n",
              "16    16    KBNA     0.0  21.7\n",
              "17    17    KBNA     0.0  19.4\n",
              "18    18    KBNA     0.0  16.1\n",
              "19    19    KBNA     0.0  12.2\n",
              "20    20    KBNA     0.0  10.6\n",
              "21    21    KBNA     0.0  10.6\n",
              "22    22    KBNA     0.0   9.4\n",
              "23    23    KBNA     0.0   9.4"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b54f5fec-1ddf-4e63-9173-1325243d03d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hour</th>\n",
              "      <th>station</th>\n",
              "      <th>precip</th>\n",
              "      <th>temp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>1.5</td>\n",
              "      <td>7.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.8</td>\n",
              "      <td>7.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.8</td>\n",
              "      <td>7.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>4.6</td>\n",
              "      <td>8.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>1.3</td>\n",
              "      <td>8.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.5</td>\n",
              "      <td>9.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>1.3</td>\n",
              "      <td>10.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.5</td>\n",
              "      <td>15.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>3.6</td>\n",
              "      <td>17.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.3</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.3</td>\n",
              "      <td>20.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b54f5fec-1ddf-4e63-9173-1325243d03d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b54f5fec-1ddf-4e63-9173-1325243d03d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b54f5fec-1ddf-4e63-9173-1325243d03d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We can now extract the weather by hour for the first 23 hours of our parquet\n",
        "\n",
        "A few things to note before moving on to refer to later...\n",
        "We need to use OVER PARTITION for Spark since DISTINCT ON was not working we cannot use GROUP BY then LIMIT 1 with DISTINCT ON with Spark for some reason\n",
        "\n",
        "Moving forward, we should be able to just add additional groupings (like hour), and get the first measurement for each of those...\n",
        "\n",
        "May need to use larger buckets (i.e. assume weather remains consistent for ~3 hours)\n",
        "\n"
      ],
      "metadata": {
        "id": "pjlRoTx7F29t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Side note: I want to fix having to copy and paste the file path each time... so let's try to fix that below"
      ],
      "metadata": {
        "id": "n2F_XCPhHdVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def rename_file(output_directory='localtest2.out'):\n",
        "  try:\n",
        "    for file in os.listdir(output_directory):\n",
        "      if 'part-00000' in file and '.crc' not in file:\n",
        "        old_full_path = os.path.abspath(os.path.join(output_directory, file))\n",
        "        new_file_name = os.path.abspath(os.path.join(output_directory, 'results.csv'))\n",
        "        os.rename(old_full_path, new_file_name)\n",
        "        print('Renamed', file)\n",
        "  except FileNotFoundError as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "2RrCh-pDHEhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rename_file()\n",
        "with open('localtest2.out/results.csv') as f:\n",
        "  lines = f.readlines()\n",
        "  time_hist = pd.read_csv('localtest2.out/results.csv', header=None, names=['hour', 'station', 'precip', 'temp'])\n",
        "time_hist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "id": "NTunXt-zJQa6",
        "outputId": "4fd0765f-6db1-4fa8-c9ae-863e6acc33d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renamed part-00000-7b23dd7c-c70a-4eda-ae2e-4aed01d65955-c000.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    hour station  precip  temp\n",
              "0      0    KBNA     1.5   7.2\n",
              "1      1    KBNA     0.8   7.2\n",
              "2      2    KBNA     0.8   7.8\n",
              "3      3    KBNA     4.6   8.3\n",
              "4      4    KBNA     1.3   8.9\n",
              "5      5    KBNA     0.5   9.4\n",
              "6      6    KBNA     0.0  10.0\n",
              "7      7    KBNA     1.3  10.6\n",
              "8      8    KBNA     0.5  15.6\n",
              "9      9    KBNA     3.6  17.2\n",
              "10    10    KBNA     0.3  20.0\n",
              "11    11    KBNA     0.3  20.6\n",
              "12    12    KBNA     0.0  22.8\n",
              "13    13    KBNA     0.0  23.3\n",
              "14    14    KBNA     0.0  23.9\n",
              "15    15    KBNA     0.0  23.3\n",
              "16    16    KBNA     0.0  21.7\n",
              "17    17    KBNA     0.0  19.4\n",
              "18    18    KBNA     0.0  16.1\n",
              "19    19    KBNA     0.0  12.2\n",
              "20    20    KBNA     0.0  10.6\n",
              "21    21    KBNA     0.0  10.6\n",
              "22    22    KBNA     0.0   9.4\n",
              "23    23    KBNA     0.0   9.4"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fc810038-8cdf-4dab-bf80-e5f1c7c8de79\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hour</th>\n",
              "      <th>station</th>\n",
              "      <th>precip</th>\n",
              "      <th>temp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>1.5</td>\n",
              "      <td>7.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.8</td>\n",
              "      <td>7.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.8</td>\n",
              "      <td>7.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>4.6</td>\n",
              "      <td>8.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>1.3</td>\n",
              "      <td>8.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.5</td>\n",
              "      <td>9.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>1.3</td>\n",
              "      <td>10.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.5</td>\n",
              "      <td>15.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>3.6</td>\n",
              "      <td>17.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.3</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.3</td>\n",
              "      <td>20.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc810038-8cdf-4dab-bf80-e5f1c7c8de79')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fc810038-8cdf-4dab-bf80-e5f1c7c8de79 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fc810038-8cdf-4dab-bf80-e5f1c7c8de79');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick intervention to get incident distribution by month instead of hour"
      ],
      "metadata": {
        "id": "dZlqkJ_HKfzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file localtest2.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('localtest2').getOrCreate()\n",
        "\n",
        "try:\n",
        "  # read in parquet file\n",
        "  # part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n",
        "  incidents_df = spark.read.parquet('incidents.parquet')\n",
        "  incidents_df.createOrReplaceTempView(\"incidents\")\n",
        "\n",
        "  # run SQL query\n",
        "  # OLD QUERY: 'SELECT HOUR(FROM_UNIXTIME(time_utc)), COUNT(incident_id) FROM \"incidents\" GROUP BY HOUR(FROM_UNIXTIME(time_utc)) ORDER BY COUNT(incident_id) DESC;'\n",
        "  counts = spark.sql('SELECT MONTH(time_local), COUNT(incident_id) FROM incidents GROUP BY MONTH(time_local) ORDER BY COUNT(incident_id) DESC;')\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = counts\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"localtest2.out\")\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KRMCuf5Kn2u",
        "outputId": "54117a97-ff30-4035-9c16-dea1c4f0b6f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting localtest2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 localtest2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sn1ch5CMK5rq",
        "outputId": "fd322572-9dda-45b2-f178-8d728256396a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-bda836c7-77f7-4dd7-8a1f-7b79e209d140;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 840ms :: artifacts dl 52ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-bda836c7-77f7-4dd7-8a1f-7b79e209d140\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/23ms)\n",
            "23/04/30 21:57:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/30 21:57:39 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/04/30 21:57:39 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 21:57:39 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/30 21:57:39 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 21:57:39 INFO SparkContext: Submitted application: localtest2\n",
            "23/04/30 21:57:39 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/30 21:57:39 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/30 21:57:39 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/30 21:57:39 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/30 21:57:39 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/30 21:57:39 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/30 21:57:39 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/30 21:57:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/30 21:57:40 INFO Utils: Successfully started service 'sparkDriver' on port 40753.\n",
            "23/04/30 21:57:40 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/30 21:57:40 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/30 21:57:40 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/30 21:57:40 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/30 21:57:40 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/30 21:57:40 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f1b78e65-92b0-4bdb-acad-6a1f06d04572\n",
            "23/04/30 21:57:40 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/30 21:57:40 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/30 21:57:41 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/30 21:57:41 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://145f628fbc4b:4040\n",
            "23/04/30 21:57:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://145f628fbc4b:40753/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://145f628fbc4b:40753/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://145f628fbc4b:40753/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://145f628fbc4b:40753/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://145f628fbc4b:40753/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://145f628fbc4b:40753/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://145f628fbc4b:40753/jars/com.101tec_zkclient-0.3.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://145f628fbc4b:40753/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://145f628fbc4b:40753/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://145f628fbc4b:40753/jars/log4j_log4j-1.2.17.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://145f628fbc4b:40753/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://145f628fbc4b:40753/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:41 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:41 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 21:57:41 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:41 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 21:57:41 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:41 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 21:57:41 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:41 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 21:57:42 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 21:57:42 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 21:57:42 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 21:57:42 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 21:57:42 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 21:57:42 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/log4j_log4j-1.2.17.jar\n",
            "23/04/30 21:57:42 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 21:57:42 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 21:57:42 INFO Executor: Starting executor ID driver on host 145f628fbc4b\n",
            "23/04/30 21:57:42 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/log4j_log4j-1.2.17.jar\n",
            "23/04/30 21:57:42 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 21:57:42 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 21:57:42 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 21:57:42 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 21:57:42 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 21:57:42 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 21:57:42 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 21:57:42 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 21:57:42 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 21:57:42 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 21:57:42 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 21:57:42 INFO Executor: Fetching spark://145f628fbc4b:40753/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO TransportClientFactory: Successfully created connection to 145f628fbc4b/172.28.0.12:40753 after 85 ms (0 ms spent in bootstraps)\n",
            "23/04/30 21:57:42 INFO Utils: Fetching spark://145f628fbc4b:40753/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp983465969631815519.tmp\n",
            "23/04/30 21:57:42 INFO Utils: /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp983465969631815519.tmp has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 21:57:42 INFO Executor: Adding file:/tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/30 21:57:42 INFO Executor: Fetching spark://145f628fbc4b:40753/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: Fetching spark://145f628fbc4b:40753/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp2642640745004346814.tmp\n",
            "23/04/30 21:57:42 INFO Utils: /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp2642640745004346814.tmp has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 21:57:42 INFO Executor: Adding file:/tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/30 21:57:42 INFO Executor: Fetching spark://145f628fbc4b:40753/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: Fetching spark://145f628fbc4b:40753/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp1116650274443239847.tmp\n",
            "23/04/30 21:57:42 INFO Utils: /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp1116650274443239847.tmp has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 21:57:42 INFO Executor: Adding file:/tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/30 21:57:42 INFO Executor: Fetching spark://145f628fbc4b:40753/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: Fetching spark://145f628fbc4b:40753/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp8803985589946545839.tmp\n",
            "23/04/30 21:57:42 INFO Utils: /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp8803985589946545839.tmp has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 21:57:42 INFO Executor: Adding file:/tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/30 21:57:42 INFO Executor: Fetching spark://145f628fbc4b:40753/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: Fetching spark://145f628fbc4b:40753/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp9138204455919864432.tmp\n",
            "23/04/30 21:57:42 INFO Utils: /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp9138204455919864432.tmp has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 21:57:42 INFO Executor: Adding file:/tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/30 21:57:42 INFO Executor: Fetching spark://145f628fbc4b:40753/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: Fetching spark://145f628fbc4b:40753/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp1462569022639145288.tmp\n",
            "23/04/30 21:57:42 INFO Utils: /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp1462569022639145288.tmp has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 21:57:42 INFO Executor: Adding file:/tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/30 21:57:42 INFO Executor: Fetching spark://145f628fbc4b:40753/jars/com.101tec_zkclient-0.3.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: Fetching spark://145f628fbc4b:40753/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp997496991991262926.tmp\n",
            "23/04/30 21:57:42 INFO Utils: /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp997496991991262926.tmp has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 21:57:42 INFO Executor: Adding file:/tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/30 21:57:42 INFO Executor: Fetching spark://145f628fbc4b:40753/jars/log4j_log4j-1.2.17.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: Fetching spark://145f628fbc4b:40753/jars/log4j_log4j-1.2.17.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp7822084414397062611.tmp\n",
            "23/04/30 21:57:42 INFO Utils: /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp7822084414397062611.tmp has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/log4j_log4j-1.2.17.jar\n",
            "23/04/30 21:57:42 INFO Executor: Adding file:/tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/30 21:57:42 INFO Executor: Fetching spark://145f628fbc4b:40753/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: Fetching spark://145f628fbc4b:40753/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp7130209292581760896.tmp\n",
            "23/04/30 21:57:42 INFO Utils: /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp7130209292581760896.tmp has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 21:57:42 INFO Executor: Adding file:/tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/30 21:57:42 INFO Executor: Fetching spark://145f628fbc4b:40753/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: Fetching spark://145f628fbc4b:40753/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp1548904290714470260.tmp\n",
            "23/04/30 21:57:42 INFO Utils: /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp1548904290714470260.tmp has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 21:57:42 INFO Executor: Adding file:/tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/30 21:57:42 INFO Executor: Fetching spark://145f628fbc4b:40753/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: Fetching spark://145f628fbc4b:40753/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp2502411579270680089.tmp\n",
            "23/04/30 21:57:42 INFO Utils: /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp2502411579270680089.tmp has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 21:57:42 INFO Executor: Adding file:/tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/30 21:57:42 INFO Executor: Fetching spark://145f628fbc4b:40753/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682891859264\n",
            "23/04/30 21:57:42 INFO Utils: Fetching spark://145f628fbc4b:40753/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp603349031422403461.tmp\n",
            "23/04/30 21:57:43 INFO Utils: /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/fetchFileTemp603349031422403461.tmp has been previously copied to /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 21:57:43 INFO Executor: Adding file:/tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/userFiles-f0f428b7-ee2d-4aa1-8663-00c7f4f76467/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/30 21:57:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37231.\n",
            "23/04/30 21:57:43 INFO NettyBlockTransferService: Server created on 145f628fbc4b:37231\n",
            "23/04/30 21:57:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/30 21:57:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 145f628fbc4b, 37231, None)\n",
            "23/04/30 21:57:43 INFO BlockManagerMasterEndpoint: Registering block manager 145f628fbc4b:37231 with 366.3 MiB RAM, BlockManagerId(driver, 145f628fbc4b, 37231, None)\n",
            "23/04/30 21:57:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 145f628fbc4b, 37231, None)\n",
            "23/04/30 21:57:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 145f628fbc4b, 37231, None)\n",
            "23/04/30 21:57:44 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/04/30 21:57:44 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/04/30 21:57:46 INFO InMemoryFileIndex: It took 42 ms to list leaf files for 1 paths.\n",
            "23/04/30 21:57:47 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 21:57:47 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 21:57:47 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 21:57:47 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 21:57:47 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 21:57:47 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 21:57:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/04/30 21:57:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/04/30 21:57:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 145f628fbc4b:37231 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 21:57:47 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 21:57:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 21:57:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/30 21:57:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4579 bytes) taskResourceAssignments Map()\n",
            "23/04/30 21:57:47 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/30 21:57:48 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2231 bytes result sent to driver\n",
            "23/04/30 21:57:48 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 986 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 21:57:48 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/30 21:57:48 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.257 s\n",
            "23/04/30 21:57:48 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 21:57:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/30 21:57:48 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.344334 s\n",
            "23/04/30 21:57:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 145f628fbc4b:37231 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 21:57:52 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/30 21:57:52 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "23/04/30 21:57:52 INFO FileSourceStrategy: Output Data Schema: struct<time_local: timestamp, Incident_ID: int>\n",
            "23/04/30 21:57:54 INFO CodeGenerator: Code generated in 680.28277 ms\n",
            "23/04/30 21:57:54 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 352.5 KiB, free 366.0 MiB)\n",
            "23/04/30 21:57:54 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 365.9 MiB)\n",
            "23/04/30 21:57:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 145f628fbc4b:37231 (size: 35.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 21:57:54 INFO SparkContext: Created broadcast 1 from csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 21:57:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/30 21:57:54 INFO DAGScheduler: Registering RDD 5 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/30 21:57:54 INFO DAGScheduler: Got map stage job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 21:57:54 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 21:57:54 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 21:57:54 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 21:57:54 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 21:57:54 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 37.7 KiB, free 365.9 MiB)\n",
            "23/04/30 21:57:54 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 365.9 MiB)\n",
            "23/04/30 21:57:54 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 145f628fbc4b:37231 (size: 17.3 KiB, free: 366.2 MiB)\n",
            "23/04/30 21:57:54 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 21:57:54 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 21:57:54 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/04/30 21:57:54 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4847 bytes) taskResourceAssignments Map()\n",
            "23/04/30 21:57:54 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/30 21:57:55 INFO CodeGenerator: Code generated in 88.05366 ms\n",
            "23/04/30 21:57:55 INFO CodeGenerator: Code generated in 35.555528 ms\n",
            "23/04/30 21:57:55 INFO CodeGenerator: Code generated in 36.614889 ms\n",
            "23/04/30 21:57:55 INFO CodeGenerator: Code generated in 31.774419 ms\n",
            "23/04/30 21:57:55 INFO FileScanRDD: Reading File path: file:///content/incidents.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/30 21:57:55 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/04/30 21:57:56 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 3013 bytes result sent to driver\n",
            "23/04/30 21:57:56 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1779 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 21:57:56 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/30 21:57:56 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 1.861 s\n",
            "23/04/30 21:57:56 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 21:57:56 INFO DAGScheduler: running: Set()\n",
            "23/04/30 21:57:56 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 21:57:56 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 21:57:56 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/30 21:57:56 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/30 21:57:56 INFO CodeGenerator: Code generated in 33.056486 ms\n",
            "23/04/30 21:57:56 INFO CodeGenerator: Code generated in 42.133055 ms\n",
            "23/04/30 21:57:56 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 21:57:56 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 21:57:56 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 21:57:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/30 21:57:56 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 21:57:56 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 21:57:57 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 36.9 KiB, free 365.8 MiB)\n",
            "23/04/30 21:57:57 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 365.8 MiB)\n",
            "23/04/30 21:57:57 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 145f628fbc4b:37231 (size: 17.4 KiB, free: 366.2 MiB)\n",
            "23/04/30 21:57:57 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 21:57:57 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/30 21:57:57 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/30 21:57:57 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
            "23/04/30 21:57:57 INFO ShuffleBlockFetcherIterator: Getting 1 (792.0 B) non-empty blocks including 1 (792.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 21:57:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms\n",
            "23/04/30 21:57:57 INFO CodeGenerator: Code generated in 17.613653 ms\n",
            "23/04/30 21:57:57 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 4581 bytes result sent to driver\n",
            "23/04/30 21:57:57 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 154 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 21:57:57 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 0.181 s\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 21:57:57 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/30 21:57:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.228691 s\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Registering RDD 11 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Got map stage job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[11] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 21:57:57 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 37.3 KiB, free 365.8 MiB)\n",
            "23/04/30 21:57:57 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 17.7 KiB, free 365.8 MiB)\n",
            "23/04/30 21:57:57 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 145f628fbc4b:37231 (size: 17.7 KiB, free: 366.2 MiB)\n",
            "23/04/30 21:57:57 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[11] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 21:57:57 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "23/04/30 21:57:57 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/30 21:57:57 INFO Executor: Running task 0.0 in stage 5.0 (TID 3)\n",
            "23/04/30 21:57:57 INFO ShuffleBlockFetcherIterator: Getting 1 (792.0 B) non-empty blocks including 1 (792.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 21:57:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 21:57:57 INFO Executor: Finished task 0.0 in stage 5.0 (TID 3). 4187 bytes result sent to driver\n",
            "23/04/30 21:57:57 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 76 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 21:57:57 INFO DAGScheduler: ShuffleMapStage 5 (csv at NativeMethodAccessorImpl.java:0) finished in 0.105 s\n",
            "23/04/30 21:57:57 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 21:57:57 INFO DAGScheduler: running: Set()\n",
            "23/04/30 21:57:57 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 21:57:57 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 21:57:57 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "23/04/30 21:57:57 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/30 21:57:57 INFO CodeGenerator: Code generated in 31.230393 ms\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Registering RDD 14 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Got map stage job 4 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[14] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 21:57:57 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 35.4 KiB, free 365.7 MiB)\n",
            "23/04/30 21:57:57 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 365.7 MiB)\n",
            "23/04/30 21:57:57 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 145f628fbc4b:37231 (size: 16.9 KiB, free: 366.2 MiB)\n",
            "23/04/30 21:57:57 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[14] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 21:57:57 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "23/04/30 21:57:57 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 4) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/30 21:57:57 INFO Executor: Running task 0.0 in stage 8.0 (TID 4)\n",
            "23/04/30 21:57:57 INFO ShuffleBlockFetcherIterator: Getting 1 (792.0 B) non-empty blocks including 1 (792.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 21:57:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "23/04/30 21:57:57 INFO CodeGenerator: Code generated in 33.309013 ms\n",
            "23/04/30 21:57:57 INFO CodeGenerator: Code generated in 26.896595 ms\n",
            "23/04/30 21:57:57 INFO Executor: Finished task 0.0 in stage 8.0 (TID 4). 5085 bytes result sent to driver\n",
            "23/04/30 21:57:57 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 4) in 129 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 21:57:57 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "23/04/30 21:57:57 INFO DAGScheduler: ShuffleMapStage 8 (csv at NativeMethodAccessorImpl.java:0) finished in 0.153 s\n",
            "23/04/30 21:57:57 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 21:57:57 INFO DAGScheduler: running: Set()\n",
            "23/04/30 21:57:57 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 21:57:57 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 21:57:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 21:57:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 21:57:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 21:57:57 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Got job 5 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Final stage: ResultStage 12 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Submitting ResultStage 12 (ShuffledRowRDD[15] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 21:57:57 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 211.3 KiB, free 365.5 MiB)\n",
            "23/04/30 21:57:57 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 75.5 KiB, free 365.4 MiB)\n",
            "23/04/30 21:57:57 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 145f628fbc4b:37231 (size: 75.5 KiB, free: 366.1 MiB)\n",
            "23/04/30 21:57:57 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (ShuffledRowRDD[15] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 21:57:57 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
            "23/04/30 21:57:57 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 5) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/30 21:57:57 INFO Executor: Running task 0.0 in stage 12.0 (TID 5)\n",
            "23/04/30 21:57:57 INFO ShuffleBlockFetcherIterator: Getting 1 (207.0 B) non-empty blocks including 1 (207.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 21:57:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 21:57:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 21:57:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 21:57:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 21:57:57 INFO FileOutputCommitter: Saved output of task 'attempt_202304302157578545936474348487334_0012_m_000000_5' to file:/content/localtest2.out/_temporary/0/task_202304302157578545936474348487334_0012_m_000000\n",
            "23/04/30 21:57:57 INFO SparkHadoopMapRedUtil: attempt_202304302157578545936474348487334_0012_m_000000_5: Committed\n",
            "23/04/30 21:57:57 INFO Executor: Finished task 0.0 in stage 12.0 (TID 5). 3483 bytes result sent to driver\n",
            "23/04/30 21:57:57 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 5) in 161 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 21:57:57 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
            "23/04/30 21:57:57 INFO DAGScheduler: ResultStage 12 (csv at NativeMethodAccessorImpl.java:0) finished in 0.206 s\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 21:57:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
            "23/04/30 21:57:57 INFO DAGScheduler: Job 5 finished: csv at NativeMethodAccessorImpl.java:0, took 0.217685 s\n",
            "23/04/30 21:57:57 INFO FileFormatWriter: Start to commit write Job da27262b-9a31-4171-993e-0145256c6ccc.\n",
            "23/04/30 21:57:57 INFO FileFormatWriter: Write Job da27262b-9a31-4171-993e-0145256c6ccc committed. Elapsed time: 23 ms.\n",
            "23/04/30 21:57:57 INFO FileFormatWriter: Finished processing stats for write job da27262b-9a31-4171-993e-0145256c6ccc.\n",
            "23/04/30 21:57:57 INFO SparkUI: Stopped Spark web UI at http://145f628fbc4b:4040\n",
            "23/04/30 21:57:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/30 21:57:58 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/30 21:57:58 INFO BlockManager: BlockManager stopped\n",
            "23/04/30 21:57:58 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/30 21:57:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/30 21:57:58 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/30 21:57:58 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/30 21:57:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb\n",
            "23/04/30 21:57:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-5a327ea1-27ed-4102-ab4e-ffa13bd7a2e6\n",
            "23/04/30 21:57:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-5dba0d2d-671f-481c-b667-e0024143e3eb/pyspark-752b60e8-6702-4646-a7be-3556c062c9b9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rename_file()\n",
        "with open('localtest2.out/results.csv') as f:\n",
        "  lines = f.readlines()\n",
        "  time_hist = pd.read_csv('localtest2.out/results.csv', header=None, names=['_col0', '_col1'])\n",
        "time_hist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "yZXxi3YxKyvM",
        "outputId": "306eb5a4-c759-4f7b-ebbe-3e65088684a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    _col0  _col1\n",
              "0       1   3008\n",
              "1       2   2819\n",
              "2      10   2665\n",
              "3       8   2551\n",
              "4      11   2466\n",
              "5       5   2425\n",
              "6       3   2394\n",
              "7       4   2317\n",
              "8       7   2311\n",
              "9       9   2277\n",
              "10      6   2274\n",
              "11     12   2258"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0294423e-32ae-40f3-a3af-7b9066bf5b58\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_col0</th>\n",
              "      <th>_col1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>3008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10</td>\n",
              "      <td>2665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>2551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>2466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>2425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>2394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4</td>\n",
              "      <td>2317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>7</td>\n",
              "      <td>2311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>2277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>6</td>\n",
              "      <td>2274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>2258</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0294423e-32ae-40f3-a3af-7b9066bf5b58')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0294423e-32ae-40f3-a3af-7b9066bf5b58 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0294423e-32ae-40f3-a3af-7b9066bf5b58');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(11.7, 8.27)\n",
        "sns.barplot(data=time_hist, x=\"_col0\", y=\"_col1\", color='steelblue', order=list(range(1, 13))).set(\n",
        "    title='Distribution of Incidents by Month', xlabel='Month of Year', ylabel='Total Count')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "id": "XdvR0GqcLJsv",
        "outputId": "67ad65e5-a5bf-4235-800d-76631e1ed7cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0.5, 1.0, 'Distribution of Incidents by Month'),\n",
              " Text(0.5, 0, 'Month of Year'),\n",
              " Text(0, 0.5, 'Total Count')]"
            ]
          },
          "metadata": {},
          "execution_count": 150
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1170x827 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file localtest2.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('localtest2').getOrCreate()\n",
        "\n",
        "try:\n",
        "  # read in parquet file\n",
        "  # part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n",
        "  incidents_df = spark.read.parquet('incidents.parquet')\n",
        "  incidents_df.createOrReplaceTempView(\"incidents\")\n",
        "\n",
        "  # run SQL query\n",
        "  # OLD QUERY: 'SELECT HOUR(FROM_UNIXTIME(time_utc)), COUNT(incident_id) FROM \"incidents\" GROUP BY HOUR(FROM_UNIXTIME(time_utc)) ORDER BY COUNT(incident_id) DESC;'\n",
        "  counts = spark.sql('SELECT YEAR(time_local), COUNT(incident_id) FROM incidents GROUP BY YEAR(time_local) ORDER BY COUNT(incident_id) DESC;')\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = counts\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"localtest2.out\")\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO6YGWD7NPa6",
        "outputId": "93cf1baa-de82-496f-da79-d10dd64fa147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting localtest2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 localtest2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPwLW1a2NYS0",
        "outputId": "2bffb1a9-fc38-4229-e46a-4b8c5bf40be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1d6c334e-1a0e-4897-973d-2dbacf2cbc0a;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 812ms :: artifacts dl 44ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-1d6c334e-1a0e-4897-973d-2dbacf2cbc0a\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/13ms)\n",
            "23/04/30 22:08:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/30 22:08:16 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/04/30 22:08:16 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 22:08:16 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/30 22:08:16 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 22:08:16 INFO SparkContext: Submitted application: localtest2\n",
            "23/04/30 22:08:16 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/30 22:08:16 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/30 22:08:16 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/30 22:08:16 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/30 22:08:16 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/30 22:08:16 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/30 22:08:16 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/30 22:08:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/30 22:08:17 INFO Utils: Successfully started service 'sparkDriver' on port 40611.\n",
            "23/04/30 22:08:17 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/30 22:08:17 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/30 22:08:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/30 22:08:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/30 22:08:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/30 22:08:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-25aefbaf-bb1c-4f6f-9d6e-c55f3f10d63f\n",
            "23/04/30 22:08:17 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/30 22:08:17 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/30 22:08:18 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/30 22:08:18 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://145f628fbc4b:4040\n",
            "23/04/30 22:08:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://145f628fbc4b:40611/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://145f628fbc4b:40611/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://145f628fbc4b:40611/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://145f628fbc4b:40611/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://145f628fbc4b:40611/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://145f628fbc4b:40611/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://145f628fbc4b:40611/jars/com.101tec_zkclient-0.3.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://145f628fbc4b:40611/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://145f628fbc4b:40611/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://145f628fbc4b:40611/jars/log4j_log4j-1.2.17.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://145f628fbc4b:40611/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://145f628fbc4b:40611/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:18 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:18 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 22:08:18 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:18 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 22:08:18 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:18 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 22:08:18 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:18 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 22:08:19 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:19 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 22:08:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:19 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 22:08:19 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:19 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 22:08:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:19 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 22:08:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:19 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 22:08:19 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:19 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/log4j_log4j-1.2.17.jar\n",
            "23/04/30 22:08:19 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:19 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 22:08:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:19 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 22:08:19 INFO Executor: Starting executor ID driver on host 145f628fbc4b\n",
            "23/04/30 22:08:19 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:19 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/log4j_log4j-1.2.17.jar\n",
            "23/04/30 22:08:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:19 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 22:08:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:19 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 22:08:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:19 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 22:08:19 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:19 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 22:08:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:19 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 22:08:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:19 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 22:08:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:19 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 22:08:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:19 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 22:08:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:19 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 22:08:19 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:19 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 22:08:19 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:19 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 22:08:19 INFO Executor: Fetching spark://145f628fbc4b:40611/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:19 INFO TransportClientFactory: Successfully created connection to 145f628fbc4b/172.28.0.12:40611 after 73 ms (0 ms spent in bootstraps)\n",
            "23/04/30 22:08:19 INFO Utils: Fetching spark://145f628fbc4b:40611/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp7702464170962915071.tmp\n",
            "23/04/30 22:08:20 INFO Utils: /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp7702464170962915071.tmp has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 22:08:20 INFO Executor: Adding file:/tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/30 22:08:20 INFO Executor: Fetching spark://145f628fbc4b:40611/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:20 INFO Utils: Fetching spark://145f628fbc4b:40611/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp5400407247502244548.tmp\n",
            "23/04/30 22:08:20 INFO Utils: /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp5400407247502244548.tmp has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 22:08:20 INFO Executor: Adding file:/tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/30 22:08:20 INFO Executor: Fetching spark://145f628fbc4b:40611/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:20 INFO Utils: Fetching spark://145f628fbc4b:40611/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp5245998316479411453.tmp\n",
            "23/04/30 22:08:20 INFO Utils: /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp5245998316479411453.tmp has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 22:08:20 INFO Executor: Adding file:/tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/30 22:08:20 INFO Executor: Fetching spark://145f628fbc4b:40611/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:20 INFO Utils: Fetching spark://145f628fbc4b:40611/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp5375512396814568777.tmp\n",
            "23/04/30 22:08:20 INFO Utils: /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp5375512396814568777.tmp has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 22:08:20 INFO Executor: Adding file:/tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/30 22:08:20 INFO Executor: Fetching spark://145f628fbc4b:40611/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:20 INFO Utils: Fetching spark://145f628fbc4b:40611/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp8924328373394314145.tmp\n",
            "23/04/30 22:08:20 INFO Utils: /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp8924328373394314145.tmp has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 22:08:20 INFO Executor: Adding file:/tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/30 22:08:20 INFO Executor: Fetching spark://145f628fbc4b:40611/jars/log4j_log4j-1.2.17.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:20 INFO Utils: Fetching spark://145f628fbc4b:40611/jars/log4j_log4j-1.2.17.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp2882362726954911187.tmp\n",
            "23/04/30 22:08:20 INFO Utils: /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp2882362726954911187.tmp has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/log4j_log4j-1.2.17.jar\n",
            "23/04/30 22:08:20 INFO Executor: Adding file:/tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/30 22:08:20 INFO Executor: Fetching spark://145f628fbc4b:40611/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:20 INFO Utils: Fetching spark://145f628fbc4b:40611/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp8179609299605351692.tmp\n",
            "23/04/30 22:08:20 INFO Utils: /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp8179609299605351692.tmp has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 22:08:20 INFO Executor: Adding file:/tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/30 22:08:20 INFO Executor: Fetching spark://145f628fbc4b:40611/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:20 INFO Utils: Fetching spark://145f628fbc4b:40611/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp2647231375170839979.tmp\n",
            "23/04/30 22:08:20 INFO Utils: /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp2647231375170839979.tmp has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 22:08:20 INFO Executor: Adding file:/tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/30 22:08:20 INFO Executor: Fetching spark://145f628fbc4b:40611/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:20 INFO Utils: Fetching spark://145f628fbc4b:40611/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp3566552074271379295.tmp\n",
            "23/04/30 22:08:20 INFO Utils: /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp3566552074271379295.tmp has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 22:08:20 INFO Executor: Adding file:/tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/30 22:08:20 INFO Executor: Fetching spark://145f628fbc4b:40611/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:20 INFO Utils: Fetching spark://145f628fbc4b:40611/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp25706813767572532.tmp\n",
            "23/04/30 22:08:20 INFO Utils: /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp25706813767572532.tmp has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 22:08:20 INFO Executor: Adding file:/tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/30 22:08:20 INFO Executor: Fetching spark://145f628fbc4b:40611/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:20 INFO Utils: Fetching spark://145f628fbc4b:40611/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp5698270251967513313.tmp\n",
            "23/04/30 22:08:20 INFO Utils: /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp5698270251967513313.tmp has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 22:08:20 INFO Executor: Adding file:/tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/30 22:08:20 INFO Executor: Fetching spark://145f628fbc4b:40611/jars/com.101tec_zkclient-0.3.jar with timestamp 1682892496385\n",
            "23/04/30 22:08:20 INFO Utils: Fetching spark://145f628fbc4b:40611/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp7678725762677272789.tmp\n",
            "23/04/30 22:08:20 INFO Utils: /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/fetchFileTemp7678725762677272789.tmp has been previously copied to /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 22:08:20 INFO Executor: Adding file:/tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/userFiles-129e0a7b-ce9d-42d4-b360-63b276556cca/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/30 22:08:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36763.\n",
            "23/04/30 22:08:20 INFO NettyBlockTransferService: Server created on 145f628fbc4b:36763\n",
            "23/04/30 22:08:20 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/30 22:08:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 145f628fbc4b, 36763, None)\n",
            "23/04/30 22:08:20 INFO BlockManagerMasterEndpoint: Registering block manager 145f628fbc4b:36763 with 366.3 MiB RAM, BlockManagerId(driver, 145f628fbc4b, 36763, None)\n",
            "23/04/30 22:08:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 145f628fbc4b, 36763, None)\n",
            "23/04/30 22:08:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 145f628fbc4b, 36763, None)\n",
            "23/04/30 22:08:21 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/04/30 22:08:21 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/04/30 22:08:23 INFO InMemoryFileIndex: It took 159 ms to list leaf files for 1 paths.\n",
            "23/04/30 22:08:26 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 22:08:26 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 22:08:26 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 22:08:26 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 22:08:26 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 22:08:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 22:08:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/04/30 22:08:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/04/30 22:08:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 145f628fbc4b:36763 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 22:08:27 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 22:08:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 22:08:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/30 22:08:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4579 bytes) taskResourceAssignments Map()\n",
            "23/04/30 22:08:27 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/30 22:08:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2274 bytes result sent to driver\n",
            "23/04/30 22:08:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1661 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 22:08:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/30 22:08:29 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.295 s\n",
            "23/04/30 22:08:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 22:08:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/30 22:08:29 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.387685 s\n",
            "23/04/30 22:08:30 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 145f628fbc4b:36763 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 22:08:33 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/30 22:08:33 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "23/04/30 22:08:33 INFO FileSourceStrategy: Output Data Schema: struct<time_local: timestamp, Incident_ID: int>\n",
            "23/04/30 22:08:34 INFO CodeGenerator: Code generated in 707.518674 ms\n",
            "23/04/30 22:08:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 352.5 KiB, free 366.0 MiB)\n",
            "23/04/30 22:08:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 365.9 MiB)\n",
            "23/04/30 22:08:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 145f628fbc4b:36763 (size: 35.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 22:08:34 INFO SparkContext: Created broadcast 1 from csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 22:08:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/30 22:08:34 INFO DAGScheduler: Registering RDD 5 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/30 22:08:34 INFO DAGScheduler: Got map stage job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 22:08:34 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 22:08:34 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 22:08:34 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 22:08:34 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 22:08:35 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 37.7 KiB, free 365.9 MiB)\n",
            "23/04/30 22:08:35 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 365.9 MiB)\n",
            "23/04/30 22:08:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 145f628fbc4b:36763 (size: 17.3 KiB, free: 366.2 MiB)\n",
            "23/04/30 22:08:35 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 22:08:35 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 22:08:35 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/04/30 22:08:35 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4847 bytes) taskResourceAssignments Map()\n",
            "23/04/30 22:08:35 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/30 22:08:35 INFO CodeGenerator: Code generated in 115.630634 ms\n",
            "23/04/30 22:08:35 INFO CodeGenerator: Code generated in 53.328125 ms\n",
            "23/04/30 22:08:35 INFO CodeGenerator: Code generated in 26.160656 ms\n",
            "23/04/30 22:08:35 INFO CodeGenerator: Code generated in 23.288661 ms\n",
            "23/04/30 22:08:35 INFO FileScanRDD: Reading File path: file:///content/incidents.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/30 22:08:36 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/04/30 22:08:37 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2970 bytes result sent to driver\n",
            "23/04/30 22:08:37 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2118 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 22:08:37 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/30 22:08:37 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 2.341 s\n",
            "23/04/30 22:08:37 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 22:08:37 INFO DAGScheduler: running: Set()\n",
            "23/04/30 22:08:37 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 22:08:37 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 22:08:37 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/30 22:08:37 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/30 22:08:37 INFO CodeGenerator: Code generated in 113.088671 ms\n",
            "23/04/30 22:08:38 INFO CodeGenerator: Code generated in 67.560032 ms\n",
            "23/04/30 22:08:38 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 22:08:38 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 22:08:38 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 22:08:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/30 22:08:38 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 22:08:38 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 22:08:38 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 36.9 KiB, free 365.8 MiB)\n",
            "23/04/30 22:08:38 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 365.8 MiB)\n",
            "23/04/30 22:08:38 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 145f628fbc4b:36763 (size: 17.4 KiB, free: 366.2 MiB)\n",
            "23/04/30 22:08:38 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 22:08:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 22:08:38 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/30 22:08:38 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/30 22:08:38 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
            "23/04/30 22:08:38 INFO ShuffleBlockFetcherIterator: Getting 1 (330.0 B) non-empty blocks including 1 (330.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 22:08:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 64 ms\n",
            "23/04/30 22:08:38 INFO CodeGenerator: Code generated in 66.651025 ms\n",
            "23/04/30 22:08:38 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 4350 bytes result sent to driver\n",
            "23/04/30 22:08:38 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 424 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 22:08:38 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 0.486 s\n",
            "23/04/30 22:08:38 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 22:08:38 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/30 22:08:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/30 22:08:38 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.594225 s\n",
            "23/04/30 22:08:38 INFO DAGScheduler: Registering RDD 11 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/30 22:08:38 INFO DAGScheduler: Got map stage job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 22:08:38 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 22:08:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
            "23/04/30 22:08:38 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 22:08:38 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[11] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 22:08:38 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 37.1 KiB, free 365.8 MiB)\n",
            "23/04/30 22:08:38 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 17.7 KiB, free 365.8 MiB)\n",
            "23/04/30 22:08:38 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 145f628fbc4b:36763 (size: 17.7 KiB, free: 366.2 MiB)\n",
            "23/04/30 22:08:38 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 22:08:38 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[11] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 22:08:38 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "23/04/30 22:08:38 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/30 22:08:38 INFO Executor: Running task 0.0 in stage 5.0 (TID 3)\n",
            "23/04/30 22:08:39 INFO ShuffleBlockFetcherIterator: Getting 1 (330.0 B) non-empty blocks including 1 (330.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 22:08:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 22:08:39 INFO Executor: Finished task 0.0 in stage 5.0 (TID 3). 4180 bytes result sent to driver\n",
            "23/04/30 22:08:39 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 186 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 22:08:39 INFO DAGScheduler: ShuffleMapStage 5 (csv at NativeMethodAccessorImpl.java:0) finished in 0.272 s\n",
            "23/04/30 22:08:39 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 22:08:39 INFO DAGScheduler: running: Set()\n",
            "23/04/30 22:08:39 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 22:08:39 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 22:08:39 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "23/04/30 22:08:39 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/30 22:08:39 INFO CodeGenerator: Code generated in 51.520386 ms\n",
            "23/04/30 22:08:39 INFO DAGScheduler: Registering RDD 14 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
            "23/04/30 22:08:39 INFO DAGScheduler: Got map stage job 4 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 22:08:39 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 22:08:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
            "23/04/30 22:08:39 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 22:08:39 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[14] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 22:08:39 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 35.4 KiB, free 365.7 MiB)\n",
            "23/04/30 22:08:39 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 365.7 MiB)\n",
            "23/04/30 22:08:39 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 145f628fbc4b:36763 (size: 16.9 KiB, free: 366.2 MiB)\n",
            "23/04/30 22:08:39 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 22:08:39 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[14] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 22:08:39 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "23/04/30 22:08:39 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 4) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/30 22:08:39 INFO Executor: Running task 0.0 in stage 8.0 (TID 4)\n",
            "23/04/30 22:08:39 INFO ShuffleBlockFetcherIterator: Getting 1 (330.0 B) non-empty blocks including 1 (330.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 22:08:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
            "23/04/30 22:08:39 INFO CodeGenerator: Code generated in 25.673943 ms\n",
            "23/04/30 22:08:39 INFO CodeGenerator: Code generated in 17.157542 ms\n",
            "23/04/30 22:08:39 INFO Executor: Finished task 0.0 in stage 8.0 (TID 4). 5085 bytes result sent to driver\n",
            "23/04/30 22:08:39 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 4) in 198 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 22:08:39 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "23/04/30 22:08:39 INFO DAGScheduler: ShuffleMapStage 8 (csv at NativeMethodAccessorImpl.java:0) finished in 0.241 s\n",
            "23/04/30 22:08:39 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 22:08:39 INFO DAGScheduler: running: Set()\n",
            "23/04/30 22:08:39 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 22:08:39 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 22:08:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 22:08:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 22:08:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 22:08:39 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 22:08:39 INFO DAGScheduler: Got job 5 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 22:08:39 INFO DAGScheduler: Final stage: ResultStage 12 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 22:08:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\n",
            "23/04/30 22:08:39 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 22:08:39 INFO DAGScheduler: Submitting ResultStage 12 (ShuffledRowRDD[15] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 22:08:39 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 211.3 KiB, free 365.5 MiB)\n",
            "23/04/30 22:08:39 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 75.6 KiB, free 365.4 MiB)\n",
            "23/04/30 22:08:39 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 145f628fbc4b:36763 (size: 75.6 KiB, free: 366.1 MiB)\n",
            "23/04/30 22:08:39 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 22:08:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (ShuffledRowRDD[15] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 22:08:39 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
            "23/04/30 22:08:39 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 5) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/30 22:08:39 INFO Executor: Running task 0.0 in stage 12.0 (TID 5)\n",
            "23/04/30 22:08:39 INFO ShuffleBlockFetcherIterator: Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 22:08:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 22:08:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 22:08:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 22:08:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 22:08:40 INFO FileOutputCommitter: Saved output of task 'attempt_202304302208398141079271649780154_0012_m_000000_5' to file:/content/localtest2.out/_temporary/0/task_202304302208398141079271649780154_0012_m_000000\n",
            "23/04/30 22:08:40 INFO SparkHadoopMapRedUtil: attempt_202304302208398141079271649780154_0012_m_000000_5: Committed\n",
            "23/04/30 22:08:40 INFO Executor: Finished task 0.0 in stage 12.0 (TID 5). 3483 bytes result sent to driver\n",
            "23/04/30 22:08:40 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 5) in 256 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 22:08:40 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
            "23/04/30 22:08:40 INFO DAGScheduler: ResultStage 12 (csv at NativeMethodAccessorImpl.java:0) finished in 0.334 s\n",
            "23/04/30 22:08:40 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 22:08:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
            "23/04/30 22:08:40 INFO DAGScheduler: Job 5 finished: csv at NativeMethodAccessorImpl.java:0, took 0.363617 s\n",
            "23/04/30 22:08:40 INFO FileFormatWriter: Start to commit write Job e4e9e693-da19-4a2c-90d7-9b1eaf451d63.\n",
            "23/04/30 22:08:40 INFO FileFormatWriter: Write Job e4e9e693-da19-4a2c-90d7-9b1eaf451d63 committed. Elapsed time: 46 ms.\n",
            "23/04/30 22:08:40 INFO FileFormatWriter: Finished processing stats for write job e4e9e693-da19-4a2c-90d7-9b1eaf451d63.\n",
            "23/04/30 22:08:40 INFO SparkUI: Stopped Spark web UI at http://145f628fbc4b:4040\n",
            "23/04/30 22:08:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/30 22:08:40 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/30 22:08:40 INFO BlockManager: BlockManager stopped\n",
            "23/04/30 22:08:40 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/30 22:08:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/30 22:08:40 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/30 22:08:40 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/30 22:08:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69/pyspark-0a2f3abd-8a3e-45be-ab3d-5add744beaca\n",
            "23/04/30 22:08:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-82775e9d-01a2-48ca-b680-58252e46626c\n",
            "23/04/30 22:08:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-e4d2037f-6341-440b-9fef-729c1fc10b69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rename_file()\n",
        "with open('localtest2.out/results.csv') as f:\n",
        "  lines = f.readlines()\n",
        "  time_hist = pd.read_csv('localtest2.out/results.csv', header=None, names=['_col0', '_col1'])\n",
        "time_hist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "s_AvHgC8NbEN",
        "outputId": "3b15105d-79ce-4506-9a06-aac695895e5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renamed part-00000-773906d6-63e8-4d73-b469-85f085b701c7-c000.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   _col0  _col1\n",
              "0   2019  11087\n",
              "1   2020   9205\n",
              "2   2017   4215\n",
              "3   2018   3970\n",
              "4   2021   1288"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-228c3903-677b-4bee-9079-0e5edc5e4a6e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_col0</th>\n",
              "      <th>_col1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019</td>\n",
              "      <td>11087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020</td>\n",
              "      <td>9205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017</td>\n",
              "      <td>4215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018</td>\n",
              "      <td>3970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021</td>\n",
              "      <td>1288</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-228c3903-677b-4bee-9079-0e5edc5e4a6e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-228c3903-677b-4bee-9079-0e5edc5e4a6e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-228c3903-677b-4bee-9079-0e5edc5e4a6e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(11.7, 8.27)\n",
        "sns.barplot(data=time_hist, x=\"_col0\", y=\"_col1\", color='steelblue', order=list(range(2017, 2022))).set(\n",
        "    title='Distribution of Incidents by Year', xlabel='Year of Incident', ylabel='Total Count')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "id": "3EriWzU3NcH1",
        "outputId": "84bcdb71-4ac6-4c9d-9ad2-9453cf951c93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0.5, 1.0, 'Distribution of Incidents by Year'),\n",
              " Text(0.5, 0, 'Year of Incident'),\n",
              " Text(0, 0.5, 'Total Count')]"
            ]
          },
          "metadata": {},
          "execution_count": 160
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1170x827 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's now try to partition a month of data by segments of "
      ],
      "metadata": {
        "id": "1cB7BGRIJutE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file localtest2.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('localtest2').getOrCreate()\n",
        "\n",
        "try:\n",
        "  # read in parquet file\n",
        "  # part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n",
        "  incidents_df = spark.read.parquet('2021-1-weather.parquet')\n",
        "  incidents_df.createOrReplaceTempView(\"weather\")\n",
        "\n",
        "  # run SQL query\n",
        "\n",
        "  \"\"\"\n",
        "  From: https://sparkbyexamples.com/pyspark/pyspark-select-first-row-of-each-group/#:~:text=Using%20the%20PySpark%20filter(),first%20row%20of%20each%20group.\n",
        "  SELECT Name, Department, Salary FROM (\n",
        "     SELECT *, row_number() OVER (\n",
        "       PARTITION BY department \n",
        "       ORDER BY salary\n",
        "     ) as rn\n",
        "     FROM EMP) tmp \n",
        "  WHERE rn = 1\")\n",
        "  \"\"\"\n",
        "\n",
        "  query=\"\"\"\n",
        "  SELECT DAY(timestamp_local), HOUR(timestamp_local), station_id, precip, temp FROM(\n",
        "    SELECT *, row_number() OVER (\n",
        "      PARTITION BY DAY(timestamp_local), HOUR(timestamp_local)\n",
        "      ORDER BY timestamp_local\n",
        "    ) as rn\n",
        "    FROM weather \n",
        "    WHERE station_id='KBNA') tmp\n",
        "  WHERE rn = 1\n",
        "  \"\"\"\n",
        "\n",
        "  counts = spark.sql(query)\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = counts\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"localtest2.out\")\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lToMbpKNKIWM",
        "outputId": "ff13f4cd-c3a8-45f1-ab9a-aa3b08a1e0e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting localtest2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 localtest2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh48bEulYeQF",
        "outputId": "c636524c-34aa-4aca-a373-15b86c63555e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-117694c3-7743-4835-adf1-b445edb15359;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 914ms :: artifacts dl 42ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-117694c3-7743-4835-adf1-b445edb15359\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/23ms)\n",
            "23/04/30 22:57:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/30 22:57:07 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/04/30 22:57:07 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 22:57:07 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/30 22:57:07 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 22:57:07 INFO SparkContext: Submitted application: localtest2\n",
            "23/04/30 22:57:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/30 22:57:07 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/30 22:57:07 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/30 22:57:07 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/30 22:57:07 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/30 22:57:07 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/30 22:57:07 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/30 22:57:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/30 22:57:08 INFO Utils: Successfully started service 'sparkDriver' on port 46035.\n",
            "23/04/30 22:57:08 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/30 22:57:08 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/30 22:57:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/30 22:57:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/30 22:57:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/30 22:57:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-416ca93e-1965-4f6e-a16b-907b0ad1c0f8\n",
            "23/04/30 22:57:08 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/30 22:57:08 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/30 22:57:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/30 22:57:09 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://145f628fbc4b:4040\n",
            "23/04/30 22:57:09 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://145f628fbc4b:46035/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://145f628fbc4b:46035/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://145f628fbc4b:46035/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://145f628fbc4b:46035/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://145f628fbc4b:46035/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://145f628fbc4b:46035/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://145f628fbc4b:46035/jars/com.101tec_zkclient-0.3.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://145f628fbc4b:46035/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://145f628fbc4b:46035/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://145f628fbc4b:46035/jars/log4j_log4j-1.2.17.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://145f628fbc4b:46035/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://145f628fbc4b:46035/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 22:57:09 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 22:57:09 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 22:57:09 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 22:57:09 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 22:57:09 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 22:57:09 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 22:57:09 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 22:57:09 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 22:57:09 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/log4j_log4j-1.2.17.jar\n",
            "23/04/30 22:57:09 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 22:57:09 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 22:57:09 INFO Executor: Starting executor ID driver on host 145f628fbc4b\n",
            "23/04/30 22:57:09 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/log4j_log4j-1.2.17.jar\n",
            "23/04/30 22:57:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 22:57:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 22:57:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 22:57:09 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 22:57:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 22:57:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 22:57:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 22:57:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 22:57:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 22:57:09 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 22:57:09 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:09 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 22:57:09 INFO Executor: Fetching spark://145f628fbc4b:46035/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:10 INFO TransportClientFactory: Successfully created connection to 145f628fbc4b/172.28.0.12:46035 after 89 ms (0 ms spent in bootstraps)\n",
            "23/04/30 22:57:10 INFO Utils: Fetching spark://145f628fbc4b:46035/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp5157331535564256531.tmp\n",
            "23/04/30 22:57:10 INFO Utils: /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp5157331535564256531.tmp has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 22:57:10 INFO Executor: Adding file:/tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/30 22:57:10 INFO Executor: Fetching spark://145f628fbc4b:46035/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:10 INFO Utils: Fetching spark://145f628fbc4b:46035/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp514926784637807822.tmp\n",
            "23/04/30 22:57:10 INFO Utils: /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp514926784637807822.tmp has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 22:57:10 INFO Executor: Adding file:/tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/30 22:57:10 INFO Executor: Fetching spark://145f628fbc4b:46035/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:10 INFO Utils: Fetching spark://145f628fbc4b:46035/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp3454105379670884025.tmp\n",
            "23/04/30 22:57:10 INFO Utils: /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp3454105379670884025.tmp has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 22:57:10 INFO Executor: Adding file:/tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/30 22:57:10 INFO Executor: Fetching spark://145f628fbc4b:46035/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:10 INFO Utils: Fetching spark://145f628fbc4b:46035/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp116172914733067209.tmp\n",
            "23/04/30 22:57:10 INFO Utils: /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp116172914733067209.tmp has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 22:57:10 INFO Executor: Adding file:/tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/30 22:57:10 INFO Executor: Fetching spark://145f628fbc4b:46035/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:10 INFO Utils: Fetching spark://145f628fbc4b:46035/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp6599984215360554474.tmp\n",
            "23/04/30 22:57:10 INFO Utils: /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp6599984215360554474.tmp has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 22:57:10 INFO Executor: Adding file:/tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/30 22:57:10 INFO Executor: Fetching spark://145f628fbc4b:46035/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:10 INFO Utils: Fetching spark://145f628fbc4b:46035/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp693006267752908067.tmp\n",
            "23/04/30 22:57:10 INFO Utils: /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp693006267752908067.tmp has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 22:57:10 INFO Executor: Adding file:/tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/30 22:57:10 INFO Executor: Fetching spark://145f628fbc4b:46035/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:10 INFO Utils: Fetching spark://145f628fbc4b:46035/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp8161057024216147234.tmp\n",
            "23/04/30 22:57:10 INFO Utils: /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp8161057024216147234.tmp has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 22:57:10 INFO Executor: Adding file:/tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/30 22:57:10 INFO Executor: Fetching spark://145f628fbc4b:46035/jars/com.101tec_zkclient-0.3.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:10 INFO Utils: Fetching spark://145f628fbc4b:46035/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp1048983662380269238.tmp\n",
            "23/04/30 22:57:10 INFO Utils: /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp1048983662380269238.tmp has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 22:57:10 INFO Executor: Adding file:/tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/30 22:57:10 INFO Executor: Fetching spark://145f628fbc4b:46035/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:10 INFO Utils: Fetching spark://145f628fbc4b:46035/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp2748813897085097051.tmp\n",
            "23/04/30 22:57:10 INFO Utils: /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp2748813897085097051.tmp has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 22:57:10 INFO Executor: Adding file:/tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/30 22:57:10 INFO Executor: Fetching spark://145f628fbc4b:46035/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:10 INFO Utils: Fetching spark://145f628fbc4b:46035/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp4080659758194071860.tmp\n",
            "23/04/30 22:57:10 INFO Utils: /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp4080659758194071860.tmp has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 22:57:10 INFO Executor: Adding file:/tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/30 22:57:10 INFO Executor: Fetching spark://145f628fbc4b:46035/jars/log4j_log4j-1.2.17.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:10 INFO Utils: Fetching spark://145f628fbc4b:46035/jars/log4j_log4j-1.2.17.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp6076438832625373203.tmp\n",
            "23/04/30 22:57:10 INFO Utils: /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp6076438832625373203.tmp has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/log4j_log4j-1.2.17.jar\n",
            "23/04/30 22:57:10 INFO Executor: Adding file:/tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/30 22:57:10 INFO Executor: Fetching spark://145f628fbc4b:46035/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682895427360\n",
            "23/04/30 22:57:10 INFO Utils: Fetching spark://145f628fbc4b:46035/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp5448548843718465153.tmp\n",
            "23/04/30 22:57:10 INFO Utils: /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/fetchFileTemp5448548843718465153.tmp has been previously copied to /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 22:57:10 INFO Executor: Adding file:/tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/userFiles-2afdd686-d6c0-4a04-9d04-bc5c2136b517/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/30 22:57:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36551.\n",
            "23/04/30 22:57:10 INFO NettyBlockTransferService: Server created on 145f628fbc4b:36551\n",
            "23/04/30 22:57:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/30 22:57:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 145f628fbc4b, 36551, None)\n",
            "23/04/30 22:57:10 INFO BlockManagerMasterEndpoint: Registering block manager 145f628fbc4b:36551 with 366.3 MiB RAM, BlockManagerId(driver, 145f628fbc4b, 36551, None)\n",
            "23/04/30 22:57:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 145f628fbc4b, 36551, None)\n",
            "23/04/30 22:57:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 145f628fbc4b, 36551, None)\n",
            "23/04/30 22:57:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/04/30 22:57:11 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/04/30 22:57:13 INFO InMemoryFileIndex: It took 107 ms to list leaf files for 1 paths.\n",
            "23/04/30 22:57:14 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 22:57:15 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 22:57:15 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 22:57:15 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 22:57:15 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 22:57:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 22:57:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/04/30 22:57:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/04/30 22:57:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 145f628fbc4b:36551 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 22:57:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 22:57:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 22:57:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/30 22:57:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4584 bytes) taskResourceAssignments Map()\n",
            "23/04/30 22:57:15 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/30 22:57:17 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3118 bytes result sent to driver\n",
            "23/04/30 22:57:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1382 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 22:57:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/30 22:57:17 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.982 s\n",
            "23/04/30 22:57:17 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 22:57:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/30 22:57:17 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.124618 s\n",
            "23/04/30 22:57:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 145f628fbc4b:36551 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 22:57:20 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "23/04/30 22:57:21 INFO FileSourceStrategy: Pushed Filters: IsNotNull(station_id),EqualTo(station_id,KBNA)\n",
            "23/04/30 22:57:21 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(station_id#0),(station_id#0 = KBNA)\n",
            "23/04/30 22:57:21 INFO FileSourceStrategy: Output Data Schema: struct<station_id: string, timestamp_local: timestamp, temp: double, precip: double ... 2 more fields>\n",
            "23/04/30 22:57:21 INFO CodeGenerator: Code generated in 304.680143 ms\n",
            "23/04/30 22:57:22 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 353.0 KiB, free 366.0 MiB)\n",
            "23/04/30 22:57:22 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 365.9 MiB)\n",
            "23/04/30 22:57:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 145f628fbc4b:36551 (size: 35.7 KiB, free: 366.3 MiB)\n",
            "23/04/30 22:57:22 INFO SparkContext: Created broadcast 1 from csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 22:57:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/30 22:57:22 INFO DAGScheduler: Registering RDD 5 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/30 22:57:22 INFO DAGScheduler: Got map stage job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 22:57:22 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 22:57:22 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 22:57:22 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 22:57:22 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 22:57:22 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 19.4 KiB, free 365.9 MiB)\n",
            "23/04/30 22:57:22 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 365.9 MiB)\n",
            "23/04/30 22:57:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 145f628fbc4b:36551 (size: 8.4 KiB, free: 366.3 MiB)\n",
            "23/04/30 22:57:22 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 22:57:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 22:57:22 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/04/30 22:57:22 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4852 bytes) taskResourceAssignments Map()\n",
            "23/04/30 22:57:22 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/30 22:57:22 INFO CodeGenerator: Code generated in 73.292239 ms\n",
            "23/04/30 22:57:22 INFO FileScanRDD: Reading File path: file:///content/2021-1-weather.parquet, range: 0-3369951, partition values: [empty row]\n",
            "23/04/30 22:57:22 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/04/30 22:57:22 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/04/30 22:57:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2327 bytes result sent to driver\n",
            "23/04/30 22:57:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1271 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 22:57:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/30 22:57:23 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 1.348 s\n",
            "23/04/30 22:57:23 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 22:57:23 INFO DAGScheduler: running: Set()\n",
            "23/04/30 22:57:23 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 22:57:23 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 22:57:23 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/30 22:57:23 INFO CodeGenerator: Code generated in 46.238852 ms\n",
            "23/04/30 22:57:23 INFO CodeGenerator: Code generated in 58.231026 ms\n",
            "23/04/30 22:57:23 INFO DAGScheduler: Registering RDD 10 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/30 22:57:23 INFO DAGScheduler: Got map stage job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 22:57:23 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 22:57:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/30 22:57:23 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 22:57:23 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 22:57:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 37.5 KiB, free 365.9 MiB)\n",
            "23/04/30 22:57:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 365.8 MiB)\n",
            "23/04/30 22:57:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 145f628fbc4b:36551 (size: 17.6 KiB, free: 366.2 MiB)\n",
            "23/04/30 22:57:24 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 22:57:24 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 22:57:24 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/30 22:57:24 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/30 22:57:24 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
            "23/04/30 22:57:24 INFO ShuffleBlockFetcherIterator: Getting 1 (35.5 KiB) non-empty blocks including 1 (35.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 22:57:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 22 ms\n",
            "23/04/30 22:57:24 INFO CodeGenerator: Code generated in 45.897713 ms\n",
            "23/04/30 22:57:24 INFO CodeGenerator: Code generated in 23.495451 ms\n",
            "23/04/30 22:57:24 INFO CodeGenerator: Code generated in 36.754256 ms\n",
            "23/04/30 22:57:24 INFO CodeGenerator: Code generated in 21.891259 ms\n",
            "23/04/30 22:57:24 INFO CodeGenerator: Code generated in 16.360636 ms\n",
            "23/04/30 22:57:24 INFO CodeGenerator: Code generated in 17.486022 ms\n",
            "23/04/30 22:57:24 INFO CodeGenerator: Code generated in 24.628828 ms\n",
            "23/04/30 22:57:24 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 3750 bytes result sent to driver\n",
            "23/04/30 22:57:24 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 569 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 22:57:24 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 0.603 s\n",
            "23/04/30 22:57:24 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 22:57:24 INFO DAGScheduler: running: Set()\n",
            "23/04/30 22:57:24 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 22:57:24 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 22:57:24 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/30 22:57:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 22:57:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 22:57:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 22:57:24 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 22:57:24 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 22:57:24 INFO DAGScheduler: Final stage: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 22:57:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
            "23/04/30 22:57:24 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 22:57:24 INFO DAGScheduler: Submitting ResultStage 6 (ShuffledRowRDD[11] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 22:57:24 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 211.9 KiB, free 365.6 MiB)\n",
            "23/04/30 22:57:24 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 75.8 KiB, free 365.6 MiB)\n",
            "23/04/30 22:57:24 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 145f628fbc4b:36551 (size: 75.8 KiB, free: 366.2 MiB)\n",
            "23/04/30 22:57:24 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 22:57:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (ShuffledRowRDD[11] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 22:57:24 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "23/04/30 22:57:24 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 3) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/30 22:57:24 INFO Executor: Running task 0.0 in stage 6.0 (TID 3)\n",
            "23/04/30 22:57:24 INFO ShuffleBlockFetcherIterator: Getting 1 (9.2 KiB) non-empty blocks including 1 (9.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 22:57:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 22:57:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 22:57:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 22:57:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 22:57:25 INFO FileOutputCommitter: Saved output of task 'attempt_202304302257247921197744577467345_0006_m_000000_3' to file:/content/localtest2.out/_temporary/0/task_202304302257247921197744577467345_0006_m_000000\n",
            "23/04/30 22:57:25 INFO SparkHadoopMapRedUtil: attempt_202304302257247921197744577467345_0006_m_000000_3: Committed\n",
            "23/04/30 22:57:25 INFO Executor: Finished task 0.0 in stage 6.0 (TID 3). 3483 bytes result sent to driver\n",
            "23/04/30 22:57:25 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 3) in 259 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 22:57:25 INFO DAGScheduler: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0) finished in 0.304 s\n",
            "23/04/30 22:57:25 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 22:57:25 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "23/04/30 22:57:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "23/04/30 22:57:25 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 0.321909 s\n",
            "23/04/30 22:57:25 INFO FileFormatWriter: Start to commit write Job 43f4fc47-d633-49cf-934b-117bc85e871d.\n",
            "23/04/30 22:57:25 INFO FileFormatWriter: Write Job 43f4fc47-d633-49cf-934b-117bc85e871d committed. Elapsed time: 20 ms.\n",
            "23/04/30 22:57:25 INFO FileFormatWriter: Finished processing stats for write job 43f4fc47-d633-49cf-934b-117bc85e871d.\n",
            "23/04/30 22:57:25 INFO SparkUI: Stopped Spark web UI at http://145f628fbc4b:4040\n",
            "23/04/30 22:57:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/30 22:57:25 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/30 22:57:25 INFO BlockManager: BlockManager stopped\n",
            "23/04/30 22:57:25 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/30 22:57:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/30 22:57:25 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/30 22:57:25 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/30 22:57:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3/pyspark-539a792f-52cc-436d-bcba-5716111a36d1\n",
            "23/04/30 22:57:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-95b7d0ca-30f5-4fc4-8103-380678d85839\n",
            "23/04/30 22:57:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-3e67ed29-c83e-4d7e-8e7d-d0dc5e0193c3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rename_file()\n",
        "with open('localtest2.out/results.csv') as f:\n",
        "  lines = f.readlines()\n",
        "  time_hist = pd.read_csv('localtest2.out/results.csv', header=None, names=['day', 'hour', 'station', 'precip', 'temp'])\n",
        "time_hist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ZROoUtUGYskV",
        "outputId": "239979f2-481a-4f23-9a98-e4ab099793ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     day  hour station  precip  temp\n",
              "0      1     0    KBNA     1.5   7.2\n",
              "1      1     1    KBNA     0.8   7.2\n",
              "2      1     2    KBNA     0.8   7.8\n",
              "3      1     3    KBNA     4.6   8.3\n",
              "4      1     4    KBNA     1.3   8.9\n",
              "..   ...   ...     ...     ...   ...\n",
              "739   31    19    KBNA     0.0   5.0\n",
              "740   31    20    KBNA     0.0   3.9\n",
              "741   31    21    KBNA     0.0   3.3\n",
              "742   31    22    KBNA     0.0   3.3\n",
              "743   31    23    KBNA     0.0   3.3\n",
              "\n",
              "[744 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c171128d-f204-4a07-aae6-9d322f4b0840\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "      <th>station</th>\n",
              "      <th>precip</th>\n",
              "      <th>temp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>1.5</td>\n",
              "      <td>7.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.8</td>\n",
              "      <td>7.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.8</td>\n",
              "      <td>7.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>4.6</td>\n",
              "      <td>8.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>1.3</td>\n",
              "      <td>8.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>739</th>\n",
              "      <td>31</td>\n",
              "      <td>19</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>740</th>\n",
              "      <td>31</td>\n",
              "      <td>20</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>741</th>\n",
              "      <td>31</td>\n",
              "      <td>21</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>742</th>\n",
              "      <td>31</td>\n",
              "      <td>22</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>743</th>\n",
              "      <td>31</td>\n",
              "      <td>23</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>744 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c171128d-f204-4a07-aae6-9d322f4b0840')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c171128d-f204-4a07-aae6-9d322f4b0840 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c171128d-f204-4a07-aae6-9d322f4b0840');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time_hist.isnull().values.any()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SEQepDkbUgL",
        "outputId": "ed2bb853-3943-4f03-963b-45e24cccdacd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's do an example of partitioning incidents by hour to make it easy to see how we will join the rows"
      ],
      "metadata": {
        "id": "B4iQwnMQemFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file localtest2.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('localtest2').getOrCreate()\n",
        "\n",
        "try:\n",
        "  # read in parquet file\n",
        "  # part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n",
        "  incidents_df = spark.read.parquet('incidents.parquet')\n",
        "  incidents_df.createOrReplaceTempView(\"incidents\")\n",
        "\n",
        "  # run SQL query\n",
        "\n",
        "  \"\"\"\n",
        "  From: https://sparkbyexamples.com/pyspark/pyspark-select-first-row-of-each-group/#:~:text=Using%20the%20PySpark%20filter(),first%20row%20of%20each%20group.\n",
        "  SELECT Name, Department, Salary FROM (\n",
        "     SELECT *, row_number() OVER (\n",
        "       PARTITION BY department \n",
        "       ORDER BY salary\n",
        "     ) as rn\n",
        "     FROM EMP) tmp \n",
        "  WHERE rn = 1\")\n",
        "  \"\"\"\n",
        "\n",
        "  query=\"\"\"\n",
        "  SELECT DAY(time_local), HOUR(time_local), ID_Original, response_time_sec FROM(\n",
        "    SELECT *, row_number() OVER (\n",
        "      PARTITION BY DAY(time_local), HOUR(time_local)\n",
        "      ORDER BY time_local\n",
        "    ) as rn\n",
        "    FROM incidents) tmp\n",
        "  WHERE rn = 1\n",
        "  \"\"\"\n",
        "\n",
        "  counts = spark.sql(query)\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = counts\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"localtest2.out\")\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5unu3i4eudn",
        "outputId": "cfe43b9a-5764-4c86-f8b7-94feccf8493b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting localtest2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 localtest2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAGCDqY9g2hJ",
        "outputId": "02f6dbda-88dd-4972-e8b2-baab0e3e4ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8190eec7-efdd-4619-9461-ee1ed49dcac7;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 1050ms :: artifacts dl 62ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-8190eec7-efdd-4619-9461-ee1ed49dcac7\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/37ms)\n",
            "23/04/30 23:33:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/30 23:33:23 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/04/30 23:33:23 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 23:33:23 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/30 23:33:23 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 23:33:23 INFO SparkContext: Submitted application: localtest2\n",
            "23/04/30 23:33:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/30 23:33:24 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/30 23:33:24 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/30 23:33:24 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/30 23:33:24 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/30 23:33:24 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/30 23:33:24 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/30 23:33:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/30 23:33:25 INFO Utils: Successfully started service 'sparkDriver' on port 45161.\n",
            "23/04/30 23:33:25 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/30 23:33:25 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/30 23:33:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/30 23:33:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/30 23:33:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/30 23:33:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6a3e0c09-0687-4649-9988-c48279e595e2\n",
            "23/04/30 23:33:25 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/30 23:33:25 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/30 23:33:26 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/30 23:33:26 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://145f628fbc4b:4040\n",
            "23/04/30 23:33:26 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://145f628fbc4b:45161/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://145f628fbc4b:45161/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://145f628fbc4b:45161/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://145f628fbc4b:45161/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://145f628fbc4b:45161/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://145f628fbc4b:45161/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://145f628fbc4b:45161/jars/com.101tec_zkclient-0.3.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://145f628fbc4b:45161/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://145f628fbc4b:45161/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://145f628fbc4b:45161/jars/log4j_log4j-1.2.17.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://145f628fbc4b:45161/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://145f628fbc4b:45161/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 23:33:26 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 23:33:26 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 23:33:26 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 23:33:26 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 23:33:26 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 23:33:26 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 23:33:26 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 23:33:26 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 23:33:26 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/log4j_log4j-1.2.17.jar\n",
            "23/04/30 23:33:26 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 23:33:26 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:26 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 23:33:27 INFO Executor: Starting executor ID driver on host 145f628fbc4b\n",
            "23/04/30 23:33:27 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/log4j_log4j-1.2.17.jar\n",
            "23/04/30 23:33:27 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 23:33:27 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 23:33:27 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 23:33:27 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 23:33:27 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 23:33:27 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 23:33:27 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 23:33:27 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 23:33:27 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 23:33:27 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 23:33:27 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 23:33:27 INFO Executor: Fetching spark://145f628fbc4b:45161/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO TransportClientFactory: Successfully created connection to 145f628fbc4b/172.28.0.12:45161 after 60 ms (0 ms spent in bootstraps)\n",
            "23/04/30 23:33:27 INFO Utils: Fetching spark://145f628fbc4b:45161/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp7293503753305517623.tmp\n",
            "23/04/30 23:33:27 INFO Utils: /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp7293503753305517623.tmp has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 23:33:27 INFO Executor: Adding file:/tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/30 23:33:27 INFO Executor: Fetching spark://145f628fbc4b:45161/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: Fetching spark://145f628fbc4b:45161/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp6696522399120925384.tmp\n",
            "23/04/30 23:33:27 INFO Utils: /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp6696522399120925384.tmp has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 23:33:27 INFO Executor: Adding file:/tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/30 23:33:27 INFO Executor: Fetching spark://145f628fbc4b:45161/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: Fetching spark://145f628fbc4b:45161/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp7941375500790047741.tmp\n",
            "23/04/30 23:33:27 INFO Utils: /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp7941375500790047741.tmp has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 23:33:27 INFO Executor: Adding file:/tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/30 23:33:27 INFO Executor: Fetching spark://145f628fbc4b:45161/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: Fetching spark://145f628fbc4b:45161/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp5539217218865238298.tmp\n",
            "23/04/30 23:33:27 INFO Utils: /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp5539217218865238298.tmp has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 23:33:27 INFO Executor: Adding file:/tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/30 23:33:27 INFO Executor: Fetching spark://145f628fbc4b:45161/jars/com.101tec_zkclient-0.3.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: Fetching spark://145f628fbc4b:45161/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp4134861162347707322.tmp\n",
            "23/04/30 23:33:27 INFO Utils: /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp4134861162347707322.tmp has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 23:33:27 INFO Executor: Adding file:/tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/30 23:33:27 INFO Executor: Fetching spark://145f628fbc4b:45161/jars/log4j_log4j-1.2.17.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: Fetching spark://145f628fbc4b:45161/jars/log4j_log4j-1.2.17.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp3462893612073067926.tmp\n",
            "23/04/30 23:33:27 INFO Utils: /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp3462893612073067926.tmp has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/log4j_log4j-1.2.17.jar\n",
            "23/04/30 23:33:27 INFO Executor: Adding file:/tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/30 23:33:27 INFO Executor: Fetching spark://145f628fbc4b:45161/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: Fetching spark://145f628fbc4b:45161/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp3197859589627137598.tmp\n",
            "23/04/30 23:33:27 INFO Utils: /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp3197859589627137598.tmp has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 23:33:27 INFO Executor: Adding file:/tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/30 23:33:27 INFO Executor: Fetching spark://145f628fbc4b:45161/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: Fetching spark://145f628fbc4b:45161/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp8698207496234144950.tmp\n",
            "23/04/30 23:33:27 INFO Utils: /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp8698207496234144950.tmp has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 23:33:27 INFO Executor: Adding file:/tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/30 23:33:27 INFO Executor: Fetching spark://145f628fbc4b:45161/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: Fetching spark://145f628fbc4b:45161/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp2790254209655438700.tmp\n",
            "23/04/30 23:33:27 INFO Utils: /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp2790254209655438700.tmp has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 23:33:27 INFO Executor: Adding file:/tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/30 23:33:27 INFO Executor: Fetching spark://145f628fbc4b:45161/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: Fetching spark://145f628fbc4b:45161/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp5796080371341236617.tmp\n",
            "23/04/30 23:33:27 INFO Utils: /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp5796080371341236617.tmp has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 23:33:27 INFO Executor: Adding file:/tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/30 23:33:27 INFO Executor: Fetching spark://145f628fbc4b:45161/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: Fetching spark://145f628fbc4b:45161/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp4114516501028570619.tmp\n",
            "23/04/30 23:33:27 INFO Utils: /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp4114516501028570619.tmp has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 23:33:27 INFO Executor: Adding file:/tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/30 23:33:27 INFO Executor: Fetching spark://145f628fbc4b:45161/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682897603683\n",
            "23/04/30 23:33:27 INFO Utils: Fetching spark://145f628fbc4b:45161/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp7865871539739878119.tmp\n",
            "23/04/30 23:33:27 INFO Utils: /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/fetchFileTemp7865871539739878119.tmp has been previously copied to /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 23:33:27 INFO Executor: Adding file:/tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/userFiles-3886daec-316c-44dd-bf05-d7948eb1843a/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/30 23:33:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36313.\n",
            "23/04/30 23:33:27 INFO NettyBlockTransferService: Server created on 145f628fbc4b:36313\n",
            "23/04/30 23:33:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/30 23:33:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 145f628fbc4b, 36313, None)\n",
            "23/04/30 23:33:28 INFO BlockManagerMasterEndpoint: Registering block manager 145f628fbc4b:36313 with 366.3 MiB RAM, BlockManagerId(driver, 145f628fbc4b, 36313, None)\n",
            "23/04/30 23:33:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 145f628fbc4b, 36313, None)\n",
            "23/04/30 23:33:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 145f628fbc4b, 36313, None)\n",
            "23/04/30 23:33:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/04/30 23:33:28 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/04/30 23:33:30 INFO InMemoryFileIndex: It took 88 ms to list leaf files for 1 paths.\n",
            "23/04/30 23:33:31 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 23:33:31 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 23:33:31 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 23:33:31 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 23:33:31 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 23:33:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 23:33:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/04/30 23:33:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/04/30 23:33:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 145f628fbc4b:36313 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 23:33:31 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 23:33:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 23:33:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/30 23:33:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4579 bytes) taskResourceAssignments Map()\n",
            "23/04/30 23:33:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/30 23:33:32 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2274 bytes result sent to driver\n",
            "23/04/30 23:33:32 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1141 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 23:33:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/30 23:33:32 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.462 s\n",
            "23/04/30 23:33:32 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 23:33:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/30 23:33:32 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.572192 s\n",
            "23/04/30 23:33:34 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 145f628fbc4b:36313 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 23:33:37 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/30 23:33:37 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "23/04/30 23:33:37 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, time_local: timestamp, response_time_sec: double ... 1 more fields>\n",
            "23/04/30 23:33:38 INFO CodeGenerator: Code generated in 627.724361 ms\n",
            "23/04/30 23:33:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 352.8 KiB, free 366.0 MiB)\n",
            "23/04/30 23:33:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 365.9 MiB)\n",
            "23/04/30 23:33:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 145f628fbc4b:36313 (size: 35.7 KiB, free: 366.3 MiB)\n",
            "23/04/30 23:33:38 INFO SparkContext: Created broadcast 1 from csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 23:33:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/30 23:33:39 INFO DAGScheduler: Registering RDD 5 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/30 23:33:39 INFO DAGScheduler: Got map stage job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 23:33:39 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 23:33:39 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 23:33:39 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 23:33:39 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 23:33:39 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 18.0 KiB, free 365.9 MiB)\n",
            "23/04/30 23:33:39 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 365.9 MiB)\n",
            "23/04/30 23:33:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 145f628fbc4b:36313 (size: 8.0 KiB, free: 366.3 MiB)\n",
            "23/04/30 23:33:39 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 23:33:39 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 23:33:39 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/04/30 23:33:39 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4847 bytes) taskResourceAssignments Map()\n",
            "23/04/30 23:33:39 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/30 23:33:39 INFO CodeGenerator: Code generated in 97.225201 ms\n",
            "23/04/30 23:33:39 INFO FileScanRDD: Reading File path: file:///content/incidents.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/30 23:33:40 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/04/30 23:33:41 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2271 bytes result sent to driver\n",
            "23/04/30 23:33:41 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2159 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 23:33:41 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/30 23:33:41 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 2.288 s\n",
            "23/04/30 23:33:41 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 23:33:41 INFO DAGScheduler: running: Set()\n",
            "23/04/30 23:33:41 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 23:33:41 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 23:33:41 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/30 23:33:41 INFO CodeGenerator: Code generated in 26.937729 ms\n",
            "23/04/30 23:33:41 INFO CodeGenerator: Code generated in 38.31013 ms\n",
            "23/04/30 23:33:41 INFO DAGScheduler: Registering RDD 10 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/30 23:33:41 INFO DAGScheduler: Got map stage job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 23:33:41 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 23:33:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/30 23:33:41 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 23:33:41 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 23:33:41 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 35.6 KiB, free 365.9 MiB)\n",
            "23/04/30 23:33:41 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 365.8 MiB)\n",
            "23/04/30 23:33:41 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 145f628fbc4b:36313 (size: 16.6 KiB, free: 366.2 MiB)\n",
            "23/04/30 23:33:41 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 23:33:41 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 23:33:41 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/30 23:33:41 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/30 23:33:41 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
            "23/04/30 23:33:41 INFO ShuffleBlockFetcherIterator: Getting 1 (931.1 KiB) non-empty blocks including 1 (931.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 23:33:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms\n",
            "23/04/30 23:33:41 INFO CodeGenerator: Code generated in 46.056706 ms\n",
            "23/04/30 23:33:41 INFO CodeGenerator: Code generated in 12.26807 ms\n",
            "23/04/30 23:33:42 INFO CodeGenerator: Code generated in 44.574254 ms\n",
            "23/04/30 23:33:42 INFO CodeGenerator: Code generated in 39.964903 ms\n",
            "23/04/30 23:33:42 INFO CodeGenerator: Code generated in 11.048863 ms\n",
            "23/04/30 23:33:42 INFO CodeGenerator: Code generated in 9.753859 ms\n",
            "23/04/30 23:33:42 INFO CodeGenerator: Code generated in 9.468308 ms\n",
            "23/04/30 23:33:42 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 3694 bytes result sent to driver\n",
            "23/04/30 23:33:42 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 749 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 23:33:42 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 0.777 s\n",
            "23/04/30 23:33:42 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 23:33:42 INFO DAGScheduler: running: Set()\n",
            "23/04/30 23:33:42 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 23:33:42 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 23:33:42 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/30 23:33:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 23:33:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 23:33:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 23:33:42 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 23:33:42 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 23:33:42 INFO DAGScheduler: Final stage: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 23:33:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
            "23/04/30 23:33:42 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 23:33:42 INFO DAGScheduler: Submitting ResultStage 6 (ShuffledRowRDD[11] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 23:33:42 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 211.8 KiB, free 365.6 MiB)\n",
            "23/04/30 23:33:42 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 75.8 KiB, free 365.6 MiB)\n",
            "23/04/30 23:33:42 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 145f628fbc4b:36313 (size: 75.8 KiB, free: 366.2 MiB)\n",
            "23/04/30 23:33:42 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 23:33:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (ShuffledRowRDD[11] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 23:33:42 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "23/04/30 23:33:42 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 3) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/30 23:33:42 INFO Executor: Running task 0.0 in stage 6.0 (TID 3)\n",
            "23/04/30 23:33:42 INFO ShuffleBlockFetcherIterator: Getting 1 (16.3 KiB) non-empty blocks including 1 (16.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 23:33:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 23:33:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 23:33:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 23:33:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 23:33:42 INFO FileOutputCommitter: Saved output of task 'attempt_202304302333421994855377793968754_0006_m_000000_3' to file:/content/localtest2.out/_temporary/0/task_202304302333421994855377793968754_0006_m_000000\n",
            "23/04/30 23:33:42 INFO SparkHadoopMapRedUtil: attempt_202304302333421994855377793968754_0006_m_000000_3: Committed\n",
            "23/04/30 23:33:42 INFO Executor: Finished task 0.0 in stage 6.0 (TID 3). 3483 bytes result sent to driver\n",
            "23/04/30 23:33:42 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 3) in 195 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 23:33:42 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "23/04/30 23:33:42 INFO DAGScheduler: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0) finished in 0.256 s\n",
            "23/04/30 23:33:42 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 23:33:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "23/04/30 23:33:42 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 0.268874 s\n",
            "23/04/30 23:33:42 INFO FileFormatWriter: Start to commit write Job 95705f0c-ee1d-43f5-9e5c-e2fd8a867ab7.\n",
            "23/04/30 23:33:42 INFO FileFormatWriter: Write Job 95705f0c-ee1d-43f5-9e5c-e2fd8a867ab7 committed. Elapsed time: 17 ms.\n",
            "23/04/30 23:33:42 INFO FileFormatWriter: Finished processing stats for write job 95705f0c-ee1d-43f5-9e5c-e2fd8a867ab7.\n",
            "23/04/30 23:33:42 INFO SparkUI: Stopped Spark web UI at http://145f628fbc4b:4040\n",
            "23/04/30 23:33:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/30 23:33:43 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/30 23:33:43 INFO BlockManager: BlockManager stopped\n",
            "23/04/30 23:33:43 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/30 23:33:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/30 23:33:43 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/30 23:33:43 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/30 23:33:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-1596e0d1-7dbc-4790-9896-483ccbea103f\n",
            "23/04/30 23:33:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b\n",
            "23/04/30 23:33:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-04efa149-d58f-40e3-931b-ca3deaf4f13b/pyspark-bcfa651e-91c2-443f-ac6c-b6fa8231d060\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rename_file()\n",
        "with open('localtest2.out/results.csv') as f:\n",
        "  lines = f.readlines()\n",
        "  time_hist = pd.read_csv('localtest2.out/results.csv', header=None, names=['day', 'hour', 'inc_id', 'resp_time'])\n",
        "time_hist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "8fS6mZFZg3V3",
        "outputId": "f996f27b-7df2-4e48-bd0b-dde2ca553f92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renamed part-00000-f0c64c78-c751-49e0-93b0-a2d5e517b69e-c000.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     day  hour                              inc_id  resp_time\n",
              "0      1     0  ObjectId(59d3ab5108f47311c8926b2d)      222.0\n",
              "1      1     1  ObjectId(59d3a81908f47311c891f8e2)      268.0\n",
              "2      1     2  ObjectId(59d3a81908f47311c891f8e6)      512.0\n",
              "3      1     3  ObjectId(59d3a81a08f47311c891f8ef)      271.0\n",
              "4      1     4  ObjectId(59d3a81a08f47311c891f8f9)      366.0\n",
              "..   ...   ...                                 ...        ...\n",
              "739   31    19  ObjectId(59d3ac2b08f47311c8928990)      236.0\n",
              "740   31    20  ObjectId(59d3aa7b08f47311c8924d93)       27.0\n",
              "741   31    21  ObjectId(59d3aa7b08f47311c8924d98)      246.0\n",
              "742   31    22  ObjectId(59d3ac2b08f47311c89289a4)      191.0\n",
              "743   31    23  ObjectId(59d3aa7c08f47311c8924da6)      168.0\n",
              "\n",
              "[744 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-730a671d-a09b-425e-b18f-9dc735d96362\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "      <th>inc_id</th>\n",
              "      <th>resp_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>ObjectId(59d3ab5108f47311c8926b2d)</td>\n",
              "      <td>222.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>ObjectId(59d3a81908f47311c891f8e2)</td>\n",
              "      <td>268.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>ObjectId(59d3a81908f47311c891f8e6)</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>ObjectId(59d3a81a08f47311c891f8ef)</td>\n",
              "      <td>271.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>ObjectId(59d3a81a08f47311c891f8f9)</td>\n",
              "      <td>366.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>739</th>\n",
              "      <td>31</td>\n",
              "      <td>19</td>\n",
              "      <td>ObjectId(59d3ac2b08f47311c8928990)</td>\n",
              "      <td>236.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>740</th>\n",
              "      <td>31</td>\n",
              "      <td>20</td>\n",
              "      <td>ObjectId(59d3aa7b08f47311c8924d93)</td>\n",
              "      <td>27.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>741</th>\n",
              "      <td>31</td>\n",
              "      <td>21</td>\n",
              "      <td>ObjectId(59d3aa7b08f47311c8924d98)</td>\n",
              "      <td>246.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>742</th>\n",
              "      <td>31</td>\n",
              "      <td>22</td>\n",
              "      <td>ObjectId(59d3ac2b08f47311c89289a4)</td>\n",
              "      <td>191.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>743</th>\n",
              "      <td>31</td>\n",
              "      <td>23</td>\n",
              "      <td>ObjectId(59d3aa7c08f47311c8924da6)</td>\n",
              "      <td>168.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>744 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-730a671d-a09b-425e-b18f-9dc735d96362')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-730a671d-a09b-425e-b18f-9dc735d96362 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-730a671d-a09b-425e-b18f-9dc735d96362');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time_hist.isnull().values.any()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkoS5VXhhN2a",
        "outputId": "fb67af45-5471-4e21-ebb8-3a026abb02d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "th_na = time_hist[time_hist.isna().any(axis=1)]\n",
        "th_na"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "ximW_10-haN4",
        "outputId": "1280c7aa-6ac0-47a6-9d69-498a8e4d2ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     day  hour                              inc_id  resp_time\n",
              "642   27    18  ObjectId(5c2ecab795fad0796f1bb92d)        NaN\n",
              "668   28    20  ObjectId(5c2e9bf495fad0643ae72731)        NaN"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e7a1a186-b765-47e6-8fb8-8de844b59dc3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "      <th>inc_id</th>\n",
              "      <th>resp_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>642</th>\n",
              "      <td>27</td>\n",
              "      <td>18</td>\n",
              "      <td>ObjectId(5c2ecab795fad0796f1bb92d)</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>668</th>\n",
              "      <td>28</td>\n",
              "      <td>20</td>\n",
              "      <td>ObjectId(5c2e9bf495fad0643ae72731)</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e7a1a186-b765-47e6-8fb8-8de844b59dc3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e7a1a186-b765-47e6-8fb8-8de844b59dc3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e7a1a186-b765-47e6-8fb8-8de844b59dc3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We can see that there are incidents where there is no available response time... we should keep this in mind for later"
      ],
      "metadata": {
        "id": "lZCvKLZ9hpSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file localtest2.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('localtest2').getOrCreate()\n",
        "\n",
        "try:\n",
        "  # read in parquet file\n",
        "  # part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n",
        "  incidents_df = spark.read.parquet('incidents.parquet')\n",
        "  incidents_df.createOrReplaceTempView(\"incidents\")\n",
        "\n",
        "  # run SQL query\n",
        "\n",
        "  \"\"\"\n",
        "  From: https://sparkbyexamples.com/pyspark/pyspark-select-first-row-of-each-group/#:~:text=Using%20the%20PySpark%20filter(),first%20row%20of%20each%20group.\n",
        "  SELECT Name, Department, Salary FROM (\n",
        "     SELECT *, row_number() OVER (\n",
        "       PARTITION BY department \n",
        "       ORDER BY salary\n",
        "     ) as rn\n",
        "     FROM EMP) tmp \n",
        "  WHERE rn = 1\")\n",
        "  \"\"\"\n",
        "\n",
        "  query=\"\"\"\n",
        "  SELECT YEAR(time_local), MONTH(time_local), DAY(time_local), HOUR(time_local), ID_Original, response_time_sec FROM(\n",
        "    SELECT *, row_number() OVER (\n",
        "      PARTITION BY DAY(time_local), HOUR(time_local)\n",
        "      ORDER BY time_local\n",
        "    ) as rn\n",
        "    FROM incidents) tmp\n",
        "  WHERE rn = 1\n",
        "  \"\"\"\n",
        "\n",
        "  counts = spark.sql(query)\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = counts\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"localtest2.out\")\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass"
      ],
      "metadata": {
        "id": "JfW6WUQtsRxI",
        "outputId": "c9cb7206-d4c8-4563-8a2c-ecedb545ee30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting localtest2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 localtest2.py"
      ],
      "metadata": {
        "id": "_86u2s_-sXjr",
        "outputId": "70a7ccf3-8c84-491e-a8b3-b53fea44031d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1ea4cc39-da5b-47a9-a3b8-039ddb4995d2;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 1411ms :: artifacts dl 75ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-1ea4cc39-da5b-47a9-a3b8-039ddb4995d2\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/35ms)\n",
            "23/05/01 00:23:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/05/01 00:23:43 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/05/01 00:23:43 INFO ResourceUtils: ==============================================================\n",
            "23/05/01 00:23:43 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/05/01 00:23:43 INFO ResourceUtils: ==============================================================\n",
            "23/05/01 00:23:43 INFO SparkContext: Submitted application: localtest2\n",
            "23/05/01 00:23:43 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/05/01 00:23:43 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/05/01 00:23:43 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/05/01 00:23:43 INFO SecurityManager: Changing view acls to: root\n",
            "23/05/01 00:23:43 INFO SecurityManager: Changing modify acls to: root\n",
            "23/05/01 00:23:43 INFO SecurityManager: Changing view acls groups to: \n",
            "23/05/01 00:23:43 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/05/01 00:23:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/05/01 00:23:43 INFO Utils: Successfully started service 'sparkDriver' on port 33325.\n",
            "23/05/01 00:23:43 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/05/01 00:23:44 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/05/01 00:23:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/05/01 00:23:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/05/01 00:23:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/05/01 00:23:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6f2ec874-8268-4149-a7a1-c1ba0ec8e499\n",
            "23/05/01 00:23:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/05/01 00:23:44 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/05/01 00:23:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/05/01 00:23:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://145f628fbc4b:4040\n",
            "23/05/01 00:23:44 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://145f628fbc4b:33325/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:44 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://145f628fbc4b:33325/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:44 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://145f628fbc4b:33325/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:44 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://145f628fbc4b:33325/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:44 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://145f628fbc4b:33325/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:44 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://145f628fbc4b:33325/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:44 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://145f628fbc4b:33325/jars/com.101tec_zkclient-0.3.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:44 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://145f628fbc4b:33325/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:44 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://145f628fbc4b:33325/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:44 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://145f628fbc4b:33325/jars/log4j_log4j-1.2.17.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:44 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://145f628fbc4b:33325/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:44 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://145f628fbc4b:33325/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:44 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:44 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 00:23:44 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:44 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 00:23:44 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:44 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 00:23:44 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:44 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 00:23:44 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:44 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 00:23:45 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 00:23:45 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 00:23:45 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 00:23:45 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 00:23:45 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/log4j_log4j-1.2.17.jar\n",
            "23/05/01 00:23:45 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 00:23:45 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 00:23:45 INFO Executor: Starting executor ID driver on host 145f628fbc4b\n",
            "23/05/01 00:23:45 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/log4j_log4j-1.2.17.jar\n",
            "23/05/01 00:23:45 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 00:23:45 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 00:23:45 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 00:23:45 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 00:23:45 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 00:23:45 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 00:23:45 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 00:23:45 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 00:23:45 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 00:23:45 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 00:23:45 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 00:23:45 INFO Executor: Fetching spark://145f628fbc4b:33325/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO TransportClientFactory: Successfully created connection to 145f628fbc4b/172.28.0.12:33325 after 61 ms (0 ms spent in bootstraps)\n",
            "23/05/01 00:23:45 INFO Utils: Fetching spark://145f628fbc4b:33325/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp5727117625423664484.tmp\n",
            "23/05/01 00:23:45 INFO Utils: /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp5727117625423664484.tmp has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 00:23:45 INFO Executor: Adding file:/tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/05/01 00:23:45 INFO Executor: Fetching spark://145f628fbc4b:33325/jars/log4j_log4j-1.2.17.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: Fetching spark://145f628fbc4b:33325/jars/log4j_log4j-1.2.17.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp3866746028771312379.tmp\n",
            "23/05/01 00:23:45 INFO Utils: /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp3866746028771312379.tmp has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/log4j_log4j-1.2.17.jar\n",
            "23/05/01 00:23:45 INFO Executor: Adding file:/tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/log4j_log4j-1.2.17.jar to class loader\n",
            "23/05/01 00:23:45 INFO Executor: Fetching spark://145f628fbc4b:33325/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: Fetching spark://145f628fbc4b:33325/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp7549481132432263772.tmp\n",
            "23/05/01 00:23:45 INFO Utils: /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp7549481132432263772.tmp has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 00:23:45 INFO Executor: Adding file:/tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/05/01 00:23:45 INFO Executor: Fetching spark://145f628fbc4b:33325/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: Fetching spark://145f628fbc4b:33325/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp4847112014834095043.tmp\n",
            "23/05/01 00:23:45 INFO Utils: /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp4847112014834095043.tmp has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 00:23:45 INFO Executor: Adding file:/tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/05/01 00:23:45 INFO Executor: Fetching spark://145f628fbc4b:33325/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: Fetching spark://145f628fbc4b:33325/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp5985140253868602453.tmp\n",
            "23/05/01 00:23:45 INFO Utils: /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp5985140253868602453.tmp has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 00:23:45 INFO Executor: Adding file:/tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/05/01 00:23:45 INFO Executor: Fetching spark://145f628fbc4b:33325/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: Fetching spark://145f628fbc4b:33325/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp8305572497599832850.tmp\n",
            "23/05/01 00:23:45 INFO Utils: /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp8305572497599832850.tmp has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 00:23:45 INFO Executor: Adding file:/tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/05/01 00:23:45 INFO Executor: Fetching spark://145f628fbc4b:33325/jars/com.101tec_zkclient-0.3.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: Fetching spark://145f628fbc4b:33325/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp3912445889160118927.tmp\n",
            "23/05/01 00:23:45 INFO Utils: /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp3912445889160118927.tmp has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 00:23:45 INFO Executor: Adding file:/tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/05/01 00:23:45 INFO Executor: Fetching spark://145f628fbc4b:33325/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: Fetching spark://145f628fbc4b:33325/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp4770378211029886111.tmp\n",
            "23/05/01 00:23:45 INFO Utils: /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp4770378211029886111.tmp has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 00:23:45 INFO Executor: Adding file:/tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/05/01 00:23:45 INFO Executor: Fetching spark://145f628fbc4b:33325/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: Fetching spark://145f628fbc4b:33325/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp3357830371010825208.tmp\n",
            "23/05/01 00:23:45 INFO Utils: /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp3357830371010825208.tmp has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 00:23:45 INFO Executor: Adding file:/tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/05/01 00:23:45 INFO Executor: Fetching spark://145f628fbc4b:33325/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: Fetching spark://145f628fbc4b:33325/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp3797906876034106642.tmp\n",
            "23/05/01 00:23:45 INFO Utils: /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp3797906876034106642.tmp has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 00:23:45 INFO Executor: Adding file:/tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/05/01 00:23:45 INFO Executor: Fetching spark://145f628fbc4b:33325/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: Fetching spark://145f628fbc4b:33325/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp4675591673934298215.tmp\n",
            "23/05/01 00:23:45 INFO Utils: /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp4675591673934298215.tmp has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 00:23:45 INFO Executor: Adding file:/tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/05/01 00:23:45 INFO Executor: Fetching spark://145f628fbc4b:33325/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682900623291\n",
            "23/05/01 00:23:45 INFO Utils: Fetching spark://145f628fbc4b:33325/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp8324251049958153926.tmp\n",
            "23/05/01 00:23:45 INFO Utils: /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/fetchFileTemp8324251049958153926.tmp has been previously copied to /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 00:23:45 INFO Executor: Adding file:/tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/userFiles-3f295272-75f3-42ed-b63f-efd94e0dd16f/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/05/01 00:23:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34087.\n",
            "23/05/01 00:23:45 INFO NettyBlockTransferService: Server created on 145f628fbc4b:34087\n",
            "23/05/01 00:23:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/05/01 00:23:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 145f628fbc4b, 34087, None)\n",
            "23/05/01 00:23:45 INFO BlockManagerMasterEndpoint: Registering block manager 145f628fbc4b:34087 with 366.3 MiB RAM, BlockManagerId(driver, 145f628fbc4b, 34087, None)\n",
            "23/05/01 00:23:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 145f628fbc4b, 34087, None)\n",
            "23/05/01 00:23:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 145f628fbc4b, 34087, None)\n",
            "23/05/01 00:23:46 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/05/01 00:23:46 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/05/01 00:23:47 INFO InMemoryFileIndex: It took 48 ms to list leaf files for 1 paths.\n",
            "23/05/01 00:23:48 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 00:23:48 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 00:23:48 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 00:23:48 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/05/01 00:23:48 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 00:23:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 00:23:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/05/01 00:23:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/05/01 00:23:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 145f628fbc4b:34087 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/05/01 00:23:48 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 00:23:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 00:23:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/05/01 00:23:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4579 bytes) taskResourceAssignments Map()\n",
            "23/05/01 00:23:48 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/05/01 00:23:50 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2274 bytes result sent to driver\n",
            "23/05/01 00:23:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1689 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/05/01 00:23:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/05/01 00:23:50 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.950 s\n",
            "23/05/01 00:23:50 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/05/01 00:23:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/05/01 00:23:50 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.076566 s\n",
            "23/05/01 00:23:53 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 145f628fbc4b:34087 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/05/01 00:23:56 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/05/01 00:23:56 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "23/05/01 00:23:56 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, time_local: timestamp, response_time_sec: double ... 1 more fields>\n",
            "23/05/01 00:23:57 INFO CodeGenerator: Code generated in 319.798957 ms\n",
            "23/05/01 00:23:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 352.8 KiB, free 366.0 MiB)\n",
            "23/05/01 00:23:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 365.9 MiB)\n",
            "23/05/01 00:23:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 145f628fbc4b:34087 (size: 35.7 KiB, free: 366.3 MiB)\n",
            "23/05/01 00:23:57 INFO SparkContext: Created broadcast 1 from csv at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 00:23:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/05/01 00:23:57 INFO DAGScheduler: Registering RDD 5 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/05/01 00:23:57 INFO DAGScheduler: Got map stage job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 00:23:57 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 00:23:57 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/05/01 00:23:57 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 00:23:57 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 00:23:57 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 18.0 KiB, free 365.9 MiB)\n",
            "23/05/01 00:23:57 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 365.9 MiB)\n",
            "23/05/01 00:23:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 145f628fbc4b:34087 (size: 8.0 KiB, free: 366.3 MiB)\n",
            "23/05/01 00:23:57 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 00:23:57 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 00:23:57 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/05/01 00:23:57 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4847 bytes) taskResourceAssignments Map()\n",
            "23/05/01 00:23:57 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/05/01 00:23:57 INFO CodeGenerator: Code generated in 59.029317 ms\n",
            "23/05/01 00:23:57 INFO FileScanRDD: Reading File path: file:///content/incidents.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/05/01 00:23:58 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/05/01 00:23:59 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2271 bytes result sent to driver\n",
            "23/05/01 00:23:59 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1390 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/05/01 00:23:59 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 1.498 s\n",
            "23/05/01 00:23:59 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/05/01 00:23:59 INFO DAGScheduler: running: Set()\n",
            "23/05/01 00:23:59 INFO DAGScheduler: waiting: Set()\n",
            "23/05/01 00:23:59 INFO DAGScheduler: failed: Set()\n",
            "23/05/01 00:23:59 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/05/01 00:23:59 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/05/01 00:23:59 INFO CodeGenerator: Code generated in 44.176669 ms\n",
            "23/05/01 00:23:59 INFO CodeGenerator: Code generated in 54.777146 ms\n",
            "23/05/01 00:23:59 INFO DAGScheduler: Registering RDD 10 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/05/01 00:23:59 INFO DAGScheduler: Got map stage job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 00:23:59 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 00:23:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/05/01 00:23:59 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 00:23:59 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 00:23:59 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 36.4 KiB, free 365.9 MiB)\n",
            "23/05/01 00:23:59 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 365.8 MiB)\n",
            "23/05/01 00:23:59 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 145f628fbc4b:34087 (size: 16.7 KiB, free: 366.2 MiB)\n",
            "23/05/01 00:23:59 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 00:23:59 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 00:23:59 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/05/01 00:23:59 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/05/01 00:23:59 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
            "23/05/01 00:23:59 INFO ShuffleBlockFetcherIterator: Getting 1 (931.1 KiB) non-empty blocks including 1 (931.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/05/01 00:23:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms\n",
            "23/05/01 00:23:59 INFO CodeGenerator: Code generated in 30.528641 ms\n",
            "23/05/01 00:23:59 INFO CodeGenerator: Code generated in 23.979208 ms\n",
            "23/05/01 00:23:59 INFO CodeGenerator: Code generated in 34.464213 ms\n",
            "23/05/01 00:23:59 INFO CodeGenerator: Code generated in 18.143105 ms\n",
            "23/05/01 00:24:00 INFO CodeGenerator: Code generated in 13.949987 ms\n",
            "23/05/01 00:24:00 INFO CodeGenerator: Code generated in 23.461683 ms\n",
            "23/05/01 00:24:00 INFO CodeGenerator: Code generated in 19.155181 ms\n",
            "23/05/01 00:24:00 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 3694 bytes result sent to driver\n",
            "23/05/01 00:24:00 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 740 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/05/01 00:24:00 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/05/01 00:24:00 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 0.772 s\n",
            "23/05/01 00:24:00 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/05/01 00:24:00 INFO DAGScheduler: running: Set()\n",
            "23/05/01 00:24:00 INFO DAGScheduler: waiting: Set()\n",
            "23/05/01 00:24:00 INFO DAGScheduler: failed: Set()\n",
            "23/05/01 00:24:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/05/01 00:24:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/05/01 00:24:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/05/01 00:24:00 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 00:24:00 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 00:24:00 INFO DAGScheduler: Final stage: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 00:24:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
            "23/05/01 00:24:00 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 00:24:00 INFO DAGScheduler: Submitting ResultStage 6 (ShuffledRowRDD[11] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 00:24:00 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 212.0 KiB, free 365.6 MiB)\n",
            "23/05/01 00:24:00 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 75.8 KiB, free 365.6 MiB)\n",
            "23/05/01 00:24:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 145f628fbc4b:34087 (size: 75.8 KiB, free: 366.2 MiB)\n",
            "23/05/01 00:24:00 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 00:24:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (ShuffledRowRDD[11] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 00:24:00 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "23/05/01 00:24:00 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 3) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/05/01 00:24:00 INFO Executor: Running task 0.0 in stage 6.0 (TID 3)\n",
            "23/05/01 00:24:00 INFO ShuffleBlockFetcherIterator: Getting 1 (17.9 KiB) non-empty blocks including 1 (17.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/05/01 00:24:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/05/01 00:24:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/05/01 00:24:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/05/01 00:24:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/05/01 00:24:00 INFO FileOutputCommitter: Saved output of task 'attempt_202305010024007370420125422608533_0006_m_000000_3' to file:/content/localtest2.out/_temporary/0/task_202305010024007370420125422608533_0006_m_000000\n",
            "23/05/01 00:24:00 INFO SparkHadoopMapRedUtil: attempt_202305010024007370420125422608533_0006_m_000000_3: Committed\n",
            "23/05/01 00:24:00 INFO Executor: Finished task 0.0 in stage 6.0 (TID 3). 3483 bytes result sent to driver\n",
            "23/05/01 00:24:00 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 3) in 192 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/05/01 00:24:00 INFO DAGScheduler: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0) finished in 0.236 s\n",
            "23/05/01 00:24:00 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/05/01 00:24:00 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "23/05/01 00:24:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "23/05/01 00:24:00 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 0.250203 s\n",
            "23/05/01 00:24:00 INFO FileFormatWriter: Start to commit write Job 945c405b-e30f-4f5b-9f74-521b7e5336ff.\n",
            "23/05/01 00:24:00 INFO FileFormatWriter: Write Job 945c405b-e30f-4f5b-9f74-521b7e5336ff committed. Elapsed time: 17 ms.\n",
            "23/05/01 00:24:00 INFO FileFormatWriter: Finished processing stats for write job 945c405b-e30f-4f5b-9f74-521b7e5336ff.\n",
            "23/05/01 00:24:00 INFO SparkUI: Stopped Spark web UI at http://145f628fbc4b:4040\n",
            "23/05/01 00:24:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/05/01 00:24:00 INFO MemoryStore: MemoryStore cleared\n",
            "23/05/01 00:24:00 INFO BlockManager: BlockManager stopped\n",
            "23/05/01 00:24:00 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/05/01 00:24:00 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/05/01 00:24:00 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/05/01 00:24:01 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/05/01 00:24:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-05bb4be3-8fcc-4b98-bff6-3a2cd5d7d316\n",
            "23/05/01 00:24:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8/pyspark-b2ab0efc-7ade-4b29-878f-738afb4fe309\n",
            "23/05/01 00:24:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-4edc30ee-302b-4a8f-8024-152adfb2c7b8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rename_file()\n",
        "with open('localtest2.out/results.csv') as f:\n",
        "  lines = f.readlines()\n",
        "  time_hist = pd.read_csv('localtest2.out/results.csv', header=None, names=['year', 'month', 'day', 'hour', 'inc_id', 'resp_time'])\n",
        "time_hist"
      ],
      "metadata": {
        "id": "E2ccfhGRsbEt",
        "outputId": "578d90dd-bd07-4e91-9b29-e130199ca827",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renamed part-00000-55185bd0-94f8-42c4-a189-8b187664d000-c000.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     year  month  day  hour                              inc_id  resp_time\n",
              "0    2017      5    1     0  ObjectId(59d3ab5108f47311c8926b2d)      222.0\n",
              "1    2017      1    1     1  ObjectId(59d3a81908f47311c891f8e2)      268.0\n",
              "2    2017      1    1     2  ObjectId(59d3a81908f47311c891f8e6)      512.0\n",
              "3    2017      1    1     3  ObjectId(59d3a81a08f47311c891f8ef)      271.0\n",
              "4    2017      1    1     4  ObjectId(59d3a81a08f47311c891f8f9)      366.0\n",
              "..    ...    ...  ...   ...                                 ...        ...\n",
              "739  2017      5   31    19  ObjectId(59d3ac2b08f47311c8928990)      236.0\n",
              "740  2017      3   31    20  ObjectId(59d3aa7b08f47311c8924d93)       27.0\n",
              "741  2017      3   31    21  ObjectId(59d3aa7b08f47311c8924d98)      246.0\n",
              "742  2017      5   31    22  ObjectId(59d3ac2b08f47311c89289a4)      191.0\n",
              "743  2017      3   31    23  ObjectId(59d3aa7c08f47311c8924da6)      168.0\n",
              "\n",
              "[744 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3373da9b-aa05-4609-9bf4-49b6e1f893d4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "      <th>inc_id</th>\n",
              "      <th>resp_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>ObjectId(59d3ab5108f47311c8926b2d)</td>\n",
              "      <td>222.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>ObjectId(59d3a81908f47311c891f8e2)</td>\n",
              "      <td>268.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>ObjectId(59d3a81908f47311c891f8e6)</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>ObjectId(59d3a81a08f47311c891f8ef)</td>\n",
              "      <td>271.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>ObjectId(59d3a81a08f47311c891f8f9)</td>\n",
              "      <td>366.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>739</th>\n",
              "      <td>2017</td>\n",
              "      <td>5</td>\n",
              "      <td>31</td>\n",
              "      <td>19</td>\n",
              "      <td>ObjectId(59d3ac2b08f47311c8928990)</td>\n",
              "      <td>236.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>740</th>\n",
              "      <td>2017</td>\n",
              "      <td>3</td>\n",
              "      <td>31</td>\n",
              "      <td>20</td>\n",
              "      <td>ObjectId(59d3aa7b08f47311c8924d93)</td>\n",
              "      <td>27.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>741</th>\n",
              "      <td>2017</td>\n",
              "      <td>3</td>\n",
              "      <td>31</td>\n",
              "      <td>21</td>\n",
              "      <td>ObjectId(59d3aa7b08f47311c8924d98)</td>\n",
              "      <td>246.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>742</th>\n",
              "      <td>2017</td>\n",
              "      <td>5</td>\n",
              "      <td>31</td>\n",
              "      <td>22</td>\n",
              "      <td>ObjectId(59d3ac2b08f47311c89289a4)</td>\n",
              "      <td>191.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>743</th>\n",
              "      <td>2017</td>\n",
              "      <td>3</td>\n",
              "      <td>31</td>\n",
              "      <td>23</td>\n",
              "      <td>ObjectId(59d3aa7c08f47311c8924da6)</td>\n",
              "      <td>168.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>744 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3373da9b-aa05-4609-9bf4-49b6e1f893d4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3373da9b-aa05-4609-9bf4-49b6e1f893d4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3373da9b-aa05-4609-9bf4-49b6e1f893d4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now, we have buckets for weather by hour and for incidents by hour\n",
        "\n",
        "Let's try to perform an left join between these two queries to get the weather information at the time of accidents in January 2021"
      ],
      "metadata": {
        "id": "PKaFs5EebgL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file localtest2.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('localtest2').getOrCreate()\n",
        "\n",
        "try:\n",
        "  weather_df = spark.read.parquet('2021-1-weather.parquet')\n",
        "  weather_df.createOrReplaceTempView(\"weather\")\n",
        "\n",
        "  incidents_df = spark.read.parquet('incidents.parquet')\n",
        "  incidents_df.createOrReplaceTempView(\"incidents\")\n",
        "\n",
        "  incidents_query=\"\"\"\n",
        "  (SELECT time_local, Incident_ID, response_time_sec FROM(\n",
        "    SELECT *, row_number() OVER (\n",
        "      PARTITION BY DAY(time_local), HOUR(time_local)\n",
        "      ORDER BY time_local\n",
        "    ) as rn\n",
        "    FROM incidents\n",
        "    WHERE MONTH(time_local) = 1 AND YEAR(time_local) = 2021) tmp\n",
        "  WHERE rn = 1) as i\n",
        "  \"\"\"\n",
        "\n",
        "  weather_query=\"\"\"\n",
        "  (SELECT tmp.timestamp_local, tmp.station_id, tmp.precip, tmp.temp FROM(\n",
        "    SELECT *, row_number() OVER (\n",
        "      PARTITION BY DAY(timestamp_local), HOUR(timestamp_local)\n",
        "      ORDER BY timestamp_local\n",
        "    ) as rn\n",
        "    FROM weather \n",
        "    WHERE station_id='KBNA') tmp\n",
        "  WHERE rn = 1) as w\n",
        "  \"\"\"\n",
        "\n",
        "  full_query = \"SELECT * FROM (\" + incidents_query + \") LEFT JOIN (\" + weather_query + \") ON DAY(i.time_local) = DAY(w.timestamp_local) AND HOUR(i.time_local) = HOUR(w.timestamp_local)\"\n",
        "\n",
        "  counts = spark.sql(full_query)\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = counts\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"localtest2.out\")\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSfDSoi3brkC",
        "outputId": "85e2a666-3cc1-4e87-8bce-72a1725ced43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing localtest2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 localtest2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVIUb3GubfA8",
        "outputId": "a9c93b51-427a-4c63-94da-c8671c6bc901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f80378c1-d8e8-46c0-bf1c-6b9f26036f91;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.4.7/spark-streaming-kafka-0-8_2.11-2.4.7.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7!spark-streaming-kafka-0-8_2.11.jar (49ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.kafka#kafka_2.11;0.8.2.1!kafka_2.11.jar (227ms)\n",
            "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
            "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (10ms)\n",
            "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.2/scala-xml_2.11-1.0.2.jar ...\n",
            "\t[SUCCESSFUL ] org.scala-lang.modules#scala-xml_2.11;1.0.2!scala-xml_2.11.jar(bundle) (44ms)\n",
            "downloading https://repo1.maven.org/maven2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar ...\n",
            "\t[SUCCESSFUL ] com.yammer.metrics#metrics-core;2.2.0!metrics-core.jar (14ms)\n",
            "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar ...\n",
            "\t[SUCCESSFUL ] org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0!scala-parser-combinators_2.11.jar(bundle) (27ms)\n",
            "downloading https://repo1.maven.org/maven2/com/101tec/zkclient/0.3/zkclient-0.3.jar ...\n",
            "\t[SUCCESSFUL ] com.101tec#zkclient;0.3!zkclient.jar (12ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;0.8.2.1!kafka-clients.jar (18ms)\n",
            "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...\n",
            "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (11ms)\n",
            "downloading https://repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar ...\n",
            "\t[SUCCESSFUL ] log4j#log4j;1.2.17!log4j.jar(bundle) (40ms)\n",
            "downloading https://repo1.maven.org/maven2/net/jpountz/lz4/lz4/1.2.0/lz4-1.2.0.jar ...\n",
            "\t[SUCCESSFUL ] net.jpountz.lz4#lz4;1.2.0!lz4.jar (17ms)\n",
            "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.7.5/snappy-java-1.1.7.5.jar ...\n",
            "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.7.5!snappy-java.jar(bundle) (59ms)\n",
            ":: resolution report :: resolve 3367ms :: artifacts dl 553ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   12  |   12  |   0   ||   12  |   12  |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-f80378c1-d8e8-46c0-bf1c-6b9f26036f91\n",
            "\tconfs: [default]\n",
            "\t12 artifacts copied, 0 already retrieved (8282kB/51ms)\n",
            "23/05/01 02:13:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/05/01 02:13:36 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/05/01 02:13:36 INFO ResourceUtils: ==============================================================\n",
            "23/05/01 02:13:36 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/05/01 02:13:36 INFO ResourceUtils: ==============================================================\n",
            "23/05/01 02:13:36 INFO SparkContext: Submitted application: localtest2\n",
            "23/05/01 02:13:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/05/01 02:13:36 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/05/01 02:13:36 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/05/01 02:13:36 INFO SecurityManager: Changing view acls to: root\n",
            "23/05/01 02:13:36 INFO SecurityManager: Changing modify acls to: root\n",
            "23/05/01 02:13:36 INFO SecurityManager: Changing view acls groups to: \n",
            "23/05/01 02:13:36 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/05/01 02:13:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/05/01 02:13:37 INFO Utils: Successfully started service 'sparkDriver' on port 41409.\n",
            "23/05/01 02:13:37 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/05/01 02:13:37 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/05/01 02:13:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/05/01 02:13:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/05/01 02:13:37 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/05/01 02:13:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-17c2e7c7-dba3-40a4-8e7d-00f804bf9250\n",
            "23/05/01 02:13:37 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/05/01 02:13:37 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/05/01 02:13:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/05/01 02:13:37 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://39049882e5a5:4040\n",
            "23/05/01 02:13:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://39049882e5a5:41409/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://39049882e5a5:41409/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://39049882e5a5:41409/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://39049882e5a5:41409/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://39049882e5a5:41409/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://39049882e5a5:41409/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://39049882e5a5:41409/jars/com.101tec_zkclient-0.3.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://39049882e5a5:41409/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://39049882e5a5:41409/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://39049882e5a5:41409/jars/log4j_log4j-1.2.17.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://39049882e5a5:41409/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://39049882e5a5:41409/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:37 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 02:13:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:37 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 02:13:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:37 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 02:13:38 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 02:13:38 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 02:13:38 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 02:13:38 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 02:13:38 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 02:13:38 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 02:13:38 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/log4j_log4j-1.2.17.jar\n",
            "23/05/01 02:13:38 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 02:13:38 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 02:13:38 INFO Executor: Starting executor ID driver on host 39049882e5a5\n",
            "23/05/01 02:13:38 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/log4j_log4j-1.2.17.jar\n",
            "23/05/01 02:13:38 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 02:13:38 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 02:13:38 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 02:13:38 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 02:13:38 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 02:13:38 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 02:13:38 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 02:13:38 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 02:13:38 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 02:13:38 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 02:13:38 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 02:13:38 INFO Executor: Fetching spark://39049882e5a5:41409/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO TransportClientFactory: Successfully created connection to 39049882e5a5/172.28.0.12:41409 after 40 ms (0 ms spent in bootstraps)\n",
            "23/05/01 02:13:38 INFO Utils: Fetching spark://39049882e5a5:41409/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp7308029232272465850.tmp\n",
            "23/05/01 02:13:38 INFO Utils: /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp7308029232272465850.tmp has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 02:13:38 INFO Executor: Adding file:/tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/05/01 02:13:38 INFO Executor: Fetching spark://39049882e5a5:41409/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: Fetching spark://39049882e5a5:41409/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp3326587021648919401.tmp\n",
            "23/05/01 02:13:38 INFO Utils: /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp3326587021648919401.tmp has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 02:13:38 INFO Executor: Adding file:/tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/05/01 02:13:38 INFO Executor: Fetching spark://39049882e5a5:41409/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: Fetching spark://39049882e5a5:41409/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp5960491539963839900.tmp\n",
            "23/05/01 02:13:38 INFO Utils: /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp5960491539963839900.tmp has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 02:13:38 INFO Executor: Adding file:/tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/05/01 02:13:38 INFO Executor: Fetching spark://39049882e5a5:41409/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: Fetching spark://39049882e5a5:41409/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp4748968819618317920.tmp\n",
            "23/05/01 02:13:38 INFO Utils: /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp4748968819618317920.tmp has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 02:13:38 INFO Executor: Adding file:/tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/05/01 02:13:38 INFO Executor: Fetching spark://39049882e5a5:41409/jars/com.101tec_zkclient-0.3.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: Fetching spark://39049882e5a5:41409/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp8339913978134712887.tmp\n",
            "23/05/01 02:13:38 INFO Utils: /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp8339913978134712887.tmp has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 02:13:38 INFO Executor: Adding file:/tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/05/01 02:13:38 INFO Executor: Fetching spark://39049882e5a5:41409/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: Fetching spark://39049882e5a5:41409/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp6159985058352844149.tmp\n",
            "23/05/01 02:13:38 INFO Utils: /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp6159985058352844149.tmp has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 02:13:38 INFO Executor: Adding file:/tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/05/01 02:13:38 INFO Executor: Fetching spark://39049882e5a5:41409/jars/log4j_log4j-1.2.17.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: Fetching spark://39049882e5a5:41409/jars/log4j_log4j-1.2.17.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp748631470343100914.tmp\n",
            "23/05/01 02:13:38 INFO Utils: /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp748631470343100914.tmp has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/log4j_log4j-1.2.17.jar\n",
            "23/05/01 02:13:38 INFO Executor: Adding file:/tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/log4j_log4j-1.2.17.jar to class loader\n",
            "23/05/01 02:13:38 INFO Executor: Fetching spark://39049882e5a5:41409/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: Fetching spark://39049882e5a5:41409/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp6136775261001592553.tmp\n",
            "23/05/01 02:13:38 INFO Utils: /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp6136775261001592553.tmp has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 02:13:38 INFO Executor: Adding file:/tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/05/01 02:13:38 INFO Executor: Fetching spark://39049882e5a5:41409/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: Fetching spark://39049882e5a5:41409/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp5702703242163133113.tmp\n",
            "23/05/01 02:13:38 INFO Utils: /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp5702703242163133113.tmp has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 02:13:38 INFO Executor: Adding file:/tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/05/01 02:13:38 INFO Executor: Fetching spark://39049882e5a5:41409/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: Fetching spark://39049882e5a5:41409/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp309615211668103239.tmp\n",
            "23/05/01 02:13:38 INFO Utils: /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp309615211668103239.tmp has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 02:13:38 INFO Executor: Adding file:/tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/05/01 02:13:38 INFO Executor: Fetching spark://39049882e5a5:41409/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: Fetching spark://39049882e5a5:41409/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp208654239703754048.tmp\n",
            "23/05/01 02:13:38 INFO Utils: /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp208654239703754048.tmp has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 02:13:38 INFO Executor: Adding file:/tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/05/01 02:13:38 INFO Executor: Fetching spark://39049882e5a5:41409/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682907216526\n",
            "23/05/01 02:13:38 INFO Utils: Fetching spark://39049882e5a5:41409/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp1599207885120819760.tmp\n",
            "23/05/01 02:13:38 INFO Utils: /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/fetchFileTemp1599207885120819760.tmp has been previously copied to /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 02:13:38 INFO Executor: Adding file:/tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/userFiles-9288b535-6c05-4894-a0a0-66a5d491854c/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/05/01 02:13:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34625.\n",
            "23/05/01 02:13:38 INFO NettyBlockTransferService: Server created on 39049882e5a5:34625\n",
            "23/05/01 02:13:38 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/05/01 02:13:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 39049882e5a5, 34625, None)\n",
            "23/05/01 02:13:38 INFO BlockManagerMasterEndpoint: Registering block manager 39049882e5a5:34625 with 366.3 MiB RAM, BlockManagerId(driver, 39049882e5a5, 34625, None)\n",
            "23/05/01 02:13:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 39049882e5a5, 34625, None)\n",
            "23/05/01 02:13:38 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 39049882e5a5, 34625, None)\n",
            "23/05/01 02:13:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/05/01 02:13:39 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/05/01 02:13:40 INFO InMemoryFileIndex: It took 46 ms to list leaf files for 1 paths.\n",
            "23/05/01 02:13:41 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 02:13:42 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 02:13:42 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 02:13:42 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/05/01 02:13:42 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 02:13:42 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 02:13:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/05/01 02:13:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/05/01 02:13:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 39049882e5a5:34625 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/05/01 02:13:42 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 02:13:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 02:13:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/05/01 02:13:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (39049882e5a5, executor driver, partition 0, PROCESS_LOCAL, 4584 bytes) taskResourceAssignments Map()\n",
            "23/05/01 02:13:42 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/05/01 02:13:44 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3118 bytes result sent to driver\n",
            "23/05/01 02:13:44 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2109 ms on 39049882e5a5 (executor driver) (1/1)\n",
            "23/05/01 02:13:44 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/05/01 02:13:44 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.714 s\n",
            "23/05/01 02:13:44 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/05/01 02:13:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/05/01 02:13:44 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.931360 s\n",
            "23/05/01 02:13:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 39049882e5a5:34625 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/05/01 02:13:47 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "23/05/01 02:13:48 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "23/05/01 02:13:48 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 02:13:48 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 02:13:48 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 02:13:48 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/05/01 02:13:48 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 02:13:48 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 02:13:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/05/01 02:13:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/05/01 02:13:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 39049882e5a5:34625 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/05/01 02:13:48 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 02:13:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 02:13:48 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/05/01 02:13:48 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (39049882e5a5, executor driver, partition 0, PROCESS_LOCAL, 4579 bytes) taskResourceAssignments Map()\n",
            "23/05/01 02:13:48 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/05/01 02:13:48 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2231 bytes result sent to driver\n",
            "23/05/01 02:13:48 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 62 ms on 39049882e5a5 (executor driver) (1/1)\n",
            "23/05/01 02:13:48 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.110 s\n",
            "23/05/01 02:13:48 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/05/01 02:13:48 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/05/01 02:13:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "23/05/01 02:13:48 INFO DAGScheduler: Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.121114 s\n",
            "23/05/01 02:13:48 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 39049882e5a5:34625 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/05/01 02:13:49 INFO FileSourceStrategy: Pushed Filters: IsNotNull(time_local)\n",
            "23/05/01 02:13:49 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(time_local#75),(month(cast(time_local#75 as date)) = 1),(year(cast(time_local#75 as date)) = 2021)\n",
            "23/05/01 02:13:49 INFO FileSourceStrategy: Output Data Schema: struct<time_local: timestamp, response_time_sec: double, Incident_ID: int ... 1 more fields>\n",
            "23/05/01 02:13:49 INFO FileSourceStrategy: Pushed Filters: IsNotNull(station_id),EqualTo(station_id,KBNA)\n",
            "23/05/01 02:13:49 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(station_id#0),(station_id#0 = KBNA)\n",
            "23/05/01 02:13:49 INFO FileSourceStrategy: Output Data Schema: struct<station_id: string, timestamp_local: timestamp, temp: double, precip: double ... 2 more fields>\n",
            "23/05/01 02:13:50 INFO CodeGenerator: Code generated in 248.468202 ms\n",
            "23/05/01 02:13:50 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 352.8 KiB, free 366.0 MiB)\n",
            "23/05/01 02:13:50 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 365.9 MiB)\n",
            "23/05/01 02:13:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 39049882e5a5:34625 (size: 35.7 KiB, free: 366.3 MiB)\n",
            "23/05/01 02:13:50 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 02:13:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/05/01 02:13:50 INFO DAGScheduler: Registering RDD 7 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/05/01 02:13:50 INFO DAGScheduler: Got map stage job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 02:13:50 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 02:13:50 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/05/01 02:13:50 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 02:13:50 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[7] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 02:13:50 INFO CodeGenerator: Code generated in 80.073462 ms\n",
            "23/05/01 02:13:50 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 19.3 KiB, free 365.9 MiB)\n",
            "23/05/01 02:13:50 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 353.0 KiB, free 365.6 MiB)\n",
            "23/05/01 02:13:50 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 365.5 MiB)\n",
            "23/05/01 02:13:50 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 39049882e5a5:34625 (size: 8.3 KiB, free: 366.3 MiB)\n",
            "23/05/01 02:13:50 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 02:13:50 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[7] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 02:13:50 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "23/05/01 02:13:50 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (39049882e5a5, executor driver, partition 0, PROCESS_LOCAL, 4847 bytes) taskResourceAssignments Map()\n",
            "23/05/01 02:13:50 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "23/05/01 02:13:50 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 365.5 MiB)\n",
            "23/05/01 02:13:50 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 39049882e5a5:34625 (size: 35.7 KiB, free: 366.2 MiB)\n",
            "23/05/01 02:13:50 INFO SparkContext: Created broadcast 3 from csv at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 02:13:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/05/01 02:13:50 INFO DAGScheduler: Registering RDD 11 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/05/01 02:13:50 INFO DAGScheduler: Got map stage job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 02:13:50 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 02:13:50 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/05/01 02:13:50 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 02:13:50 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[11] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 02:13:50 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 19.4 KiB, free 365.5 MiB)\n",
            "23/05/01 02:13:50 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 365.5 MiB)\n",
            "23/05/01 02:13:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 39049882e5a5:34625 (size: 8.4 KiB, free: 366.2 MiB)\n",
            "23/05/01 02:13:50 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 02:13:50 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[11] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 02:13:50 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/05/01 02:13:50 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (39049882e5a5, executor driver, partition 0, PROCESS_LOCAL, 4852 bytes) taskResourceAssignments Map()\n",
            "23/05/01 02:13:50 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "23/05/01 02:13:50 INFO CodeGenerator: Code generated in 63.982736 ms\n",
            "23/05/01 02:13:50 INFO CodeGenerator: Code generated in 66.376628 ms\n",
            "23/05/01 02:13:51 INFO FileScanRDD: Reading File path: file:///content/2021-1-weather.parquet, range: 0-3369951, partition values: [empty row]\n",
            "23/05/01 02:13:51 INFO FileScanRDD: Reading File path: file:///content/incidents.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/05/01 02:13:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 02:13:51 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/05/01 02:13:51 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/05/01 02:13:52 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2327 bytes result sent to driver\n",
            "23/05/01 02:13:52 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2284 bytes result sent to driver\n",
            "23/05/01 02:13:52 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1509 ms on 39049882e5a5 (executor driver) (1/1)\n",
            "23/05/01 02:13:52 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "23/05/01 02:13:52 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1402 ms on 39049882e5a5 (executor driver) (1/1)\n",
            "23/05/01 02:13:52 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/05/01 02:13:52 INFO DAGScheduler: ShuffleMapStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 1.625 s\n",
            "23/05/01 02:13:52 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/05/01 02:13:52 INFO DAGScheduler: running: Set(ShuffleMapStage 3)\n",
            "23/05/01 02:13:52 INFO DAGScheduler: waiting: Set()\n",
            "23/05/01 02:13:52 INFO DAGScheduler: failed: Set()\n",
            "23/05/01 02:13:52 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 1.455 s\n",
            "23/05/01 02:13:52 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/05/01 02:13:52 INFO DAGScheduler: running: Set()\n",
            "23/05/01 02:13:52 INFO DAGScheduler: waiting: Set()\n",
            "23/05/01 02:13:52 INFO DAGScheduler: failed: Set()\n",
            "23/05/01 02:13:52 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/05/01 02:13:52 INFO CodeGenerator: Code generated in 65.767232 ms\n",
            "23/05/01 02:13:52 INFO CodeGenerator: Code generated in 91.233255 ms\n",
            "23/05/01 02:13:53 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
            "23/05/01 02:13:53 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
            "23/05/01 02:13:53 INFO DAGScheduler: Final stage: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
            "23/05/01 02:13:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
            "23/05/01 02:13:53 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 02:13:53 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[16] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
            "23/05/01 02:13:53 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 36.1 KiB, free 365.5 MiB)\n",
            "23/05/01 02:13:53 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 365.4 MiB)\n",
            "23/05/01 02:13:53 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 39049882e5a5:34625 (size: 16.9 KiB, free: 366.2 MiB)\n",
            "23/05/01 02:13:53 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 02:13:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[16] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 02:13:53 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "23/05/01 02:13:53 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (39049882e5a5, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/05/01 02:13:53 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)\n",
            "23/05/01 02:13:53 INFO ShuffleBlockFetcherIterator: Getting 1 (35.5 KiB) non-empty blocks including 1 (35.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/05/01 02:13:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms\n",
            "23/05/01 02:13:53 INFO CodeGenerator: Code generated in 37.44856 ms\n",
            "23/05/01 02:13:53 INFO CodeGenerator: Code generated in 22.105744 ms\n",
            "23/05/01 02:13:53 INFO CodeGenerator: Code generated in 94.795281 ms\n",
            "23/05/01 02:13:53 INFO CodeGenerator: Code generated in 69.928026 ms\n",
            "23/05/01 02:13:54 INFO CodeGenerator: Code generated in 40.936827 ms\n",
            "23/05/01 02:13:54 INFO CodeGenerator: Code generated in 24.411296 ms\n",
            "23/05/01 02:13:54 INFO CodeGenerator: Code generated in 34.456648 ms\n",
            "23/05/01 02:13:54 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 12929 bytes result sent to driver\n",
            "23/05/01 02:13:54 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 981 ms on 39049882e5a5 (executor driver) (1/1)\n",
            "23/05/01 02:13:54 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "23/05/01 02:13:54 INFO DAGScheduler: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1.001 s\n",
            "23/05/01 02:13:54 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/05/01 02:13:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "23/05/01 02:13:54 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 1.048958 s\n",
            "23/05/01 02:13:54 INFO CodeGenerator: Code generated in 32.954376 ms\n",
            "23/05/01 02:13:54 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 1056.0 KiB, free 364.4 MiB)\n",
            "23/05/01 02:13:54 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 18.4 KiB, free 364.4 MiB)\n",
            "23/05/01 02:13:54 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 39049882e5a5:34625 (size: 18.4 KiB, free: 366.2 MiB)\n",
            "23/05/01 02:13:54 INFO SparkContext: Created broadcast 7 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
            "23/05/01 02:13:54 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/05/01 02:13:54 INFO CodeGenerator: Code generated in 32.038199 ms\n",
            "23/05/01 02:13:54 INFO CodeGenerator: Code generated in 35.659543 ms\n",
            "23/05/01 02:13:54 INFO DAGScheduler: Registering RDD 21 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
            "23/05/01 02:13:54 INFO DAGScheduler: Got map stage job 5 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 02:13:54 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 02:13:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
            "23/05/01 02:13:54 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 02:13:54 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 02:13:54 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 41.4 KiB, free 364.3 MiB)\n",
            "23/05/01 02:13:54 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 364.3 MiB)\n",
            "23/05/01 02:13:54 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 39049882e5a5:34625 (size: 19.2 KiB, free: 366.2 MiB)\n",
            "23/05/01 02:13:54 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 02:13:54 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[21] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 02:13:54 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "23/05/01 02:13:54 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 5) (39049882e5a5, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/05/01 02:13:54 INFO Executor: Running task 0.0 in stage 7.0 (TID 5)\n",
            "23/05/01 02:13:54 INFO ShuffleBlockFetcherIterator: Getting 1 (27.1 KiB) non-empty blocks including 1 (27.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/05/01 02:13:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "23/05/01 02:13:54 INFO CodeGenerator: Code generated in 33.974099 ms\n",
            "23/05/01 02:13:54 INFO CodeGenerator: Code generated in 14.6097 ms\n",
            "23/05/01 02:13:54 INFO CodeGenerator: Code generated in 30.912003 ms\n",
            "23/05/01 02:13:54 INFO CodeGenerator: Code generated in 10.042966 ms\n",
            "23/05/01 02:13:54 INFO Executor: Finished task 0.0 in stage 7.0 (TID 5). 3806 bytes result sent to driver\n",
            "23/05/01 02:13:54 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 5) in 215 ms on 39049882e5a5 (executor driver) (1/1)\n",
            "23/05/01 02:13:54 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "23/05/01 02:13:54 INFO DAGScheduler: ShuffleMapStage 7 (csv at NativeMethodAccessorImpl.java:0) finished in 0.239 s\n",
            "23/05/01 02:13:54 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/05/01 02:13:54 INFO DAGScheduler: running: Set()\n",
            "23/05/01 02:13:54 INFO DAGScheduler: waiting: Set()\n",
            "23/05/01 02:13:54 INFO DAGScheduler: failed: Set()\n",
            "23/05/01 02:13:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/05/01 02:13:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/05/01 02:13:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/05/01 02:13:55 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 02:13:55 INFO DAGScheduler: Got job 6 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 02:13:55 INFO DAGScheduler: Final stage: ResultStage 10 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 02:13:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\n",
            "23/05/01 02:13:55 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 02:13:55 INFO DAGScheduler: Submitting ResultStage 10 (ShuffledRowRDD[22] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 02:13:55 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 211.9 KiB, free 364.1 MiB)\n",
            "23/05/01 02:13:55 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 75.8 KiB, free 364.0 MiB)\n",
            "23/05/01 02:13:55 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 39049882e5a5:34625 (size: 75.8 KiB, free: 366.1 MiB)\n",
            "23/05/01 02:13:55 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 02:13:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRowRDD[22] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 02:13:55 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
            "23/05/01 02:13:55 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 6) (39049882e5a5, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/05/01 02:13:55 INFO Executor: Running task 0.0 in stage 10.0 (TID 6)\n",
            "23/05/01 02:13:55 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/05/01 02:13:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
            "23/05/01 02:13:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/05/01 02:13:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/05/01 02:13:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/05/01 02:13:55 INFO FileOutputCommitter: Saved output of task 'attempt_202305010213547339978577857514899_0010_m_000000_6' to file:/content/localtest2.out/_temporary/0/task_202305010213547339978577857514899_0010_m_000000\n",
            "23/05/01 02:13:55 INFO SparkHadoopMapRedUtil: attempt_202305010213547339978577857514899_0010_m_000000_6: Committed\n",
            "23/05/01 02:13:55 INFO Executor: Finished task 0.0 in stage 10.0 (TID 6). 3483 bytes result sent to driver\n",
            "23/05/01 02:13:55 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 6) in 423 ms on 39049882e5a5 (executor driver) (1/1)\n",
            "23/05/01 02:13:55 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "23/05/01 02:13:55 INFO DAGScheduler: ResultStage 10 (csv at NativeMethodAccessorImpl.java:0) finished in 0.543 s\n",
            "23/05/01 02:13:55 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/05/01 02:13:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
            "23/05/01 02:13:55 INFO DAGScheduler: Job 6 finished: csv at NativeMethodAccessorImpl.java:0, took 0.562924 s\n",
            "23/05/01 02:13:55 INFO FileFormatWriter: Start to commit write Job af1c54e9-8e4e-40ed-b793-fb0d2451e6dc.\n",
            "23/05/01 02:13:55 INFO FileFormatWriter: Write Job af1c54e9-8e4e-40ed-b793-fb0d2451e6dc committed. Elapsed time: 36 ms.\n",
            "23/05/01 02:13:55 INFO FileFormatWriter: Finished processing stats for write job af1c54e9-8e4e-40ed-b793-fb0d2451e6dc.\n",
            "23/05/01 02:13:55 INFO SparkUI: Stopped Spark web UI at http://39049882e5a5:4040\n",
            "23/05/01 02:13:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/05/01 02:13:55 INFO MemoryStore: MemoryStore cleared\n",
            "23/05/01 02:13:55 INFO BlockManager: BlockManager stopped\n",
            "23/05/01 02:13:55 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/05/01 02:13:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/05/01 02:13:55 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/05/01 02:13:56 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/05/01 02:13:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf\n",
            "23/05/01 02:13:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-e9bb8f13-d6dd-43b9-a583-47c9d1da2fdf/pyspark-d41dd143-062d-4809-b735-6be31119967f\n",
            "23/05/01 02:13:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-d0a26996-d8ef-401e-96d8-3ac78c5ac8df\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rename_file()\n",
        "with open('localtest2.out/results.csv') as f:\n",
        "  lines = f.readlines()\n",
        "  time_hist = pd.read_csv('localtest2.out/results.csv', header=None, names=['time_local', 'Incident_ID', 'resp_time', 'ts_loc', 'station_id', 'precip', 'temp'])\n",
        "time_hist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "R4gzze98pVHi",
        "outputId": "228b301d-774d-4199-f14d-5ba4f8d83f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   time_local  Incident_ID  resp_time  \\\n",
              "0    2021-01-01T00:07:43.843Z        28487      542.0   \n",
              "1    2021-01-01T01:02:13.027Z        28489      284.0   \n",
              "2    2021-01-01T02:05:23.727Z        28494      319.0   \n",
              "3    2021-01-01T04:38:20.617Z        28498      448.0   \n",
              "4    2021-01-01T05:09:21.057Z        28500      383.0   \n",
              "..                        ...          ...        ...   \n",
              "412  2021-01-31T16:00:24.740Z        29200        NaN   \n",
              "413  2021-01-31T19:43:18.557Z        29203      177.0   \n",
              "414  2021-01-31T21:04:59.320Z        29204      463.0   \n",
              "415  2021-01-31T22:48:15.233Z        29206      459.0   \n",
              "416  2021-01-31T23:44:05.257Z        29208      907.0   \n",
              "\n",
              "                       ts_loc station_id  precip  temp  \n",
              "0    2021-01-01T00:00:00.000Z       KBNA     1.5   7.2  \n",
              "1    2021-01-01T01:00:00.000Z       KBNA     0.8   7.2  \n",
              "2    2021-01-01T02:00:00.000Z       KBNA     0.8   7.8  \n",
              "3    2021-01-01T04:00:00.000Z       KBNA     1.3   8.9  \n",
              "4    2021-01-01T05:00:00.000Z       KBNA     0.5   9.4  \n",
              "..                        ...        ...     ...   ...  \n",
              "412  2021-01-31T16:00:00.000Z       KBNA     0.0   7.8  \n",
              "413  2021-01-31T19:00:00.000Z       KBNA     0.0   5.0  \n",
              "414  2021-01-31T21:00:00.000Z       KBNA     0.0   3.3  \n",
              "415  2021-01-31T22:00:00.000Z       KBNA     0.0   3.3  \n",
              "416  2021-01-31T23:00:00.000Z       KBNA     0.0   3.3  \n",
              "\n",
              "[417 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c47f16ef-5bcc-4b6b-bcdc-8dd2b1979206\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time_local</th>\n",
              "      <th>Incident_ID</th>\n",
              "      <th>resp_time</th>\n",
              "      <th>ts_loc</th>\n",
              "      <th>station_id</th>\n",
              "      <th>precip</th>\n",
              "      <th>temp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-01-01T00:07:43.843Z</td>\n",
              "      <td>28487</td>\n",
              "      <td>542.0</td>\n",
              "      <td>2021-01-01T00:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>1.5</td>\n",
              "      <td>7.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-01-01T01:02:13.027Z</td>\n",
              "      <td>28489</td>\n",
              "      <td>284.0</td>\n",
              "      <td>2021-01-01T01:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.8</td>\n",
              "      <td>7.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-01-01T02:05:23.727Z</td>\n",
              "      <td>28494</td>\n",
              "      <td>319.0</td>\n",
              "      <td>2021-01-01T02:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.8</td>\n",
              "      <td>7.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-01-01T04:38:20.617Z</td>\n",
              "      <td>28498</td>\n",
              "      <td>448.0</td>\n",
              "      <td>2021-01-01T04:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>1.3</td>\n",
              "      <td>8.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-01-01T05:09:21.057Z</td>\n",
              "      <td>28500</td>\n",
              "      <td>383.0</td>\n",
              "      <td>2021-01-01T05:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.5</td>\n",
              "      <td>9.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>412</th>\n",
              "      <td>2021-01-31T16:00:24.740Z</td>\n",
              "      <td>29200</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2021-01-31T16:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413</th>\n",
              "      <td>2021-01-31T19:43:18.557Z</td>\n",
              "      <td>29203</td>\n",
              "      <td>177.0</td>\n",
              "      <td>2021-01-31T19:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414</th>\n",
              "      <td>2021-01-31T21:04:59.320Z</td>\n",
              "      <td>29204</td>\n",
              "      <td>463.0</td>\n",
              "      <td>2021-01-31T21:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>415</th>\n",
              "      <td>2021-01-31T22:48:15.233Z</td>\n",
              "      <td>29206</td>\n",
              "      <td>459.0</td>\n",
              "      <td>2021-01-31T22:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416</th>\n",
              "      <td>2021-01-31T23:44:05.257Z</td>\n",
              "      <td>29208</td>\n",
              "      <td>907.0</td>\n",
              "      <td>2021-01-31T23:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>417 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c47f16ef-5bcc-4b6b-bcdc-8dd2b1979206')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c47f16ef-5bcc-4b6b-bcdc-8dd2b1979206 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c47f16ef-5bcc-4b6b-bcdc-8dd2b1979206');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now, let's add year to our partition and find the left join across all values"
      ],
      "metadata": {
        "id": "dPVlHv9rHNPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with a basic test...\n",
        "I want to check whether I can call spark.read.parquet on a directory of partitioned files and have it work as expected"
      ],
      "metadata": {
        "id": "ClVFGFElJN6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FOR THIS TEST... USE THE FOLLOWING FILE CONFIG\n",
        "```\n",
        "  weather_tn.parquet\n",
        "  +-- month=1/\n",
        "  |   +-- part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n",
        "  +-- month=2/\n",
        "  |   +-- part-00023-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n",
        "```"
      ],
      "metadata": {
        "id": "mJtDmZ7oOaIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PLACE THIS DOWNLOADED FILE IN weather_tn.parquet/month=1/\n",
        "full_path = 'weather_tn.parquet/year=2021/month=1/part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet'\n",
        "\n",
        "local_name = 'part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet'\n",
        "s3.download_file('cs4266-finalproject', full_path, local_name)\n",
        "print(\"Downloaded\", local_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9FDHbMnJWLJ",
        "outputId": "b868c3be-c91f-4a74-e551-8203451ea46b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PLACE THIS DOWNLOADED FILE IN weather_tn.parquet/month=2/\n",
        "full_path = 'weather_tn.parquet/year=2021/month=1/part-00023-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet'\n",
        "\n",
        "local_name = 'part-00023-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet'\n",
        "s3.download_file('cs4266-finalproject', full_path, local_name)\n",
        "print(\"Downloaded\", local_name)"
      ],
      "metadata": {
        "id": "TqGNmbhtN5cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file localtest2.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('localtest2').getOrCreate()\n",
        "\n",
        "try:\n",
        "  # read in parquet file\n",
        "  # part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n",
        "  incidents_df = spark.read.parquet('weather_tn.parquet')\n",
        "  incidents_df.createOrReplaceTempView(\"weather\")\n",
        "\n",
        "  # run SQL query\n",
        "\n",
        "  \"\"\"\n",
        "  From: https://sparkbyexamples.com/pyspark/pyspark-select-first-row-of-each-group/#:~:text=Using%20the%20PySpark%20filter(),first%20row%20of%20each%20group.\n",
        "  SELECT Name, Department, Salary FROM (\n",
        "     SELECT *, row_number() OVER (\n",
        "       PARTITION BY department \n",
        "       ORDER BY salary\n",
        "     ) as rn\n",
        "     FROM EMP) tmp \n",
        "  WHERE rn = 1\")\n",
        "  \"\"\"\n",
        "\n",
        "  query=\"\"\"\n",
        "  SELECT MONTH(timestamp_local), DAY(timestamp_local), HOUR(timestamp_local), station_id, precip, temp FROM(\n",
        "    SELECT *, row_number() OVER (\n",
        "      PARTITION BY MONTH(timestamp_local), DAY(timestamp_local), HOUR(timestamp_local)\n",
        "      ORDER BY timestamp_local\n",
        "    ) as rn\n",
        "    FROM weather \n",
        "    WHERE station_id='KBNA') tmp\n",
        "  WHERE rn = 1\n",
        "  \"\"\"\n",
        "\n",
        "  counts = spark.sql(query)\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = counts\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"localtest2.out\")\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCeVzIpfHLv3",
        "outputId": "b6bc6e68-156e-42cb-b35e-bb5dc0dd75dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting localtest2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 localtest2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtRwGZqrIyNx",
        "outputId": "7ad327dd-3d3e-4fe1-e478-ab0512b3f0cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f2e60345-cb83-4d2a-a4e3-9e5a232444ae;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 859ms :: artifacts dl 46ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-f2e60345-cb83-4d2a-a4e3-9e5a232444ae\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/25ms)\n",
            "23/05/01 02:46:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/05/01 02:46:58 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/05/01 02:46:58 INFO ResourceUtils: ==============================================================\n",
            "23/05/01 02:46:58 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/05/01 02:46:58 INFO ResourceUtils: ==============================================================\n",
            "23/05/01 02:46:58 INFO SparkContext: Submitted application: localtest2\n",
            "23/05/01 02:46:58 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/05/01 02:46:58 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/05/01 02:46:58 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/05/01 02:46:58 INFO SecurityManager: Changing view acls to: root\n",
            "23/05/01 02:46:58 INFO SecurityManager: Changing modify acls to: root\n",
            "23/05/01 02:46:58 INFO SecurityManager: Changing view acls groups to: \n",
            "23/05/01 02:46:58 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/05/01 02:46:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/05/01 02:46:59 INFO Utils: Successfully started service 'sparkDriver' on port 34905.\n",
            "23/05/01 02:47:00 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/05/01 02:47:00 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/05/01 02:47:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/05/01 02:47:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/05/01 02:47:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/05/01 02:47:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-277c92de-cbc3-4c0d-937d-d97677ae8b64\n",
            "23/05/01 02:47:01 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/05/01 02:47:01 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/05/01 02:47:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/05/01 02:47:02 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://39049882e5a5:4040\n",
            "23/05/01 02:47:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://39049882e5a5:34905/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://39049882e5a5:34905/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://39049882e5a5:34905/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://39049882e5a5:34905/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://39049882e5a5:34905/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://39049882e5a5:34905/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://39049882e5a5:34905/jars/com.101tec_zkclient-0.3.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://39049882e5a5:34905/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://39049882e5a5:34905/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://39049882e5a5:34905/jars/log4j_log4j-1.2.17.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://39049882e5a5:34905/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:02 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://39049882e5a5:34905/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:02 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:02 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 02:47:02 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:02 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 02:47:02 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:02 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 02:47:02 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:02 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 02:47:03 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:03 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 02:47:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:03 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 02:47:03 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:03 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 02:47:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:03 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 02:47:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:03 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 02:47:03 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:03 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/log4j_log4j-1.2.17.jar\n",
            "23/05/01 02:47:03 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:03 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 02:47:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:03 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 02:47:04 INFO Executor: Starting executor ID driver on host 39049882e5a5\n",
            "23/05/01 02:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:04 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/log4j_log4j-1.2.17.jar\n",
            "23/05/01 02:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:04 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 02:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:04 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 02:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:04 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 02:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:04 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 02:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:04 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 02:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:04 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 02:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:04 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 02:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:04 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 02:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:04 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 02:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:04 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 02:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:04 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 02:47:04 INFO Executor: Fetching spark://39049882e5a5:34905/jars/log4j_log4j-1.2.17.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:04 INFO TransportClientFactory: Successfully created connection to 39049882e5a5/172.28.0.12:34905 after 93 ms (0 ms spent in bootstraps)\n",
            "23/05/01 02:47:04 INFO Utils: Fetching spark://39049882e5a5:34905/jars/log4j_log4j-1.2.17.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp904016218416262725.tmp\n",
            "23/05/01 02:47:04 INFO Utils: /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp904016218416262725.tmp has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/log4j_log4j-1.2.17.jar\n",
            "23/05/01 02:47:04 INFO Executor: Adding file:/tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/log4j_log4j-1.2.17.jar to class loader\n",
            "23/05/01 02:47:04 INFO Executor: Fetching spark://39049882e5a5:34905/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:04 INFO Utils: Fetching spark://39049882e5a5:34905/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp6477359115727985670.tmp\n",
            "23/05/01 02:47:04 INFO Utils: /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp6477359115727985670.tmp has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 02:47:04 INFO Executor: Adding file:/tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/05/01 02:47:04 INFO Executor: Fetching spark://39049882e5a5:34905/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:04 INFO Utils: Fetching spark://39049882e5a5:34905/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp1577079931285242600.tmp\n",
            "23/05/01 02:47:04 INFO Utils: /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp1577079931285242600.tmp has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 02:47:04 INFO Executor: Adding file:/tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/05/01 02:47:04 INFO Executor: Fetching spark://39049882e5a5:34905/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:04 INFO Utils: Fetching spark://39049882e5a5:34905/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp1793158148287634124.tmp\n",
            "23/05/01 02:47:04 INFO Utils: /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp1793158148287634124.tmp has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 02:47:04 INFO Executor: Adding file:/tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/05/01 02:47:04 INFO Executor: Fetching spark://39049882e5a5:34905/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:04 INFO Utils: Fetching spark://39049882e5a5:34905/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp1947285145659794563.tmp\n",
            "23/05/01 02:47:04 INFO Utils: /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp1947285145659794563.tmp has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 02:47:04 INFO Executor: Adding file:/tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/05/01 02:47:04 INFO Executor: Fetching spark://39049882e5a5:34905/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:04 INFO Utils: Fetching spark://39049882e5a5:34905/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp9122877955798920366.tmp\n",
            "23/05/01 02:47:04 INFO Utils: /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp9122877955798920366.tmp has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 02:47:04 INFO Executor: Adding file:/tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/05/01 02:47:04 INFO Executor: Fetching spark://39049882e5a5:34905/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:04 INFO Utils: Fetching spark://39049882e5a5:34905/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp2986610080889011768.tmp\n",
            "23/05/01 02:47:04 INFO Utils: /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp2986610080889011768.tmp has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 02:47:04 INFO Executor: Adding file:/tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/05/01 02:47:04 INFO Executor: Fetching spark://39049882e5a5:34905/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:04 INFO Utils: Fetching spark://39049882e5a5:34905/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp3806585814019203959.tmp\n",
            "23/05/01 02:47:04 INFO Utils: /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp3806585814019203959.tmp has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 02:47:04 INFO Executor: Adding file:/tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/05/01 02:47:04 INFO Executor: Fetching spark://39049882e5a5:34905/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:04 INFO Utils: Fetching spark://39049882e5a5:34905/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp5296456007607345166.tmp\n",
            "23/05/01 02:47:04 INFO Utils: /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp5296456007607345166.tmp has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 02:47:05 INFO Executor: Adding file:/tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/05/01 02:47:05 INFO Executor: Fetching spark://39049882e5a5:34905/jars/com.101tec_zkclient-0.3.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:05 INFO Utils: Fetching spark://39049882e5a5:34905/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp7788875748898927563.tmp\n",
            "23/05/01 02:47:05 INFO Utils: /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp7788875748898927563.tmp has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 02:47:05 INFO Executor: Adding file:/tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/05/01 02:47:05 INFO Executor: Fetching spark://39049882e5a5:34905/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:05 INFO Utils: Fetching spark://39049882e5a5:34905/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp284679324382414756.tmp\n",
            "23/05/01 02:47:05 INFO Utils: /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp284679324382414756.tmp has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 02:47:05 INFO Executor: Adding file:/tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/05/01 02:47:05 INFO Executor: Fetching spark://39049882e5a5:34905/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682909218428\n",
            "23/05/01 02:47:05 INFO Utils: Fetching spark://39049882e5a5:34905/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp6417545721756269058.tmp\n",
            "23/05/01 02:47:05 INFO Utils: /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/fetchFileTemp6417545721756269058.tmp has been previously copied to /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 02:47:05 INFO Executor: Adding file:/tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/userFiles-1bda2eba-9168-40de-b56d-fa53d36c0fcb/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/05/01 02:47:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45811.\n",
            "23/05/01 02:47:05 INFO NettyBlockTransferService: Server created on 39049882e5a5:45811\n",
            "23/05/01 02:47:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/05/01 02:47:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 39049882e5a5, 45811, None)\n",
            "23/05/01 02:47:05 INFO BlockManagerMasterEndpoint: Registering block manager 39049882e5a5:45811 with 366.3 MiB RAM, BlockManagerId(driver, 39049882e5a5, 45811, None)\n",
            "23/05/01 02:47:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 39049882e5a5, 45811, None)\n",
            "23/05/01 02:47:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 39049882e5a5, 45811, None)\n",
            "23/05/01 02:47:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/05/01 02:47:06 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/05/01 02:47:07 INFO InMemoryFileIndex: It took 46 ms to list leaf files for 1 paths.\n",
            "23/05/01 02:47:08 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 02:47:08 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 02:47:08 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 02:47:08 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/05/01 02:47:08 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 02:47:08 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 02:47:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/05/01 02:47:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/05/01 02:47:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 39049882e5a5:45811 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/05/01 02:47:09 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 02:47:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 02:47:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/05/01 02:47:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (39049882e5a5, executor driver, partition 0, PROCESS_LOCAL, 4656 bytes) taskResourceAssignments Map()\n",
            "23/05/01 02:47:09 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/05/01 02:47:10 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3161 bytes result sent to driver\n",
            "23/05/01 02:47:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1157 ms on 39049882e5a5 (executor driver) (1/1)\n",
            "23/05/01 02:47:10 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/05/01 02:47:10 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.498 s\n",
            "23/05/01 02:47:10 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/05/01 02:47:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/05/01 02:47:10 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.630804 s\n",
            "23/05/01 02:47:11 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 39049882e5a5:45811 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/05/01 02:47:13 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "23/05/01 02:47:15 INFO DataSourceStrategy: Pruning directories with: \n",
            "23/05/01 02:47:15 INFO FileSourceStrategy: Pushed Filters: IsNotNull(station_id),EqualTo(station_id,KBNA)\n",
            "23/05/01 02:47:15 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(station_id#0),(station_id#0 = KBNA)\n",
            "23/05/01 02:47:15 INFO FileSourceStrategy: Output Data Schema: struct<station_id: string, timestamp_local: timestamp, temp: double, precip: double ... 2 more fields>\n",
            "23/05/01 02:47:16 INFO CodeGenerator: Code generated in 636.662116 ms\n",
            "23/05/01 02:47:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 353.0 KiB, free 366.0 MiB)\n",
            "23/05/01 02:47:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 365.9 MiB)\n",
            "23/05/01 02:47:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 39049882e5a5:45811 (size: 35.7 KiB, free: 366.3 MiB)\n",
            "23/05/01 02:47:16 INFO SparkContext: Created broadcast 1 from csv at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 02:47:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 7431667 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/05/01 02:47:16 INFO DAGScheduler: Registering RDD 5 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/05/01 02:47:16 INFO DAGScheduler: Got map stage job 1 (csv at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
            "23/05/01 02:47:16 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 02:47:17 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/05/01 02:47:17 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 02:47:17 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 02:47:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 20.0 KiB, free 365.9 MiB)\n",
            "23/05/01 02:47:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 365.9 MiB)\n",
            "23/05/01 02:47:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 39049882e5a5:45811 (size: 8.5 KiB, free: 366.3 MiB)\n",
            "23/05/01 02:47:17 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 02:47:17 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/05/01 02:47:17 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "23/05/01 02:47:17 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (39049882e5a5, executor driver, partition 0, PROCESS_LOCAL, 4929 bytes) taskResourceAssignments Map()\n",
            "23/05/01 02:47:17 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (39049882e5a5, executor driver, partition 1, PROCESS_LOCAL, 4934 bytes) taskResourceAssignments Map()\n",
            "23/05/01 02:47:17 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/05/01 02:47:17 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
            "23/05/01 02:47:17 INFO CodeGenerator: Code generated in 70.705052 ms\n",
            "23/05/01 02:47:17 INFO FileScanRDD: Reading File path: file:///content/weather_tn.parquet/month=2/part-00023-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3104775, partition values: [2]\n",
            "23/05/01 02:47:17 INFO FileScanRDD: Reading File path: file:///content/weather_tn.parquet/month=1/part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3369951, partition values: [1]\n",
            "23/05/01 02:47:17 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 02:47:17 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 02:47:18 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/05/01 02:47:18 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/05/01 02:47:19 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2284 bytes result sent to driver\n",
            "23/05/01 02:47:19 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2327 bytes result sent to driver\n",
            "23/05/01 02:47:19 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1909 ms on 39049882e5a5 (executor driver) (1/2)\n",
            "23/05/01 02:47:19 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 1904 ms on 39049882e5a5 (executor driver) (2/2)\n",
            "23/05/01 02:47:19 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/05/01 02:47:19 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 2.077 s\n",
            "23/05/01 02:47:19 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/05/01 02:47:19 INFO DAGScheduler: running: Set()\n",
            "23/05/01 02:47:19 INFO DAGScheduler: waiting: Set()\n",
            "23/05/01 02:47:19 INFO DAGScheduler: failed: Set()\n",
            "23/05/01 02:47:19 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/05/01 02:47:19 INFO CodeGenerator: Code generated in 42.1308 ms\n",
            "23/05/01 02:47:19 INFO CodeGenerator: Code generated in 26.666745 ms\n",
            "23/05/01 02:47:19 INFO DAGScheduler: Registering RDD 10 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/05/01 02:47:19 INFO DAGScheduler: Got map stage job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 02:47:19 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 02:47:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/05/01 02:47:19 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 02:47:19 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 02:47:19 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 38.8 KiB, free 365.9 MiB)\n",
            "23/05/01 02:47:19 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 18.0 KiB, free 365.8 MiB)\n",
            "23/05/01 02:47:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 39049882e5a5:45811 (size: 18.0 KiB, free: 366.2 MiB)\n",
            "23/05/01 02:47:19 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 02:47:19 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 02:47:19 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/05/01 02:47:19 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (39049882e5a5, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/05/01 02:47:19 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "23/05/01 02:47:19 INFO ShuffleBlockFetcherIterator: Getting 2 (74.9 KiB) non-empty blocks including 2 (74.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/05/01 02:47:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms\n",
            "23/05/01 02:47:19 INFO CodeGenerator: Code generated in 26.711134 ms\n",
            "23/05/01 02:47:19 INFO CodeGenerator: Code generated in 11.860606 ms\n",
            "23/05/01 02:47:19 INFO CodeGenerator: Code generated in 45.324295 ms\n",
            "23/05/01 02:47:19 INFO CodeGenerator: Code generated in 23.124093 ms\n",
            "23/05/01 02:47:20 INFO CodeGenerator: Code generated in 17.102868 ms\n",
            "23/05/01 02:47:20 INFO CodeGenerator: Code generated in 23.028524 ms\n",
            "23/05/01 02:47:20 INFO CodeGenerator: Code generated in 19.463329 ms\n",
            "23/05/01 02:47:20 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3925 bytes result sent to driver\n",
            "23/05/01 02:47:20 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 574 ms on 39049882e5a5 (executor driver) (1/1)\n",
            "23/05/01 02:47:20 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 0.596 s\n",
            "23/05/01 02:47:20 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/05/01 02:47:20 INFO DAGScheduler: running: Set()\n",
            "23/05/01 02:47:20 INFO DAGScheduler: waiting: Set()\n",
            "23/05/01 02:47:20 INFO DAGScheduler: failed: Set()\n",
            "23/05/01 02:47:20 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/05/01 02:47:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/05/01 02:47:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/05/01 02:47:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/05/01 02:47:20 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 02:47:20 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 02:47:20 INFO DAGScheduler: Final stage: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 02:47:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
            "23/05/01 02:47:20 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 02:47:20 INFO DAGScheduler: Submitting ResultStage 6 (ShuffledRowRDD[11] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 02:47:20 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 212.0 KiB, free 365.6 MiB)\n",
            "23/05/01 02:47:20 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 75.8 KiB, free 365.6 MiB)\n",
            "23/05/01 02:47:20 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 39049882e5a5:45811 (size: 75.8 KiB, free: 366.2 MiB)\n",
            "23/05/01 02:47:20 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 02:47:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (ShuffledRowRDD[11] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 02:47:20 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "23/05/01 02:47:20 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (39049882e5a5, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/05/01 02:47:20 INFO Executor: Running task 0.0 in stage 6.0 (TID 4)\n",
            "23/05/01 02:47:20 INFO ShuffleBlockFetcherIterator: Getting 1 (17.9 KiB) non-empty blocks including 1 (17.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/05/01 02:47:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/05/01 02:47:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/05/01 02:47:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/05/01 02:47:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/05/01 02:47:20 INFO FileOutputCommitter: Saved output of task 'attempt_20230501024720518667092358238166_0006_m_000000_4' to file:/content/localtest2.out/_temporary/0/task_20230501024720518667092358238166_0006_m_000000\n",
            "23/05/01 02:47:20 INFO SparkHadoopMapRedUtil: attempt_20230501024720518667092358238166_0006_m_000000_4: Committed\n",
            "23/05/01 02:47:20 INFO Executor: Finished task 0.0 in stage 6.0 (TID 4). 3483 bytes result sent to driver\n",
            "23/05/01 02:47:20 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 237 ms on 39049882e5a5 (executor driver) (1/1)\n",
            "23/05/01 02:47:20 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "23/05/01 02:47:20 INFO DAGScheduler: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0) finished in 0.288 s\n",
            "23/05/01 02:47:20 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/05/01 02:47:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "23/05/01 02:47:20 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 0.305886 s\n",
            "23/05/01 02:47:20 INFO FileFormatWriter: Start to commit write Job 84edfcfe-28e4-41b4-a50b-466361316f97.\n",
            "23/05/01 02:47:20 INFO FileFormatWriter: Write Job 84edfcfe-28e4-41b4-a50b-466361316f97 committed. Elapsed time: 20 ms.\n",
            "23/05/01 02:47:20 INFO FileFormatWriter: Finished processing stats for write job 84edfcfe-28e4-41b4-a50b-466361316f97.\n",
            "23/05/01 02:47:20 INFO SparkUI: Stopped Spark web UI at http://39049882e5a5:4040\n",
            "23/05/01 02:47:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/05/01 02:47:20 INFO MemoryStore: MemoryStore cleared\n",
            "23/05/01 02:47:20 INFO BlockManager: BlockManager stopped\n",
            "23/05/01 02:47:20 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/05/01 02:47:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/05/01 02:47:20 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/05/01 02:47:21 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/05/01 02:47:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065\n",
            "23/05/01 02:47:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-dc7f37ac-0921-40e8-8f16-2953a76bdea5\n",
            "23/05/01 02:47:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-c1741b34-ed72-4d47-a54f-945baffca065/pyspark-37bec500-194b-4753-84fd-2a1418183680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rename_file()\n",
        "with open('localtest2.out/results.csv') as f:\n",
        "  lines = f.readlines()\n",
        "  time_hist = pd.read_csv('localtest2.out/results.csv', header=None, names=['month', 'day', 'hour', 'station', 'precip', 'temp'])\n",
        "time_hist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "pjzQQ_TPI1aM",
        "outputId": "722a06d8-75a7-4f26-aa37-8a05ca817420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renamed part-00000-8bc096a6-a21e-41df-bcf7-7f3c7a98edcd-c000.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      month  day  hour station  precip  temp\n",
              "0         1    1     0    KBNA     1.5   7.2\n",
              "1         1    1     1    KBNA     0.8   7.2\n",
              "2         1    1     2    KBNA     0.8   7.8\n",
              "3         1    1     3    KBNA     4.6   8.3\n",
              "4         1    1     4    KBNA     1.3   8.9\n",
              "...     ...  ...   ...     ...     ...   ...\n",
              "1411      2   28    19    KBNA     1.0  21.7\n",
              "1412      2   28    20    KBNA     1.0  16.1\n",
              "1413      2   28    21    KBNA    12.2  15.6\n",
              "1414      2   28    22    KBNA     4.3  15.0\n",
              "1415      2   28    23    KBNA     3.8  15.0\n",
              "\n",
              "[1416 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6fdea1bb-4638-4b83-9162-b0e0a42397ba\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "      <th>station</th>\n",
              "      <th>precip</th>\n",
              "      <th>temp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>1.5</td>\n",
              "      <td>7.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.8</td>\n",
              "      <td>7.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.8</td>\n",
              "      <td>7.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>4.6</td>\n",
              "      <td>8.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>1.3</td>\n",
              "      <td>8.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1411</th>\n",
              "      <td>2</td>\n",
              "      <td>28</td>\n",
              "      <td>19</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1412</th>\n",
              "      <td>2</td>\n",
              "      <td>28</td>\n",
              "      <td>20</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1413</th>\n",
              "      <td>2</td>\n",
              "      <td>28</td>\n",
              "      <td>21</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>12.2</td>\n",
              "      <td>15.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1414</th>\n",
              "      <td>2</td>\n",
              "      <td>28</td>\n",
              "      <td>22</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>4.3</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1415</th>\n",
              "      <td>2</td>\n",
              "      <td>28</td>\n",
              "      <td>23</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>3.8</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1416 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6fdea1bb-4638-4b83-9162-b0e0a42397ba')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6fdea1bb-4638-4b83-9162-b0e0a42397ba button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6fdea1bb-4638-4b83-9162-b0e0a42397ba');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now that we know that we can process all of the data... let's find the weather for ALL events"
      ],
      "metadata": {
        "id": "V49M_Rp0QRet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To start, we need to get all of the files from S3 organized into their respective folders\n",
        "\n",
        "We only need weather data for 2017, 2018, 2019, 2020, and 2021 since those are the only years that we have data for events\n",
        "\n",
        "For now, just downloading manually, will eventually automate pull from S3"
      ],
      "metadata": {
        "id": "5LBuL9obQZu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip weather_tn.parquet.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcbckvSqQYmG",
        "outputId": "bcf199f4-798e-4d8f-cc38-b44dac76e2ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  weather_tn.parquet.zip\n",
            "   creating: weather_tn.parquet/weather_tn.parquet/\n",
            " extracting: weather_tn.parquet/weather_tn.parquet/._SUCCESS.crc  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2010/\n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2010/month=1/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=1/.part-00038-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=1/part-00038-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2010/month=10/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=10/.part-00027-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=10/part-00027-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2010/month=11/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=11/.part-00013-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=11/part-00013-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2010/month=12/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=12/.part-00000-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=12/part-00000-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2010/month=2/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=2/.part-00028-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=2/part-00028-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2010/month=3/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=3/.part-00028-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=3/part-00028-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2010/month=4/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=4/.part-00033-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=4/part-00033-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2010/month=5/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=5/.part-00016-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=5/part-00016-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2010/month=6/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=6/.part-00015-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=6/part-00015-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2010/month=7/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=7/.part-00000-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=7/part-00000-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2010/month=8/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=8/.part-00024-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=8/part-00024-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2010/month=9/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=9/.part-00017-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2010/month=9/part-00017-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2011/\n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2011/month=1/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=1/.part-00036-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=1/part-00036-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2011/month=10/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=10/.part-00034-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=10/part-00034-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2011/month=11/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=11/.part-00028-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=11/part-00028-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2011/month=12/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=12/.part-00018-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=12/part-00018-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2011/month=2/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=2/.part-00017-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=2/part-00017-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2011/month=3/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=3/.part-00013-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=3/part-00013-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2011/month=4/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=4/.part-00017-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=4/part-00017-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2011/month=5/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=5/.part-00023-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=5/part-00023-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2011/month=6/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=6/.part-00026-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=6/part-00026-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2011/month=7/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=7/.part-00023-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=7/part-00023-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2011/month=8/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=8/.part-00019-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=8/part-00019-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2011/month=9/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=9/.part-00039-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2011/month=9/part-00039-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2012/\n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2012/month=1/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=1/.part-00029-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=1/part-00029-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2012/month=10/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=10/.part-00000-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=10/part-00000-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2012/month=11/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=11/.part-00025-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=11/part-00025-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2012/month=12/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=12/.part-00006-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=12/part-00006-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2012/month=2/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=2/.part-00031-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=2/part-00031-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2012/month=3/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=3/.part-00033-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=3/part-00033-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2012/month=4/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=4/.part-00006-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=4/part-00006-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2012/month=5/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=5/.part-00034-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=5/part-00034-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2012/month=6/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=6/.part-00038-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=6/part-00038-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2012/month=7/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=7/.part-00015-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=7/part-00015-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2012/month=8/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=8/.part-00005-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=8/part-00005-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2012/month=9/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=9/.part-00015-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2012/month=9/part-00015-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2013/\n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2013/month=1/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=1/.part-00034-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=1/part-00034-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2013/month=10/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=10/.part-00009-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=10/part-00009-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2013/month=11/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=11/.part-00022-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=11/part-00022-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2013/month=12/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=12/.part-00012-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=12/part-00012-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2013/month=2/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=2/.part-00005-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=2/part-00005-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2013/month=3/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=3/.part-00014-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=3/part-00014-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2013/month=4/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=4/.part-00021-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=4/part-00021-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2013/month=5/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=5/.part-00023-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=5/part-00023-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2013/month=6/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=6/.part-00014-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=6/part-00014-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2013/month=7/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=7/.part-00020-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=7/part-00020-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2013/month=8/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=8/.part-00035-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=8/part-00035-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2013/month=9/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=9/.part-00009-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2013/month=9/part-00009-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2014/\n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2014/month=1/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=1/.part-00012-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=1/part-00012-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2014/month=10/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=10/.part-00008-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=10/part-00008-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2014/month=11/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=11/.part-00032-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=11/part-00032-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2014/month=12/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=12/.part-00008-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=12/part-00008-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2014/month=2/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=2/.part-00024-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=2/part-00024-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2014/month=3/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=3/.part-00021-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=3/part-00021-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2014/month=4/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=4/.part-00001-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=4/part-00001-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2014/month=5/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=5/.part-00010-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=5/part-00010-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2014/month=6/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=6/.part-00026-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=6/part-00026-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2014/month=7/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=7/.part-00039-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=7/part-00039-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2014/month=8/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=8/.part-00014-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=8/part-00014-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2014/month=9/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=9/.part-00020-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2014/month=9/part-00020-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2015/\n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2015/month=1/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=1/.part-00036-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=1/part-00036-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2015/month=10/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=10/.part-00021-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=10/part-00021-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2015/month=11/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=11/.part-00018-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=11/part-00018-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2015/month=12/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=12/.part-00003-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=12/part-00003-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2015/month=2/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=2/.part-00000-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=2/part-00000-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2015/month=3/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=3/.part-00027-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=3/part-00027-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2015/month=4/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=4/.part-00015-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=4/part-00015-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2015/month=5/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=5/.part-00037-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=5/part-00037-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2015/month=6/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=6/.part-00032-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=6/part-00032-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2015/month=7/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=7/.part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=7/part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2015/month=8/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=8/.part-00016-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=8/part-00016-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2015/month=9/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=9/.part-00019-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2015/month=9/part-00019-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2016/\n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2016/month=1/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=1/.part-00025-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=1/part-00025-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2016/month=10/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=10/.part-00019-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=10/part-00019-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2016/month=11/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=11/.part-00004-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=11/part-00004-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2016/month=12/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=12/.part-00024-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=12/part-00024-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2016/month=2/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=2/.part-00010-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=2/part-00010-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2016/month=3/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=3/.part-00040-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=3/part-00040-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2016/month=4/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=4/.part-00025-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=4/part-00025-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2016/month=5/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=5/.part-00008-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=5/part-00008-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2016/month=6/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=6/.part-00022-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=6/part-00022-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2016/month=7/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=7/.part-00003-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=7/part-00003-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2016/month=8/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=8/.part-00035-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=8/part-00035-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2016/month=9/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=9/.part-00016-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2016/month=9/part-00016-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2017/\n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2017/month=1/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=1/.part-00039-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=1/part-00039-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2017/month=10/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=10/.part-00002-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=10/part-00002-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2017/month=11/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=11/.part-00035-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=11/part-00035-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2017/month=12/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=12/.part-00020-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=12/part-00020-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2017/month=2/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=2/.part-00035-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=2/part-00035-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2017/month=3/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=3/.part-00001-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=3/part-00001-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2017/month=4/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=4/.part-00026-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=4/part-00026-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2017/month=5/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=5/.part-00036-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=5/part-00036-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2017/month=6/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=6/.part-00038-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=6/part-00038-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2017/month=7/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=7/.part-00011-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=7/part-00011-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2017/month=8/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=8/.part-00001-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=8/part-00001-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2017/month=9/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=9/.part-00021-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2017/month=9/part-00021-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2018/\n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2018/month=1/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=1/.part-00007-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=1/part-00007-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2018/month=10/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=10/.part-00002-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=10/part-00002-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2018/month=11/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=11/.part-00033-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=11/part-00033-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2018/month=12/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=12/.part-00033-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=12/part-00033-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2018/month=2/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=2/.part-00037-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=2/part-00037-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2018/month=3/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=3/.part-00007-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=3/part-00007-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2018/month=4/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=4/.part-00040-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=4/part-00040-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2018/month=5/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=5/.part-00018-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=5/part-00018-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2018/month=6/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=6/.part-00034-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=6/part-00034-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2018/month=7/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=7/.part-00031-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=7/part-00031-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2018/month=8/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=8/.part-00009-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=8/part-00009-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2018/month=9/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=9/.part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2018/month=9/part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2019/\n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2019/month=1/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=1/.part-00014-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=1/part-00014-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2019/month=10/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=10/.part-00001-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=10/part-00001-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2019/month=11/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=11/.part-00037-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=11/part-00037-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2019/month=12/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=12/.part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=12/part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2019/month=2/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=2/.part-00016-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=2/part-00016-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2019/month=3/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=3/.part-00005-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=3/part-00005-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2019/month=4/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=4/.part-00026-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=4/part-00026-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2019/month=5/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=5/.part-00002-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=5/part-00002-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2019/month=6/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=6/.part-00011-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=6/part-00011-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2019/month=7/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=7/.part-00037-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=7/part-00037-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2019/month=8/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=8/.part-00010-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=8/part-00010-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2019/month=9/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=9/.part-00029-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2019/month=9/part-00029-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2020/\n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2020/month=1/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=1/.part-00025-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=1/part-00025-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2020/month=10/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=10/.part-00032-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=10/part-00032-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2020/month=11/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=11/.part-00006-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=11/part-00006-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2020/month=12/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=12/.part-00017-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=12/part-00017-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2020/month=2/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=2/.part-00032-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=2/part-00032-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2020/month=3/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=3/.part-00007-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=3/part-00007-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2020/month=4/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=4/.part-00020-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=4/part-00020-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2020/month=5/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=5/.part-00038-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=5/part-00038-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2020/month=6/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=6/.part-00002-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=6/part-00002-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2020/month=7/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=7/.part-00031-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=7/part-00031-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2020/month=8/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=8/.part-00029-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=8/part-00029-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2020/month=9/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=9/.part-00008-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2020/month=9/part-00008-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2021/\n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2021/month=1/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=1/.part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=1/part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2021/month=10/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=10/.part-00006-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=10/part-00006-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2021/month=11/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=11/.part-00010-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=11/part-00010-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2021/month=12/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=12/.part-00018-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=12/part-00018-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2021/month=2/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=2/.part-00023-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=2/part-00023-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2021/month=3/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=3/.part-00022-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=3/part-00022-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2021/month=4/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=4/.part-00039-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=4/part-00039-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2021/month=5/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=5/.part-00005-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=5/part-00005-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2021/month=6/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=6/.part-00004-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=6/part-00004-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2021/month=7/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=7/.part-00019-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=7/part-00019-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2021/month=8/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=8/.part-00003-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=8/part-00003-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2021/month=9/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=9/.part-00012-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2021/month=9/part-00012-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2022/\n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2022/month=1/\n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2022/month=1/.part-00029-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2022/month=1/part-00029-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n",
            "   creating: weather_tn.parquet/weather_tn.parquet/year=2022/month=2/\n",
            " extracting: weather_tn.parquet/weather_tn.parquet/year=2022/month=2/.part-00009-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet.crc  \n",
            "  inflating: weather_tn.parquet/weather_tn.parquet/year=2022/month=2/part-00009-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file localtest2.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('localtest2').getOrCreate()\n",
        "\n",
        "try:\n",
        "  weather_df = spark.read.parquet('weather_tn/weather_tn.parquet')\n",
        "  weather_df.createOrReplaceTempView(\"weather\")\n",
        "\n",
        "  incidents_df = spark.read.parquet('incidents.parquet')\n",
        "  incidents_df.createOrReplaceTempView(\"incidents\")\n",
        "\n",
        "  incidents_query=\"\"\"\n",
        "  (SELECT time_local, Incident_ID, response_time_sec, latitude, longitude FROM(\n",
        "    SELECT *, row_number() OVER (\n",
        "      PARTITION BY YEAR(time_local), MONTH(time_local), DAY(time_local), HOUR(time_local)\n",
        "      ORDER BY time_local\n",
        "    ) as rn\n",
        "    FROM incidents\n",
        "    WHERE YEAR(time_local) >= 2017 AND YEAR(time_local) <= 2021) tmp\n",
        "  WHERE rn = 1) as i\n",
        "  \"\"\"\n",
        "\n",
        "  weather_query=\"\"\"\n",
        "  (SELECT tmp.timestamp_local, tmp.station_id, tmp.precip, tmp.temp FROM(\n",
        "    SELECT *, row_number() OVER (\n",
        "      PARTITION BY YEAR(timestamp_local), MONTH(timestamp_local), DAY(timestamp_local), HOUR(timestamp_local)\n",
        "      ORDER BY timestamp_local\n",
        "    ) as rn\n",
        "    FROM weather \n",
        "    WHERE station_id='KBNA') tmp\n",
        "  WHERE rn = 1) as w\n",
        "  \"\"\"\n",
        "\n",
        "  full_query = \"SELECT * FROM (\" + incidents_query + \") LEFT JOIN (\" + weather_query + \") ON YEAR(i.time_local) = YEAR(w.timestamp_local) AND MONTH(i.time_local) = MONTH(w.timestamp_local) AND DAY(i.time_local) = DAY(w.timestamp_local) AND HOUR(i.time_local) = HOUR(w.timestamp_local)\"\n",
        "\n",
        "  counts = spark.sql(full_query)\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = counts\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"localtest2.out\")\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vNT656LYzpj",
        "outputId": "d5ced9b5-4154-46e9-f97d-2123326048c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting localtest2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 localtest2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXiU-1juaHOx",
        "outputId": "ea104e49-b90c-4dcc-dbd9-8cc10ec8f900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-88953f69-f618-4b8c-9d2b-189dbf3ae01f;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 1553ms :: artifacts dl 60ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-88953f69-f618-4b8c-9d2b-189dbf3ae01f\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/39ms)\n",
            "23/05/01 04:27:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/05/01 04:27:20 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/05/01 04:27:21 INFO ResourceUtils: ==============================================================\n",
            "23/05/01 04:27:21 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/05/01 04:27:21 INFO ResourceUtils: ==============================================================\n",
            "23/05/01 04:27:21 INFO SparkContext: Submitted application: localtest2\n",
            "23/05/01 04:27:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/05/01 04:27:21 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/05/01 04:27:21 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/05/01 04:27:21 INFO SecurityManager: Changing view acls to: root\n",
            "23/05/01 04:27:21 INFO SecurityManager: Changing modify acls to: root\n",
            "23/05/01 04:27:21 INFO SecurityManager: Changing view acls groups to: \n",
            "23/05/01 04:27:21 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/05/01 04:27:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/05/01 04:27:22 INFO Utils: Successfully started service 'sparkDriver' on port 44049.\n",
            "23/05/01 04:27:22 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/05/01 04:27:22 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/05/01 04:27:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/05/01 04:27:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/05/01 04:27:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/05/01 04:27:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8e3e6603-e115-497c-babb-d2ed289d080c\n",
            "23/05/01 04:27:22 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/05/01 04:27:22 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/05/01 04:27:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/05/01 04:27:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://39049882e5a5:4040\n",
            "23/05/01 04:27:23 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://39049882e5a5:44049/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://39049882e5a5:44049/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://39049882e5a5:44049/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://39049882e5a5:44049/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://39049882e5a5:44049/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://39049882e5a5:44049/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://39049882e5a5:44049/jars/com.101tec_zkclient-0.3.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://39049882e5a5:44049/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://39049882e5a5:44049/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://39049882e5a5:44049/jars/log4j_log4j-1.2.17.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://39049882e5a5:44049/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://39049882e5a5:44049/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 04:27:23 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 04:27:23 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 04:27:23 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 04:27:23 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 04:27:23 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 04:27:23 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 04:27:23 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 04:27:23 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 04:27:23 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/log4j_log4j-1.2.17.jar\n",
            "23/05/01 04:27:23 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 04:27:23 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 04:27:23 INFO Executor: Starting executor ID driver on host 39049882e5a5\n",
            "23/05/01 04:27:23 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:23 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/log4j_log4j-1.2.17.jar\n",
            "23/05/01 04:27:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 04:27:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 04:27:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 04:27:24 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 04:27:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 04:27:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 04:27:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 04:27:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 04:27:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 04:27:24 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 04:27:24 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 04:27:24 INFO Executor: Fetching spark://39049882e5a5:44049/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO TransportClientFactory: Successfully created connection to 39049882e5a5/172.28.0.12:44049 after 45 ms (0 ms spent in bootstraps)\n",
            "23/05/01 04:27:24 INFO Utils: Fetching spark://39049882e5a5:44049/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp4911340716084302095.tmp\n",
            "23/05/01 04:27:24 INFO Utils: /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp4911340716084302095.tmp has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 04:27:24 INFO Executor: Adding file:/tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/05/01 04:27:24 INFO Executor: Fetching spark://39049882e5a5:44049/jars/log4j_log4j-1.2.17.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: Fetching spark://39049882e5a5:44049/jars/log4j_log4j-1.2.17.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp2872978723139028158.tmp\n",
            "23/05/01 04:27:24 INFO Utils: /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp2872978723139028158.tmp has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/log4j_log4j-1.2.17.jar\n",
            "23/05/01 04:27:24 INFO Executor: Adding file:/tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/log4j_log4j-1.2.17.jar to class loader\n",
            "23/05/01 04:27:24 INFO Executor: Fetching spark://39049882e5a5:44049/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: Fetching spark://39049882e5a5:44049/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp4045593071371057489.tmp\n",
            "23/05/01 04:27:24 INFO Utils: /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp4045593071371057489.tmp has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 04:27:24 INFO Executor: Adding file:/tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/05/01 04:27:24 INFO Executor: Fetching spark://39049882e5a5:44049/jars/com.101tec_zkclient-0.3.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: Fetching spark://39049882e5a5:44049/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp2300376471493505567.tmp\n",
            "23/05/01 04:27:24 INFO Utils: /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp2300376471493505567.tmp has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 04:27:24 INFO Executor: Adding file:/tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/05/01 04:27:24 INFO Executor: Fetching spark://39049882e5a5:44049/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: Fetching spark://39049882e5a5:44049/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp3151228552202850052.tmp\n",
            "23/05/01 04:27:24 INFO Utils: /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp3151228552202850052.tmp has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 04:27:24 INFO Executor: Adding file:/tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/05/01 04:27:24 INFO Executor: Fetching spark://39049882e5a5:44049/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: Fetching spark://39049882e5a5:44049/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp3366634640594882209.tmp\n",
            "23/05/01 04:27:24 INFO Utils: /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp3366634640594882209.tmp has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 04:27:24 INFO Executor: Adding file:/tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/05/01 04:27:24 INFO Executor: Fetching spark://39049882e5a5:44049/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: Fetching spark://39049882e5a5:44049/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp2395496049607091203.tmp\n",
            "23/05/01 04:27:24 INFO Utils: /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp2395496049607091203.tmp has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 04:27:24 INFO Executor: Adding file:/tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/05/01 04:27:24 INFO Executor: Fetching spark://39049882e5a5:44049/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: Fetching spark://39049882e5a5:44049/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp170020250539268891.tmp\n",
            "23/05/01 04:27:24 INFO Utils: /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp170020250539268891.tmp has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 04:27:24 INFO Executor: Adding file:/tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/05/01 04:27:24 INFO Executor: Fetching spark://39049882e5a5:44049/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: Fetching spark://39049882e5a5:44049/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp2291653305724650100.tmp\n",
            "23/05/01 04:27:24 INFO Utils: /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp2291653305724650100.tmp has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 04:27:24 INFO Executor: Adding file:/tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/05/01 04:27:24 INFO Executor: Fetching spark://39049882e5a5:44049/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: Fetching spark://39049882e5a5:44049/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp1912476113934826520.tmp\n",
            "23/05/01 04:27:24 INFO Utils: /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp1912476113934826520.tmp has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 04:27:24 INFO Executor: Adding file:/tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/05/01 04:27:24 INFO Executor: Fetching spark://39049882e5a5:44049/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: Fetching spark://39049882e5a5:44049/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp974255656298085264.tmp\n",
            "23/05/01 04:27:24 INFO Utils: /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp974255656298085264.tmp has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 04:27:24 INFO Executor: Adding file:/tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/05/01 04:27:24 INFO Executor: Fetching spark://39049882e5a5:44049/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682915240945\n",
            "23/05/01 04:27:24 INFO Utils: Fetching spark://39049882e5a5:44049/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp5065071604717743434.tmp\n",
            "23/05/01 04:27:24 INFO Utils: /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/fetchFileTemp5065071604717743434.tmp has been previously copied to /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 04:27:24 INFO Executor: Adding file:/tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/userFiles-468fd72f-4f3e-41a1-ae41-95a3969c5f1d/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/05/01 04:27:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39965.\n",
            "23/05/01 04:27:24 INFO NettyBlockTransferService: Server created on 39049882e5a5:39965\n",
            "23/05/01 04:27:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/05/01 04:27:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 39049882e5a5, 39965, None)\n",
            "23/05/01 04:27:24 INFO BlockManagerMasterEndpoint: Registering block manager 39049882e5a5:39965 with 366.3 MiB RAM, BlockManagerId(driver, 39049882e5a5, 39965, None)\n",
            "23/05/01 04:27:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 39049882e5a5, 39965, None)\n",
            "23/05/01 04:27:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 39049882e5a5, 39965, None)\n",
            "23/05/01 04:27:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/05/01 04:27:25 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/05/01 04:27:27 INFO InMemoryFileIndex: It took 264 ms to list leaf files for 1 paths.\n",
            "23/05/01 04:27:30 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 04:27:30 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 04:27:30 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 04:27:30 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/05/01 04:27:30 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 04:27:30 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 04:27:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/05/01 04:27:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/05/01 04:27:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 39049882e5a5:39965 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/05/01 04:27:31 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 04:27:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 04:27:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/05/01 04:27:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (39049882e5a5, executor driver, partition 0, PROCESS_LOCAL, 4677 bytes) taskResourceAssignments Map()\n",
            "23/05/01 04:27:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/05/01 04:27:34 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3118 bytes result sent to driver\n",
            "23/05/01 04:27:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3166 ms on 39049882e5a5 (executor driver) (1/1)\n",
            "23/05/01 04:27:34 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/05/01 04:27:34 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 4.098 s\n",
            "23/05/01 04:27:34 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/05/01 04:27:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/05/01 04:27:34 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 4.319176 s\n",
            "23/05/01 04:27:35 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 39049882e5a5:39965 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/05/01 04:27:39 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "23/05/01 04:27:39 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "23/05/01 04:27:39 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 04:27:39 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 04:27:39 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 04:27:39 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/05/01 04:27:39 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 04:27:39 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 04:27:39 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/05/01 04:27:39 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/05/01 04:27:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 39049882e5a5:39965 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/05/01 04:27:39 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 04:27:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 04:27:39 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/05/01 04:27:39 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (39049882e5a5, executor driver, partition 0, PROCESS_LOCAL, 4579 bytes) taskResourceAssignments Map()\n",
            "23/05/01 04:27:39 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/05/01 04:27:39 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2231 bytes result sent to driver\n",
            "23/05/01 04:27:39 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 78 ms on 39049882e5a5 (executor driver) (1/1)\n",
            "23/05/01 04:27:39 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.115 s\n",
            "23/05/01 04:27:39 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/05/01 04:27:39 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/05/01 04:27:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "23/05/01 04:27:39 INFO DAGScheduler: Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.123052 s\n",
            "23/05/01 04:27:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(time_local)\n",
            "23/05/01 04:27:40 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(time_local#79),(year(cast(time_local#79 as date)) >= 2017),(year(cast(time_local#79 as date)) <= 2021)\n",
            "23/05/01 04:27:40 INFO FileSourceStrategy: Output Data Schema: struct<latitude: double, longitude: double, time_local: timestamp, response_time_sec: double, Incident_ID: int ... 3 more fields>\n",
            "23/05/01 04:27:40 INFO DataSourceStrategy: Pruning directories with: \n",
            "23/05/01 04:27:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(station_id),EqualTo(station_id,KBNA)\n",
            "23/05/01 04:27:40 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(station_id#0),(station_id#0 = KBNA)\n",
            "23/05/01 04:27:40 INFO FileSourceStrategy: Output Data Schema: struct<station_id: string, timestamp_local: timestamp, temp: double, precip: double ... 2 more fields>\n",
            "23/05/01 04:27:41 INFO CodeGenerator: Code generated in 300.884592 ms\n",
            "23/05/01 04:27:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 353.3 KiB, free 365.8 MiB)\n",
            "23/05/01 04:27:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 35.8 KiB, free 365.8 MiB)\n",
            "23/05/01 04:27:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 39049882e5a5:39965 (size: 35.8 KiB, free: 366.2 MiB)\n",
            "23/05/01 04:27:41 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 04:27:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/05/01 04:27:41 INFO DAGScheduler: Registering RDD 7 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/05/01 04:27:42 INFO DAGScheduler: Got map stage job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 04:27:42 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 04:27:42 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/05/01 04:27:42 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 04:27:42 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[7] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 04:27:42 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.2 KiB, free 365.8 MiB)\n",
            "23/05/01 04:27:42 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 365.8 MiB)\n",
            "23/05/01 04:27:42 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 39049882e5a5:39965 (size: 8.6 KiB, free: 366.2 MiB)\n",
            "23/05/01 04:27:42 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 04:27:42 INFO CodeGenerator: Code generated in 105.255638 ms\n",
            "23/05/01 04:27:42 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[7] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 04:27:42 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "23/05/01 04:27:42 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (39049882e5a5, executor driver, partition 0, PROCESS_LOCAL, 4847 bytes) taskResourceAssignments Map()\n",
            "23/05/01 04:27:42 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "23/05/01 04:27:42 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 353.0 KiB, free 365.4 MiB)\n",
            "23/05/01 04:27:42 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 39049882e5a5:39965 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/05/01 04:27:42 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 365.5 MiB)\n",
            "23/05/01 04:27:42 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 39049882e5a5:39965 (size: 35.7 KiB, free: 366.2 MiB)\n",
            "23/05/01 04:27:42 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 04:27:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/05/01 04:27:42 INFO DAGScheduler: Registering RDD 11 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/05/01 04:27:42 INFO DAGScheduler: Got map stage job 3 (csv at NativeMethodAccessorImpl.java:0) with 9 output partitions\n",
            "23/05/01 04:27:42 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 04:27:42 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/05/01 04:27:42 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 04:27:42 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[11] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 04:27:42 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 20.6 KiB, free 365.5 MiB)\n",
            "23/05/01 04:27:42 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 365.5 MiB)\n",
            "23/05/01 04:27:42 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 39049882e5a5:39965 (size: 8.6 KiB, free: 366.2 MiB)\n",
            "23/05/01 04:27:42 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 04:27:42 INFO DAGScheduler: Submitting 9 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[11] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8))\n",
            "23/05/01 04:27:42 INFO TaskSchedulerImpl: Adding task set 3.0 with 9 tasks resource profile 0\n",
            "23/05/01 04:27:42 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (39049882e5a5, executor driver, partition 0, PROCESS_LOCAL, 7785 bytes) taskResourceAssignments Map()\n",
            "23/05/01 04:27:42 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "23/05/01 04:27:42 INFO CodeGenerator: Code generated in 102.008879 ms\n",
            "23/05/01 04:27:42 INFO CodeGenerator: Code generated in 101.988954 ms\n",
            "23/05/01 04:27:42 INFO FileScanRDD: Reading File path: file:///content/incidents.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/05/01 04:27:42 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=1/part-00025-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-5557546, partition values: [2016,1]\n",
            "23/05/01 04:27:42 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:43 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/05/01 04:27:43 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/05/01 04:27:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=5/part-00010-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-4205877, partition values: [2014,5]\n",
            "23/05/01 04:27:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:45 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=2/part-00010-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-4128191, partition values: [2016,2]\n",
            "23/05/01 04:27:45 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:45 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2327 bytes result sent to driver\n",
            "23/05/01 04:27:45 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4) (39049882e5a5, executor driver, partition 1, PROCESS_LOCAL, 7972 bytes) taskResourceAssignments Map()\n",
            "23/05/01 04:27:45 INFO Executor: Running task 1.0 in stage 3.0 (TID 4)\n",
            "23/05/01 04:27:45 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 3277 ms on 39049882e5a5 (executor driver) (1/1)\n",
            "23/05/01 04:27:45 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "23/05/01 04:27:45 INFO DAGScheduler: ShuffleMapStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 3.406 s\n",
            "23/05/01 04:27:45 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/05/01 04:27:45 INFO DAGScheduler: running: Set(ShuffleMapStage 3)\n",
            "23/05/01 04:27:45 INFO DAGScheduler: waiting: Set()\n",
            "23/05/01 04:27:45 INFO DAGScheduler: failed: Set()\n",
            "23/05/01 04:27:45 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=7/part-00039-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3951624, partition values: [2014,7]\n",
            "23/05/01 04:27:45 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:45 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=7/part-00023-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-4058620, partition values: [2011,7]\n",
            "23/05/01 04:27:45 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:45 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/05/01 04:27:45 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=7/part-00003-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3951093, partition values: [2016,7]\n",
            "23/05/01 04:27:46 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:46 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 39049882e5a5:39965 in memory (size: 8.6 KiB, free: 366.2 MiB)\n",
            "23/05/01 04:27:46 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=7/part-00031-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-4028984, partition values: [2018,7]\n",
            "23/05/01 04:27:46 INFO CodeGenerator: Code generated in 79.773038 ms\n",
            "23/05/01 04:27:46 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:46 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=8/part-00009-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3945089, partition values: [2018,8]\n",
            "23/05/01 04:27:46 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=5/part-00018-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-4025263, partition values: [2018,5]\n",
            "23/05/01 04:27:46 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:46 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:46 INFO CodeGenerator: Code generated in 114.861778 ms\n",
            "23/05/01 04:27:46 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=7/part-00015-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3929948, partition values: [2012,7]\n",
            "23/05/01 04:27:46 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=5/part-00002-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-4007338, partition values: [2019,5]\n",
            "23/05/01 04:27:46 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:46 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:46 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=5/part-00034-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3928208, partition values: [2012,5]\n",
            "23/05/01 04:27:46 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=7/part-00037-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-4006988, partition values: [2019,7]\n",
            "23/05/01 04:27:46 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:46 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:46 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=5/part-00023-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3928147, partition values: [2013,5]\n",
            "23/05/01 04:27:46 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:46 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=5/part-00016-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-4004210, partition values: [2010,5]\n",
            "23/05/01 04:27:46 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:46 INFO DAGScheduler: Registering RDD 16 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
            "23/05/01 04:27:46 INFO DAGScheduler: Got map stage job 4 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 04:27:46 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 04:27:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
            "23/05/01 04:27:46 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 04:27:46 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[16] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 04:27:46 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=5/part-00023-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3924804, partition values: [2011,5]\n",
            "23/05/01 04:27:46 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=7/part-00031-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3996313, partition values: [2020,7]\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=8/part-00029-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3923168, partition values: [2020,8]\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 38.9 KiB, free 365.5 MiB)\n",
            "23/05/01 04:27:47 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 365.5 MiB)\n",
            "23/05/01 04:27:47 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 39049882e5a5:39965 (size: 17.9 KiB, free: 366.2 MiB)\n",
            "23/05/01 04:27:47 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 04:27:47 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[16] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 04:27:47 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=5/part-00038-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3988631, partition values: [2020,5]\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=6/part-00011-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3921171, partition values: [2019,6]\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=5/part-00036-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3979045, partition values: [2017,5]\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=8/part-00035-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3917866, partition values: [2016,8]\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=7/part-00000-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3975611, partition values: [2010,7]\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=7/part-00011-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3961937, partition values: [2017,7]\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=7/part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3915811, partition values: [2015,7]\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=5/part-00008-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3955624, partition values: [2016,5]\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=7/part-00020-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3954022, partition values: [2013,7]\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=8/part-00010-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3915434, partition values: [2019,8]\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=6/part-00002-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3897004, partition values: [2020,6]\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2284 bytes result sent to driver\n",
            "23/05/01 04:27:47 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 5) (39049882e5a5, executor driver, partition 2, PROCESS_LOCAL, 7972 bytes) taskResourceAssignments Map()\n",
            "23/05/01 04:27:47 INFO Executor: Running task 2.0 in stage 3.0 (TID 5)\n",
            "23/05/01 04:27:47 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 5172 ms on 39049882e5a5 (executor driver) (1/9)\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=6/part-00034-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3874146, partition values: [2018,6]\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=8/part-00001-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3894424, partition values: [2017,8]\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=8/part-00005-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3879981, partition values: [2012,8]\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=6/part-00022-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3856025, partition values: [2016,6]\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=8/part-00014-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3855225, partition values: [2014,8]\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=8/part-00024-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3876431, partition values: [2010,8]\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=6/part-00038-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3852960, partition values: [2017,6]\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=8/part-00035-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3852867, partition values: [2013,8]\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=5/part-00037-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3875299, partition values: [2015,5]\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:47 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=8/part-00019-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3851227, partition values: [2011,8]\n",
            "23/05/01 04:27:47 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=6/part-00015-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3842507, partition values: [2010,6]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO Executor: Finished task 1.0 in stage 3.0 (TID 4). 2284 bytes result sent to driver\n",
            "23/05/01 04:27:48 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 6) (39049882e5a5, executor driver, partition 3, PROCESS_LOCAL, 7978 bytes) taskResourceAssignments Map()\n",
            "23/05/01 04:27:48 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 2683 ms on 39049882e5a5 (executor driver) (2/9)\n",
            "23/05/01 04:27:48 INFO Executor: Running task 3.0 in stage 3.0 (TID 6)\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=6/part-00026-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3834361, partition values: [2014,6]\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=3/part-00033-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3795570, partition values: [2012,3]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=6/part-00032-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3794690, partition values: [2015,6]\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=8/part-00016-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3832453, partition values: [2015,8]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=3/part-00001-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3793866, partition values: [2017,3]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=4/part-00033-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3792147, partition values: [2010,4]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=6/part-00014-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3829631, partition values: [2013,6]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=3/part-00007-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3788827, partition values: [2018,3]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=7/part-00019-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3824728, partition values: [2021,7]\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=3/part-00005-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3787406, partition values: [2019,3]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=4/part-00020-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3785899, partition values: [2020,4]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=3/part-00007-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3779093, partition values: [2020,3]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=4/part-00026-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3824033, partition values: [2017,4]\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=4/part-00025-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3777885, partition values: [2016,4]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=4/part-00015-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3820655, partition values: [2015,4]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=4/part-00017-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3764094, partition values: [2011,4]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=6/part-00026-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3816534, partition values: [2011,6]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=10/part-00001-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3759905, partition values: [2019,10]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=6/part-00038-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3816141, partition values: [2012,6]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=3/part-00040-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3758371, partition values: [2016,3]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=4/part-00026-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3802262, partition values: [2019,4]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=3/part-00014-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3801190, partition values: [2013,3]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=8/part-00003-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3757784, partition values: [2021,8]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO Executor: Finished task 2.0 in stage 3.0 (TID 5). 2284 bytes result sent to driver\n",
            "23/05/01 04:27:48 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 7) (39049882e5a5, executor driver, partition 4, PROCESS_LOCAL, 7978 bytes) taskResourceAssignments Map()\n",
            "23/05/01 04:27:48 INFO Executor: Running task 4.0 in stage 3.0 (TID 7)\n",
            "23/05/01 04:27:48 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 5) in 1217 ms on 39049882e5a5 (executor driver) (3/9)\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=9/part-00008-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3743670, partition values: [2020,9]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=4/part-00001-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3755059, partition values: [2014,4]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=4/part-00040-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3752250, partition values: [2018,4]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:48 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=9/part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3742820, partition values: [2018,9]\n",
            "23/05/01 04:27:48 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=4/part-00006-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3736359, partition values: [2012,4]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=4/part-00021-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3745610, partition values: [2013,4]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=9/part-00016-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3730733, partition values: [2016,9]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=5/part-00005-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3745255, partition values: [2021,5]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=10/part-00002-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3728411, partition values: [2017,10]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=3/part-00028-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3728405, partition values: [2010,3]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=10/part-00009-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3723627, partition values: [2013,10]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO Executor: Finished task 3.0 in stage 3.0 (TID 6). 2284 bytes result sent to driver\n",
            "23/05/01 04:27:49 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 8) (39049882e5a5, executor driver, partition 5, PROCESS_LOCAL, 7983 bytes) taskResourceAssignments Map()\n",
            "23/05/01 04:27:49 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 6) in 1207 ms on 39049882e5a5 (executor driver) (4/9)\n",
            "23/05/01 04:27:49 INFO Executor: Running task 5.0 in stage 3.0 (TID 8)\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=10/part-00000-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3680270, partition values: [2012,10]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=10/part-00002-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3719786, partition values: [2018,10]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=3/part-00021-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3714074, partition values: [2014,3]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=10/part-00032-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3713927, partition values: [2020,10]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=9/part-00021-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3712616, partition values: [2017,9]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=9/part-00017-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3675914, partition values: [2010,9]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=10/part-00019-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3706093, partition values: [2016,10]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=3/part-00027-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3701137, partition values: [2015,3]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=10/part-00034-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3673428, partition values: [2011,10]\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=9/part-00029-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3700362, partition values: [2019,9]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=10/part-00008-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3699777, partition values: [2014,10]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=9/part-00039-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3673335, partition values: [2011,9]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=6/part-00004-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3694819, partition values: [2021,6]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=3/part-00013-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3682940, partition values: [2011,3]\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=9/part-00015-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3667357, partition values: [2012,9]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=9/part-00009-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3666989, partition values: [2013,9]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO Executor: Finished task 4.0 in stage 3.0 (TID 7). 2284 bytes result sent to driver\n",
            "23/05/01 04:27:49 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 9) (39049882e5a5, executor driver, partition 6, PROCESS_LOCAL, 7981 bytes) taskResourceAssignments Map()\n",
            "23/05/01 04:27:49 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 7) in 940 ms on 39049882e5a5 (executor driver) (5/9)\n",
            "23/05/01 04:27:49 INFO Executor: Running task 6.0 in stage 3.0 (TID 9)\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=12/part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3542917, partition values: [2019,12]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=9/part-00012-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3538397, partition values: [2021,9]\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=9/part-00020-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3660994, partition values: [2014,9]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=1/part-00029-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3538017, partition values: [2012,1]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=10/part-00027-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3638910, partition values: [2010,10]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:49 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=3/part-00022-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3537995, partition values: [2021,3]\n",
            "23/05/01 04:27:49 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=12/part-00033-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3533037, partition values: [2018,12]\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=9/part-00019-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3621904, partition values: [2015,9]\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=10/part-00021-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3610882, partition values: [2015,10]\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=1/part-00025-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3531889, partition values: [2020,1]\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=4/part-00039-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3593468, partition values: [2021,4]\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=1/part-00014-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3528153, partition values: [2019,1]\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=12/part-00017-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3527889, partition values: [2020,12]\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=1/part-00039-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3584493, partition values: [2017,1]\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=1/part-00034-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3521996, partition values: [2013,1]\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=12/part-00000-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3583973, partition values: [2010,12]\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=12/part-00006-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3519059, partition values: [2012,12]\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=1/part-00038-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3578989, partition values: [2010,1]\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=1/part-00012-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3515591, partition values: [2014,1]\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=10/part-00006-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3558470, partition values: [2021,10]\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=11/part-00037-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3502944, partition values: [2019,11]\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=1/part-00007-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3552780, partition values: [2018,1]\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=12/part-00003-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3500110, partition values: [2015,12]\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=12/part-00024-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3491506, partition values: [2016,12]\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=1/part-00036-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3550882, partition values: [2011,1]\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=12/part-00020-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3468416, partition values: [2017,12]\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO Executor: Finished task 5.0 in stage 3.0 (TID 8). 2284 bytes result sent to driver\n",
            "23/05/01 04:27:50 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 10) (39049882e5a5, executor driver, partition 7, PROCESS_LOCAL, 8172 bytes) taskResourceAssignments Map()\n",
            "23/05/01 04:27:50 INFO Executor: Running task 7.0 in stage 3.0 (TID 10)\n",
            "23/05/01 04:27:50 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 8) in 1663 ms on 39049882e5a5 (executor driver) (6/9)\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=11/part-00013-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3453230, partition values: [2010,11]\n",
            "23/05/01 04:27:50 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=12/part-00018-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3468292, partition values: [2011,12]\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:50 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=1/part-00036-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3457643, partition values: [2015,1]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=2/part-00032-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3431993, partition values: [2020,2]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=12/part-00012-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3431383, partition values: [2013,12]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO Executor: Finished task 6.0 in stage 3.0 (TID 9). 2284 bytes result sent to driver\n",
            "23/05/01 04:27:51 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 11) (39049882e5a5, executor driver, partition 8, PROCESS_LOCAL, 6654 bytes) taskResourceAssignments Map()\n",
            "23/05/01 04:27:51 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 9) in 1367 ms on 39049882e5a5 (executor driver) (7/9)\n",
            "23/05/01 04:27:51 INFO Executor: Running task 8.0 in stage 3.0 (TID 11)\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=2/part-00016-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3334098, partition values: [2019,2]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=2/part-00035-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3319158, partition values: [2017,2]\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=11/part-00033-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3430659, partition values: [2018,11]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=2/part-00028-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3282783, partition values: [2010,2]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=11/part-00006-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3424084, partition values: [2020,11]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=2/part-00017-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3263332, partition values: [2011,2]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=11/part-00004-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3419463, partition values: [2016,11]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=11/part-00035-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3411280, partition values: [2017,11]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=2/part-00024-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3255844, partition values: [2014,2]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2022/month=1/part-00029-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3411037, partition values: [2022,1]\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=2/part-00005-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3238792, partition values: [2013,2]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=11/part-00028-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3403323, partition values: [2011,11]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=2/part-00000-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3233715, partition values: [2015,2]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=11/part-00032-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3401421, partition values: [2014,11]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=11/part-00010-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3181347, partition values: [2021,11]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=12/part-00008-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3394472, partition values: [2014,12]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=2/part-00023-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3104775, partition values: [2021,2]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=11/part-00025-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3392997, partition values: [2012,11]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2022/month=2/part-00009-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-1166956, partition values: [2022,2]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=11/part-00018-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3372381, partition values: [2015,11]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=1/part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3369951, partition values: [2021,1]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=2/part-00031-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3368054, partition values: [2012,2]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO Executor: Finished task 8.0 in stage 3.0 (TID 11). 2284 bytes result sent to driver\n",
            "23/05/01 04:27:51 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 12) (39049882e5a5, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/05/01 04:27:51 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 11) in 626 ms on 39049882e5a5 (executor driver) (8/9)\n",
            "23/05/01 04:27:51 INFO Executor: Running task 0.0 in stage 5.0 (TID 12)\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=11/part-00022-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3366832, partition values: [2013,11]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=12/part-00018-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3357932, partition values: [2021,12]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:51 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=2/part-00037-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3345692, partition values: [2018,2]\n",
            "23/05/01 04:27:51 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 04:27:52 INFO ShuffleBlockFetcherIterator: Getting 1 (1217.4 KiB) non-empty blocks including 1 (1217.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/05/01 04:27:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 46 ms\n",
            "23/05/01 04:27:52 INFO Executor: Finished task 7.0 in stage 3.0 (TID 10). 2284 bytes result sent to driver\n",
            "23/05/01 04:27:52 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 10) in 1150 ms on 39049882e5a5 (executor driver) (9/9)\n",
            "23/05/01 04:27:52 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/05/01 04:27:52 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 9.659 s\n",
            "23/05/01 04:27:52 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/05/01 04:27:52 INFO DAGScheduler: running: Set(ShuffleMapStage 5)\n",
            "23/05/01 04:27:52 INFO DAGScheduler: waiting: Set()\n",
            "23/05/01 04:27:52 INFO DAGScheduler: failed: Set()\n",
            "23/05/01 04:27:52 INFO CodeGenerator: Code generated in 38.593628 ms\n",
            "23/05/01 04:27:52 INFO CodeGenerator: Code generated in 14.031405 ms\n",
            "23/05/01 04:27:52 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1462337, minimum partition size: 1048576\n",
            "23/05/01 04:27:52 INFO CodeGenerator: Code generated in 36.590182 ms\n",
            "23/05/01 04:27:52 INFO CodeGenerator: Code generated in 43.765545 ms\n",
            "23/05/01 04:27:52 INFO CodeGenerator: Code generated in 41.293346 ms\n",
            "23/05/01 04:27:52 INFO CodeGenerator: Code generated in 38.261865 ms\n",
            "23/05/01 04:27:52 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
            "23/05/01 04:27:52 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 2 output partitions\n",
            "23/05/01 04:27:52 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
            "23/05/01 04:27:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
            "23/05/01 04:27:52 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 04:27:52 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
            "23/05/01 04:27:52 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 37.8 KiB, free 356.4 MiB)\n",
            "23/05/01 04:27:52 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 356.4 MiB)\n",
            "23/05/01 04:27:52 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 39049882e5a5:39965 (size: 17.6 KiB, free: 366.2 MiB)\n",
            "23/05/01 04:27:52 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 04:27:52 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/05/01 04:27:52 INFO TaskSchedulerImpl: Adding task set 7.0 with 2 tasks resource profile 0\n",
            "23/05/01 04:27:52 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 13) (39049882e5a5, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/05/01 04:27:52 INFO Executor: Running task 0.0 in stage 7.0 (TID 13)\n",
            "23/05/01 04:27:52 INFO ShuffleBlockFetcherIterator: Getting 9 (1424.4 KiB) non-empty blocks including 9 (1424.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/05/01 04:27:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms\n",
            "23/05/01 04:27:52 INFO CodeGenerator: Code generated in 30.075439 ms\n",
            "23/05/01 04:27:52 INFO CodeGenerator: Code generated in 22.587707 ms\n",
            "23/05/01 04:27:52 INFO CodeGenerator: Code generated in 22.892128 ms\n",
            "23/05/01 04:27:52 INFO CodeGenerator: Code generated in 21.916699 ms\n",
            "23/05/01 04:27:52 INFO CodeGenerator: Code generated in 16.885201 ms\n",
            "23/05/01 04:27:52 INFO CodeGenerator: Code generated in 36.6326 ms\n",
            "23/05/01 04:27:53 INFO CodeGenerator: Code generated in 15.227044 ms\n",
            "23/05/01 04:27:53 INFO CodeGenerator: Code generated in 54.979838 ms\n",
            "23/05/01 04:27:53 INFO Executor: Finished task 0.0 in stage 5.0 (TID 12). 3995 bytes result sent to driver\n",
            "23/05/01 04:27:53 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 14) (39049882e5a5, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/05/01 04:27:53 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 12) in 1599 ms on 39049882e5a5 (executor driver) (1/1)\n",
            "23/05/01 04:27:53 INFO Executor: Running task 1.0 in stage 7.0 (TID 14)\n",
            "23/05/01 04:27:53 INFO DAGScheduler: ShuffleMapStage 5 (csv at NativeMethodAccessorImpl.java:0) finished in 6.371 s\n",
            "23/05/01 04:27:53 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "23/05/01 04:27:53 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/05/01 04:27:53 INFO DAGScheduler: running: Set(ResultStage 7)\n",
            "23/05/01 04:27:53 INFO DAGScheduler: waiting: Set()\n",
            "23/05/01 04:27:53 INFO DAGScheduler: failed: Set()\n",
            "23/05/01 04:27:53 INFO ShuffleBlockFetcherIterator: Getting 9 (1431.7 KiB) non-empty blocks including 9 (1431.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/05/01 04:27:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
            "23/05/01 04:27:53 INFO Executor: Finished task 0.0 in stage 7.0 (TID 13). 719874 bytes result sent to driver\n",
            "23/05/01 04:27:53 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 13) in 1295 ms on 39049882e5a5 (executor driver) (1/2)\n",
            "23/05/01 04:27:54 INFO Executor: Finished task 1.0 in stage 7.0 (TID 14). 724478 bytes result sent to driver\n",
            "23/05/01 04:27:54 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 14) in 638 ms on 39049882e5a5 (executor driver) (2/2)\n",
            "23/05/01 04:27:54 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "23/05/01 04:27:54 INFO DAGScheduler: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1.583 s\n",
            "23/05/01 04:27:54 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/05/01 04:27:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "23/05/01 04:27:54 INFO DAGScheduler: Job 5 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 1.607155 s\n",
            "23/05/01 04:27:54 INFO CodeGenerator: Code generated in 39.040245 ms\n",
            "23/05/01 04:27:54 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 20.0 MiB, free 345.4 MiB)\n",
            "23/05/01 04:27:54 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 39049882e5a5:39965 in memory (size: 17.6 KiB, free: 366.2 MiB)\n",
            "23/05/01 04:27:54 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.0 MiB, free 343.4 MiB)\n",
            "23/05/01 04:27:54 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 39049882e5a5:39965 (size: 2.0 MiB, free: 364.2 MiB)\n",
            "23/05/01 04:27:54 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
            "23/05/01 04:27:54 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/05/01 04:27:54 INFO CodeGenerator: Code generated in 31.043777 ms\n",
            "23/05/01 04:27:54 INFO DAGScheduler: Registering RDD 24 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 3\n",
            "23/05/01 04:27:54 INFO DAGScheduler: Got map stage job 6 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 04:27:54 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 04:27:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\n",
            "23/05/01 04:27:54 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 04:27:54 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[24] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 04:27:54 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 18.1 KiB, free 343.4 MiB)\n",
            "23/05/01 04:27:54 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 343.4 MiB)\n",
            "23/05/01 04:27:54 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 39049882e5a5:39965 (size: 8.1 KiB, free: 364.2 MiB)\n",
            "23/05/01 04:27:54 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 04:27:54 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[24] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 04:27:54 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
            "23/05/01 04:27:54 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 15) (39049882e5a5, executor driver, partition 0, NODE_LOCAL, 4461 bytes) taskResourceAssignments Map()\n",
            "23/05/01 04:27:54 INFO Executor: Running task 0.0 in stage 10.0 (TID 15)\n",
            "23/05/01 04:27:54 INFO ShuffleBlockFetcherIterator: Getting 1 (579.0 KiB) non-empty blocks including 1 (579.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/05/01 04:27:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/05/01 04:27:54 INFO Executor: Finished task 0.0 in stage 10.0 (TID 15). 2797 bytes result sent to driver\n",
            "23/05/01 04:27:54 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 15) in 100 ms on 39049882e5a5 (executor driver) (1/1)\n",
            "23/05/01 04:27:54 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "23/05/01 04:27:54 INFO DAGScheduler: ShuffleMapStage 10 (csv at NativeMethodAccessorImpl.java:0) finished in 0.116 s\n",
            "23/05/01 04:27:54 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/05/01 04:27:54 INFO DAGScheduler: running: Set()\n",
            "23/05/01 04:27:54 INFO DAGScheduler: waiting: Set()\n",
            "23/05/01 04:27:54 INFO DAGScheduler: failed: Set()\n",
            "23/05/01 04:27:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/05/01 04:27:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/05/01 04:27:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/05/01 04:27:54 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 04:27:54 INFO DAGScheduler: Got job 7 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 04:27:54 INFO DAGScheduler: Final stage: ResultStage 14 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 04:27:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\n",
            "23/05/01 04:27:54 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 04:27:54 INFO DAGScheduler: Submitting ResultStage 14 (ShuffledRowRDD[25] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 04:27:54 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 212.1 KiB, free 343.2 MiB)\n",
            "23/05/01 04:27:54 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 75.8 KiB, free 343.1 MiB)\n",
            "23/05/01 04:27:54 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 39049882e5a5:39965 (size: 75.8 KiB, free: 364.1 MiB)\n",
            "23/05/01 04:27:54 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 04:27:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (ShuffledRowRDD[25] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 04:27:54 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
            "23/05/01 04:27:54 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 16) (39049882e5a5, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/05/01 04:27:54 INFO Executor: Running task 0.0 in stage 14.0 (TID 16)\n",
            "23/05/01 04:27:54 INFO ShuffleBlockFetcherIterator: Getting 1 (810.7 KiB) non-empty blocks including 1 (810.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/05/01 04:27:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/05/01 04:27:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/05/01 04:27:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/05/01 04:27:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/05/01 04:27:55 INFO FileOutputCommitter: Saved output of task 'attempt_202305010427541100813964141781358_0014_m_000000_16' to file:/content/localtest2.out/_temporary/0/task_202305010427541100813964141781358_0014_m_000000\n",
            "23/05/01 04:27:55 INFO SparkHadoopMapRedUtil: attempt_202305010427541100813964141781358_0014_m_000000_16: Committed\n",
            "23/05/01 04:27:55 INFO Executor: Finished task 0.0 in stage 14.0 (TID 16). 3483 bytes result sent to driver\n",
            "23/05/01 04:27:55 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 16) in 567 ms on 39049882e5a5 (executor driver) (1/1)\n",
            "23/05/01 04:27:55 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
            "23/05/01 04:27:55 INFO DAGScheduler: ResultStage 14 (csv at NativeMethodAccessorImpl.java:0) finished in 0.618 s\n",
            "23/05/01 04:27:55 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/05/01 04:27:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
            "23/05/01 04:27:55 INFO DAGScheduler: Job 7 finished: csv at NativeMethodAccessorImpl.java:0, took 0.627486 s\n",
            "23/05/01 04:27:55 INFO FileFormatWriter: Start to commit write Job 753c7ae3-c246-44d5-bea2-5d1135d2952d.\n",
            "23/05/01 04:27:55 INFO FileFormatWriter: Write Job 753c7ae3-c246-44d5-bea2-5d1135d2952d committed. Elapsed time: 18 ms.\n",
            "23/05/01 04:27:55 INFO FileFormatWriter: Finished processing stats for write job 753c7ae3-c246-44d5-bea2-5d1135d2952d.\n",
            "23/05/01 04:27:55 INFO SparkUI: Stopped Spark web UI at http://39049882e5a5:4040\n",
            "23/05/01 04:27:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/05/01 04:27:55 INFO MemoryStore: MemoryStore cleared\n",
            "23/05/01 04:27:55 INFO BlockManager: BlockManager stopped\n",
            "23/05/01 04:27:55 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/05/01 04:27:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/05/01 04:27:55 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/05/01 04:27:56 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/05/01 04:27:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-d7012381-6a0e-4cc6-8eaf-da403cf9c0c4\n",
            "23/05/01 04:27:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f/pyspark-b4a0d636-a7d9-492b-b805-e80d07a8902a\n",
            "23/05/01 04:27:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-bdcfc6db-b571-4564-af47-068f27520a4f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rename_file()\n",
        "with open('localtest2.out/results.csv') as f:\n",
        "  lines = f.readlines()\n",
        "  time_hist = pd.read_csv('localtest2.out/results.csv', header=None, names=['time_local', 'Incident_ID', 'resp_time', 'latitude', 'longitude', 'ts_loc', 'station_id', 'precip', 'temp'])\n",
        "time_hist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "3aabRdKsaH-P",
        "outputId": "dfe4d4e7-494d-488c-b889-4ed33174256e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renamed part-00000-ddf94952-3583-4747-935a-1a97962d8a04-c000.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     time_local  Incident_ID  resp_time   latitude  longitude  \\\n",
              "0      2017-01-07T11:10:30.000Z          203      567.0  36.054320 -86.988325   \n",
              "1      2017-01-10T16:18:04.000Z          263      227.0  36.181699 -86.796756   \n",
              "2      2017-01-17T11:07:45.000Z          430      329.0  36.130078 -86.900782   \n",
              "3      2017-01-26T05:05:07.150Z          619      278.0  36.100614 -86.740289   \n",
              "4      2017-01-26T10:11:20.247Z          623      227.0  36.169373 -86.679898   \n",
              "...                         ...          ...        ...        ...        ...   \n",
              "15854  2021-01-26T05:24:32.653Z        29085      648.0  36.074566 -86.693142   \n",
              "15855  2021-02-13T19:19:29.613Z        29441      193.0  36.262998 -86.680948   \n",
              "15856  2021-02-23T09:31:54.293Z        29621       27.0  36.151630 -86.768547   \n",
              "15857  2021-02-23T21:19:36.277Z        29640      139.0  36.221706 -86.761679   \n",
              "15858  2021-02-27T18:11:24.320Z        29725      251.0  36.186880 -86.608581   \n",
              "\n",
              "                         ts_loc station_id  precip  temp  \n",
              "0      2017-01-07T11:00:00.000Z       KBNA     0.0  -7.8  \n",
              "1      2017-01-10T16:00:00.000Z       KBNA     1.0  13.9  \n",
              "2      2017-01-17T11:00:00.000Z       KBNA     1.0  17.2  \n",
              "3      2017-01-26T05:00:00.000Z       KBNA     0.0   5.0  \n",
              "4      2017-01-26T10:00:00.000Z       KBNA     0.0   5.0  \n",
              "...                         ...        ...     ...   ...  \n",
              "15854  2021-01-26T05:00:00.000Z       KBNA     0.0  12.2  \n",
              "15855  2021-02-13T19:00:00.000Z       KBNA     0.0  -3.3  \n",
              "15856  2021-02-23T09:00:00.000Z       KBNA     0.0  12.2  \n",
              "15857  2021-02-23T21:00:00.000Z       KBNA     0.0  10.0  \n",
              "15858  2021-02-27T18:00:00.000Z       KBNA     0.5  13.9  \n",
              "\n",
              "[15859 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-92243572-30b3-4679-b371-56cc630451c8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time_local</th>\n",
              "      <th>Incident_ID</th>\n",
              "      <th>resp_time</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>ts_loc</th>\n",
              "      <th>station_id</th>\n",
              "      <th>precip</th>\n",
              "      <th>temp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017-01-07T11:10:30.000Z</td>\n",
              "      <td>203</td>\n",
              "      <td>567.0</td>\n",
              "      <td>36.054320</td>\n",
              "      <td>-86.988325</td>\n",
              "      <td>2017-01-07T11:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-7.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017-01-10T16:18:04.000Z</td>\n",
              "      <td>263</td>\n",
              "      <td>227.0</td>\n",
              "      <td>36.181699</td>\n",
              "      <td>-86.796756</td>\n",
              "      <td>2017-01-10T16:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>1.0</td>\n",
              "      <td>13.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017-01-17T11:07:45.000Z</td>\n",
              "      <td>430</td>\n",
              "      <td>329.0</td>\n",
              "      <td>36.130078</td>\n",
              "      <td>-86.900782</td>\n",
              "      <td>2017-01-17T11:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017-01-26T05:05:07.150Z</td>\n",
              "      <td>619</td>\n",
              "      <td>278.0</td>\n",
              "      <td>36.100614</td>\n",
              "      <td>-86.740289</td>\n",
              "      <td>2017-01-26T05:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017-01-26T10:11:20.247Z</td>\n",
              "      <td>623</td>\n",
              "      <td>227.0</td>\n",
              "      <td>36.169373</td>\n",
              "      <td>-86.679898</td>\n",
              "      <td>2017-01-26T10:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15854</th>\n",
              "      <td>2021-01-26T05:24:32.653Z</td>\n",
              "      <td>29085</td>\n",
              "      <td>648.0</td>\n",
              "      <td>36.074566</td>\n",
              "      <td>-86.693142</td>\n",
              "      <td>2021-01-26T05:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15855</th>\n",
              "      <td>2021-02-13T19:19:29.613Z</td>\n",
              "      <td>29441</td>\n",
              "      <td>193.0</td>\n",
              "      <td>36.262998</td>\n",
              "      <td>-86.680948</td>\n",
              "      <td>2021-02-13T19:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-3.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15856</th>\n",
              "      <td>2021-02-23T09:31:54.293Z</td>\n",
              "      <td>29621</td>\n",
              "      <td>27.0</td>\n",
              "      <td>36.151630</td>\n",
              "      <td>-86.768547</td>\n",
              "      <td>2021-02-23T09:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15857</th>\n",
              "      <td>2021-02-23T21:19:36.277Z</td>\n",
              "      <td>29640</td>\n",
              "      <td>139.0</td>\n",
              "      <td>36.221706</td>\n",
              "      <td>-86.761679</td>\n",
              "      <td>2021-02-23T21:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15858</th>\n",
              "      <td>2021-02-27T18:11:24.320Z</td>\n",
              "      <td>29725</td>\n",
              "      <td>251.0</td>\n",
              "      <td>36.186880</td>\n",
              "      <td>-86.608581</td>\n",
              "      <td>2021-02-27T18:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.5</td>\n",
              "      <td>13.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15859 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92243572-30b3-4679-b371-56cc630451c8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-92243572-30b3-4679-b371-56cc630451c8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-92243572-30b3-4679-b371-56cc630451c8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(time_hist.columns)\n",
        "order_by_time = time_hist.sort_values('time_local', axis=0, ascending=True)\n",
        "order_by_time[2000:2050]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "28tKCZnnqhRC",
        "outputId": "bf8c7f74-ec6f-4f5e-e19c-db39146ff731"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['time_local', 'Incident_ID', 'resp_time', 'latitude', 'longitude',\n",
            "       'ts_loc', 'station_id', 'precip', 'temp'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     time_local  Incident_ID  resp_time   latitude  longitude  \\\n",
              "2009   2017-05-25T14:06:08.000Z         3580      123.0  36.262217 -86.712215   \n",
              "10141  2017-05-25T17:20:27.000Z         3581      273.0  36.155024 -86.627114   \n",
              "12370  2017-05-25T18:27:50.000Z         3583      272.0  36.139300 -86.700827   \n",
              "8586   2017-05-25T23:38:10.000Z         3585      299.0  36.192306 -86.829532   \n",
              "14179  2017-05-26T01:53:18.000Z         3586      375.0  36.157125 -86.803619   \n",
              "15414  2017-05-26T02:01:42.000Z         3587      307.0  36.144859 -86.711042   \n",
              "12033  2017-05-26T03:10:18.000Z         3590      393.0  36.152722 -86.633926   \n",
              "4772   2017-05-26T04:11:40.000Z         3591      425.0  36.090659 -86.649397   \n",
              "4932   2017-05-26T05:45:41.000Z         3592      220.0  36.042316 -86.726529   \n",
              "11493  2017-05-26T06:45:29.000Z         3593      221.0  36.074486 -86.920950   \n",
              "7911   2017-05-26T07:41:00.000Z         3595      647.0  36.155706 -86.629095   \n",
              "13539  2017-05-26T08:03:15.000Z         3596      298.0  36.109940 -86.734610   \n",
              "4860   2017-05-26T09:11:25.000Z         3601      264.0  36.167802 -86.820752   \n",
              "6205   2017-05-26T10:13:38.000Z         3602      539.0  36.115934 -86.785744   \n",
              "650    2017-05-26T11:13:29.000Z         3606      468.0  36.184219 -86.798068   \n",
              "7171   2017-05-26T12:05:54.000Z         3609      192.0  36.082110 -86.761709   \n",
              "10300  2017-05-26T13:34:23.000Z         3614      207.0  36.169373 -86.679898   \n",
              "13390  2017-05-26T14:01:23.000Z         3616       12.0  36.276159 -86.834295   \n",
              "15788  2017-05-26T16:05:04.000Z         3619      264.0  36.206630 -86.777911   \n",
              "2389   2017-05-26T17:07:57.000Z         3620      496.0  36.039435 -86.649135   \n",
              "2788   2017-05-26T18:33:07.000Z         3625      342.0  36.145959 -86.666606   \n",
              "6038   2017-05-26T20:19:23.000Z         3627      281.0  36.147078 -86.765558   \n",
              "10681  2017-05-26T21:05:49.000Z         3628      243.0  36.148396 -86.805988   \n",
              "4111   2017-05-26T22:29:41.000Z         3632      212.0  36.170412 -86.768454   \n",
              "1078   2017-05-27T03:17:27.000Z         3634       37.0  36.222822 -86.725828   \n",
              "10998  2017-05-27T04:33:35.000Z         3635      127.0  36.045746 -86.674355   \n",
              "11874  2017-05-27T07:48:47.000Z         3637      390.0  36.308619 -86.681543   \n",
              "1627   2017-05-27T09:59:24.000Z         3638      150.0  36.156936 -86.804474   \n",
              "14260  2017-05-27T11:03:41.000Z         3639      288.0  36.081120 -86.755394   \n",
              "13070  2017-05-27T12:21:49.000Z         3645      351.0  36.237839 -86.633597   \n",
              "8213   2017-05-27T14:39:07.000Z         3647      310.0  36.144383 -86.696950   \n",
              "5402   2017-05-27T15:07:46.000Z         3649      290.0  36.116136 -86.773316   \n",
              "14261  2017-05-27T16:31:45.000Z         3651      435.0  36.038895 -86.782816   \n",
              "10142  2017-05-27T19:13:51.000Z         3653      332.0  36.069106 -86.690682   \n",
              "5475   2017-05-27T20:11:34.000Z         3655      807.0  36.083946 -86.689862   \n",
              "10462  2017-05-27T21:26:24.000Z         3656      423.0  36.069205 -86.689417   \n",
              "2553   2017-05-27T22:07:14.000Z         3658      301.0  36.090031 -86.733669   \n",
              "2481   2017-05-27T23:00:40.000Z         3661      453.0  36.092373 -86.705219   \n",
              "5403   2017-05-28T00:48:43.000Z         3662      206.0  36.160523 -86.801872   \n",
              "6431   2017-05-28T01:51:09.000Z         3663      390.0  36.174043 -86.605942   \n",
              "11875  2017-05-28T02:21:47.000Z         3664      172.0  36.082678 -86.710501   \n",
              "10463  2017-05-28T03:19:11.000Z         3665      295.0  36.245795 -86.779417   \n",
              "6508   2017-05-28T05:25:38.000Z         3666      469.0  36.307379 -86.694720   \n",
              "2950   2017-05-28T06:52:56.000Z         3667      327.0  36.145239 -86.666754   \n",
              "15725  2017-05-28T07:14:23.000Z         3668      236.0  36.130399 -86.901371   \n",
              "6432   2017-05-28T08:19:49.000Z         3670      418.0  36.077117 -86.777361   \n",
              "8062   2017-05-28T11:33:02.000Z         3671      423.0  36.045746 -86.674355   \n",
              "11494  2017-05-28T13:21:40.000Z         3672      190.0  36.122247 -86.704782   \n",
              "15507  2017-05-28T14:45:52.000Z         3673      238.0  36.142941 -86.830840   \n",
              "9657   2017-05-28T15:28:28.000Z         3674      271.0  36.072143 -86.732899   \n",
              "\n",
              "                         ts_loc station_id  precip  temp  \n",
              "2009   2017-05-25T14:00:00.000Z       KBNA     0.0  22.8  \n",
              "10141  2017-05-25T17:00:00.000Z       KBNA     0.0  23.9  \n",
              "12370  2017-05-25T18:00:00.000Z       KBNA     0.0  22.8  \n",
              "8586   2017-05-25T23:00:00.000Z       KBNA     0.0  15.0  \n",
              "14179  2017-05-26T01:00:00.000Z       KBNA     0.0  13.3  \n",
              "15414  2017-05-26T02:00:00.000Z       KBNA     0.0  13.3  \n",
              "12033  2017-05-26T03:00:00.000Z       KBNA     0.0  13.3  \n",
              "4772   2017-05-26T04:00:00.000Z       KBNA     0.0  12.2  \n",
              "4932   2017-05-26T05:00:00.000Z       KBNA     0.0  12.2  \n",
              "11493  2017-05-26T06:00:00.000Z       KBNA     0.0  12.2  \n",
              "7911   2017-05-26T07:00:00.000Z       KBNA     0.0  15.0  \n",
              "13539  2017-05-26T08:00:00.000Z       KBNA     0.0  18.3  \n",
              "4860   2017-05-26T09:00:00.000Z       KBNA     0.0  21.7  \n",
              "6205   2017-05-26T10:00:00.000Z       KBNA     0.0  22.8  \n",
              "650    2017-05-26T11:00:00.000Z       KBNA     0.0  23.3  \n",
              "7171   2017-05-26T12:00:00.000Z       KBNA     0.0  26.7  \n",
              "10300  2017-05-26T13:00:00.000Z       KBNA     0.0  28.3  \n",
              "13390  2017-05-26T14:00:00.000Z       KBNA     0.0  29.4  \n",
              "15788  2017-05-26T16:00:00.000Z       KBNA     0.0  30.6  \n",
              "2389   2017-05-26T17:00:00.000Z       KBNA     0.0  30.0  \n",
              "2788   2017-05-26T18:00:00.000Z       KBNA     0.0  29.4  \n",
              "6038   2017-05-26T20:00:00.000Z       KBNA     0.0  27.2  \n",
              "10681  2017-05-26T21:00:00.000Z       KBNA     0.0  25.6  \n",
              "4111   2017-05-26T22:00:00.000Z       KBNA     0.0  23.3  \n",
              "1078   2017-05-27T03:00:00.000Z       KBNA     0.0  23.9  \n",
              "10998  2017-05-27T04:00:00.000Z       KBNA     0.0  22.8  \n",
              "11874  2017-05-27T07:00:00.000Z       KBNA     0.0  22.8  \n",
              "1627   2017-05-27T09:00:00.000Z       KBNA     0.0  23.3  \n",
              "14260  2017-05-27T11:00:00.000Z       KBNA     0.5  22.8  \n",
              "13070  2017-05-27T12:00:00.000Z       KBNA     2.0  27.8  \n",
              "8213   2017-05-27T14:00:00.000Z       KBNA     0.0  26.7  \n",
              "5402   2017-05-27T15:00:00.000Z       KBNA     0.0  27.2  \n",
              "14261  2017-05-27T16:00:00.000Z       KBNA     0.0  26.1  \n",
              "10142  2017-05-27T19:00:00.000Z       KBNA     0.0  24.4  \n",
              "5475   2017-05-27T20:00:00.000Z       KBNA     0.0  19.4  \n",
              "10462  2017-05-27T21:00:00.000Z       KBNA     0.0  18.9  \n",
              "2553   2017-05-27T22:00:00.000Z       KBNA     0.0  18.3  \n",
              "2481   2017-05-27T23:00:00.000Z       KBNA     0.0  18.9  \n",
              "5403   2017-05-28T00:00:00.000Z       KBNA     0.0  18.3  \n",
              "6431   2017-05-28T01:00:00.000Z       KBNA     0.5  18.3  \n",
              "11875  2017-05-28T02:00:00.000Z       KBNA     1.0  18.3  \n",
              "10463  2017-05-28T03:00:00.000Z       KBNA     0.5  18.3  \n",
              "6508   2017-05-28T05:00:00.000Z       KBNA     0.0  18.3  \n",
              "2950   2017-05-28T06:00:00.000Z       KBNA     0.0  18.3  \n",
              "15725  2017-05-28T07:00:00.000Z       KBNA     0.0  18.9  \n",
              "6432   2017-05-28T08:00:00.000Z       KBNA     0.0  18.9  \n",
              "8062   2017-05-28T11:00:00.000Z       KBNA     0.0  21.7  \n",
              "11494  2017-05-28T13:00:00.000Z       KBNA     0.0  25.6  \n",
              "15507  2017-05-28T14:00:00.000Z       KBNA     0.0  25.6  \n",
              "9657   2017-05-28T15:00:00.000Z       KBNA     0.0  26.1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-13369605-fb8e-4355-8fa7-cbb5acf07505\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time_local</th>\n",
              "      <th>Incident_ID</th>\n",
              "      <th>resp_time</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>ts_loc</th>\n",
              "      <th>station_id</th>\n",
              "      <th>precip</th>\n",
              "      <th>temp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2009</th>\n",
              "      <td>2017-05-25T14:06:08.000Z</td>\n",
              "      <td>3580</td>\n",
              "      <td>123.0</td>\n",
              "      <td>36.262217</td>\n",
              "      <td>-86.712215</td>\n",
              "      <td>2017-05-25T14:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10141</th>\n",
              "      <td>2017-05-25T17:20:27.000Z</td>\n",
              "      <td>3581</td>\n",
              "      <td>273.0</td>\n",
              "      <td>36.155024</td>\n",
              "      <td>-86.627114</td>\n",
              "      <td>2017-05-25T17:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12370</th>\n",
              "      <td>2017-05-25T18:27:50.000Z</td>\n",
              "      <td>3583</td>\n",
              "      <td>272.0</td>\n",
              "      <td>36.139300</td>\n",
              "      <td>-86.700827</td>\n",
              "      <td>2017-05-25T18:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8586</th>\n",
              "      <td>2017-05-25T23:38:10.000Z</td>\n",
              "      <td>3585</td>\n",
              "      <td>299.0</td>\n",
              "      <td>36.192306</td>\n",
              "      <td>-86.829532</td>\n",
              "      <td>2017-05-25T23:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14179</th>\n",
              "      <td>2017-05-26T01:53:18.000Z</td>\n",
              "      <td>3586</td>\n",
              "      <td>375.0</td>\n",
              "      <td>36.157125</td>\n",
              "      <td>-86.803619</td>\n",
              "      <td>2017-05-26T01:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15414</th>\n",
              "      <td>2017-05-26T02:01:42.000Z</td>\n",
              "      <td>3587</td>\n",
              "      <td>307.0</td>\n",
              "      <td>36.144859</td>\n",
              "      <td>-86.711042</td>\n",
              "      <td>2017-05-26T02:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12033</th>\n",
              "      <td>2017-05-26T03:10:18.000Z</td>\n",
              "      <td>3590</td>\n",
              "      <td>393.0</td>\n",
              "      <td>36.152722</td>\n",
              "      <td>-86.633926</td>\n",
              "      <td>2017-05-26T03:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4772</th>\n",
              "      <td>2017-05-26T04:11:40.000Z</td>\n",
              "      <td>3591</td>\n",
              "      <td>425.0</td>\n",
              "      <td>36.090659</td>\n",
              "      <td>-86.649397</td>\n",
              "      <td>2017-05-26T04:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4932</th>\n",
              "      <td>2017-05-26T05:45:41.000Z</td>\n",
              "      <td>3592</td>\n",
              "      <td>220.0</td>\n",
              "      <td>36.042316</td>\n",
              "      <td>-86.726529</td>\n",
              "      <td>2017-05-26T05:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11493</th>\n",
              "      <td>2017-05-26T06:45:29.000Z</td>\n",
              "      <td>3593</td>\n",
              "      <td>221.0</td>\n",
              "      <td>36.074486</td>\n",
              "      <td>-86.920950</td>\n",
              "      <td>2017-05-26T06:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7911</th>\n",
              "      <td>2017-05-26T07:41:00.000Z</td>\n",
              "      <td>3595</td>\n",
              "      <td>647.0</td>\n",
              "      <td>36.155706</td>\n",
              "      <td>-86.629095</td>\n",
              "      <td>2017-05-26T07:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13539</th>\n",
              "      <td>2017-05-26T08:03:15.000Z</td>\n",
              "      <td>3596</td>\n",
              "      <td>298.0</td>\n",
              "      <td>36.109940</td>\n",
              "      <td>-86.734610</td>\n",
              "      <td>2017-05-26T08:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4860</th>\n",
              "      <td>2017-05-26T09:11:25.000Z</td>\n",
              "      <td>3601</td>\n",
              "      <td>264.0</td>\n",
              "      <td>36.167802</td>\n",
              "      <td>-86.820752</td>\n",
              "      <td>2017-05-26T09:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6205</th>\n",
              "      <td>2017-05-26T10:13:38.000Z</td>\n",
              "      <td>3602</td>\n",
              "      <td>539.0</td>\n",
              "      <td>36.115934</td>\n",
              "      <td>-86.785744</td>\n",
              "      <td>2017-05-26T10:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>650</th>\n",
              "      <td>2017-05-26T11:13:29.000Z</td>\n",
              "      <td>3606</td>\n",
              "      <td>468.0</td>\n",
              "      <td>36.184219</td>\n",
              "      <td>-86.798068</td>\n",
              "      <td>2017-05-26T11:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7171</th>\n",
              "      <td>2017-05-26T12:05:54.000Z</td>\n",
              "      <td>3609</td>\n",
              "      <td>192.0</td>\n",
              "      <td>36.082110</td>\n",
              "      <td>-86.761709</td>\n",
              "      <td>2017-05-26T12:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10300</th>\n",
              "      <td>2017-05-26T13:34:23.000Z</td>\n",
              "      <td>3614</td>\n",
              "      <td>207.0</td>\n",
              "      <td>36.169373</td>\n",
              "      <td>-86.679898</td>\n",
              "      <td>2017-05-26T13:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13390</th>\n",
              "      <td>2017-05-26T14:01:23.000Z</td>\n",
              "      <td>3616</td>\n",
              "      <td>12.0</td>\n",
              "      <td>36.276159</td>\n",
              "      <td>-86.834295</td>\n",
              "      <td>2017-05-26T14:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15788</th>\n",
              "      <td>2017-05-26T16:05:04.000Z</td>\n",
              "      <td>3619</td>\n",
              "      <td>264.0</td>\n",
              "      <td>36.206630</td>\n",
              "      <td>-86.777911</td>\n",
              "      <td>2017-05-26T16:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2389</th>\n",
              "      <td>2017-05-26T17:07:57.000Z</td>\n",
              "      <td>3620</td>\n",
              "      <td>496.0</td>\n",
              "      <td>36.039435</td>\n",
              "      <td>-86.649135</td>\n",
              "      <td>2017-05-26T17:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2788</th>\n",
              "      <td>2017-05-26T18:33:07.000Z</td>\n",
              "      <td>3625</td>\n",
              "      <td>342.0</td>\n",
              "      <td>36.145959</td>\n",
              "      <td>-86.666606</td>\n",
              "      <td>2017-05-26T18:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6038</th>\n",
              "      <td>2017-05-26T20:19:23.000Z</td>\n",
              "      <td>3627</td>\n",
              "      <td>281.0</td>\n",
              "      <td>36.147078</td>\n",
              "      <td>-86.765558</td>\n",
              "      <td>2017-05-26T20:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10681</th>\n",
              "      <td>2017-05-26T21:05:49.000Z</td>\n",
              "      <td>3628</td>\n",
              "      <td>243.0</td>\n",
              "      <td>36.148396</td>\n",
              "      <td>-86.805988</td>\n",
              "      <td>2017-05-26T21:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4111</th>\n",
              "      <td>2017-05-26T22:29:41.000Z</td>\n",
              "      <td>3632</td>\n",
              "      <td>212.0</td>\n",
              "      <td>36.170412</td>\n",
              "      <td>-86.768454</td>\n",
              "      <td>2017-05-26T22:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1078</th>\n",
              "      <td>2017-05-27T03:17:27.000Z</td>\n",
              "      <td>3634</td>\n",
              "      <td>37.0</td>\n",
              "      <td>36.222822</td>\n",
              "      <td>-86.725828</td>\n",
              "      <td>2017-05-27T03:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10998</th>\n",
              "      <td>2017-05-27T04:33:35.000Z</td>\n",
              "      <td>3635</td>\n",
              "      <td>127.0</td>\n",
              "      <td>36.045746</td>\n",
              "      <td>-86.674355</td>\n",
              "      <td>2017-05-27T04:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11874</th>\n",
              "      <td>2017-05-27T07:48:47.000Z</td>\n",
              "      <td>3637</td>\n",
              "      <td>390.0</td>\n",
              "      <td>36.308619</td>\n",
              "      <td>-86.681543</td>\n",
              "      <td>2017-05-27T07:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1627</th>\n",
              "      <td>2017-05-27T09:59:24.000Z</td>\n",
              "      <td>3638</td>\n",
              "      <td>150.0</td>\n",
              "      <td>36.156936</td>\n",
              "      <td>-86.804474</td>\n",
              "      <td>2017-05-27T09:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14260</th>\n",
              "      <td>2017-05-27T11:03:41.000Z</td>\n",
              "      <td>3639</td>\n",
              "      <td>288.0</td>\n",
              "      <td>36.081120</td>\n",
              "      <td>-86.755394</td>\n",
              "      <td>2017-05-27T11:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.5</td>\n",
              "      <td>22.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13070</th>\n",
              "      <td>2017-05-27T12:21:49.000Z</td>\n",
              "      <td>3645</td>\n",
              "      <td>351.0</td>\n",
              "      <td>36.237839</td>\n",
              "      <td>-86.633597</td>\n",
              "      <td>2017-05-27T12:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>2.0</td>\n",
              "      <td>27.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8213</th>\n",
              "      <td>2017-05-27T14:39:07.000Z</td>\n",
              "      <td>3647</td>\n",
              "      <td>310.0</td>\n",
              "      <td>36.144383</td>\n",
              "      <td>-86.696950</td>\n",
              "      <td>2017-05-27T14:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5402</th>\n",
              "      <td>2017-05-27T15:07:46.000Z</td>\n",
              "      <td>3649</td>\n",
              "      <td>290.0</td>\n",
              "      <td>36.116136</td>\n",
              "      <td>-86.773316</td>\n",
              "      <td>2017-05-27T15:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14261</th>\n",
              "      <td>2017-05-27T16:31:45.000Z</td>\n",
              "      <td>3651</td>\n",
              "      <td>435.0</td>\n",
              "      <td>36.038895</td>\n",
              "      <td>-86.782816</td>\n",
              "      <td>2017-05-27T16:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10142</th>\n",
              "      <td>2017-05-27T19:13:51.000Z</td>\n",
              "      <td>3653</td>\n",
              "      <td>332.0</td>\n",
              "      <td>36.069106</td>\n",
              "      <td>-86.690682</td>\n",
              "      <td>2017-05-27T19:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5475</th>\n",
              "      <td>2017-05-27T20:11:34.000Z</td>\n",
              "      <td>3655</td>\n",
              "      <td>807.0</td>\n",
              "      <td>36.083946</td>\n",
              "      <td>-86.689862</td>\n",
              "      <td>2017-05-27T20:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10462</th>\n",
              "      <td>2017-05-27T21:26:24.000Z</td>\n",
              "      <td>3656</td>\n",
              "      <td>423.0</td>\n",
              "      <td>36.069205</td>\n",
              "      <td>-86.689417</td>\n",
              "      <td>2017-05-27T21:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2553</th>\n",
              "      <td>2017-05-27T22:07:14.000Z</td>\n",
              "      <td>3658</td>\n",
              "      <td>301.0</td>\n",
              "      <td>36.090031</td>\n",
              "      <td>-86.733669</td>\n",
              "      <td>2017-05-27T22:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2481</th>\n",
              "      <td>2017-05-27T23:00:40.000Z</td>\n",
              "      <td>3661</td>\n",
              "      <td>453.0</td>\n",
              "      <td>36.092373</td>\n",
              "      <td>-86.705219</td>\n",
              "      <td>2017-05-27T23:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5403</th>\n",
              "      <td>2017-05-28T00:48:43.000Z</td>\n",
              "      <td>3662</td>\n",
              "      <td>206.0</td>\n",
              "      <td>36.160523</td>\n",
              "      <td>-86.801872</td>\n",
              "      <td>2017-05-28T00:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6431</th>\n",
              "      <td>2017-05-28T01:51:09.000Z</td>\n",
              "      <td>3663</td>\n",
              "      <td>390.0</td>\n",
              "      <td>36.174043</td>\n",
              "      <td>-86.605942</td>\n",
              "      <td>2017-05-28T01:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.5</td>\n",
              "      <td>18.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11875</th>\n",
              "      <td>2017-05-28T02:21:47.000Z</td>\n",
              "      <td>3664</td>\n",
              "      <td>172.0</td>\n",
              "      <td>36.082678</td>\n",
              "      <td>-86.710501</td>\n",
              "      <td>2017-05-28T02:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>1.0</td>\n",
              "      <td>18.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10463</th>\n",
              "      <td>2017-05-28T03:19:11.000Z</td>\n",
              "      <td>3665</td>\n",
              "      <td>295.0</td>\n",
              "      <td>36.245795</td>\n",
              "      <td>-86.779417</td>\n",
              "      <td>2017-05-28T03:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.5</td>\n",
              "      <td>18.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6508</th>\n",
              "      <td>2017-05-28T05:25:38.000Z</td>\n",
              "      <td>3666</td>\n",
              "      <td>469.0</td>\n",
              "      <td>36.307379</td>\n",
              "      <td>-86.694720</td>\n",
              "      <td>2017-05-28T05:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2950</th>\n",
              "      <td>2017-05-28T06:52:56.000Z</td>\n",
              "      <td>3667</td>\n",
              "      <td>327.0</td>\n",
              "      <td>36.145239</td>\n",
              "      <td>-86.666754</td>\n",
              "      <td>2017-05-28T06:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15725</th>\n",
              "      <td>2017-05-28T07:14:23.000Z</td>\n",
              "      <td>3668</td>\n",
              "      <td>236.0</td>\n",
              "      <td>36.130399</td>\n",
              "      <td>-86.901371</td>\n",
              "      <td>2017-05-28T07:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6432</th>\n",
              "      <td>2017-05-28T08:19:49.000Z</td>\n",
              "      <td>3670</td>\n",
              "      <td>418.0</td>\n",
              "      <td>36.077117</td>\n",
              "      <td>-86.777361</td>\n",
              "      <td>2017-05-28T08:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8062</th>\n",
              "      <td>2017-05-28T11:33:02.000Z</td>\n",
              "      <td>3671</td>\n",
              "      <td>423.0</td>\n",
              "      <td>36.045746</td>\n",
              "      <td>-86.674355</td>\n",
              "      <td>2017-05-28T11:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11494</th>\n",
              "      <td>2017-05-28T13:21:40.000Z</td>\n",
              "      <td>3672</td>\n",
              "      <td>190.0</td>\n",
              "      <td>36.122247</td>\n",
              "      <td>-86.704782</td>\n",
              "      <td>2017-05-28T13:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15507</th>\n",
              "      <td>2017-05-28T14:45:52.000Z</td>\n",
              "      <td>3673</td>\n",
              "      <td>238.0</td>\n",
              "      <td>36.142941</td>\n",
              "      <td>-86.830840</td>\n",
              "      <td>2017-05-28T14:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9657</th>\n",
              "      <td>2017-05-28T15:28:28.000Z</td>\n",
              "      <td>3674</td>\n",
              "      <td>271.0</td>\n",
              "      <td>36.072143</td>\n",
              "      <td>-86.732899</td>\n",
              "      <td>2017-05-28T15:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13369605-fb8e-4355-8fa7-cbb5acf07505')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-13369605-fb8e-4355-8fa7-cbb5acf07505 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-13369605-fb8e-4355-8fa7-cbb5acf07505');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving this join in a separate file so that it does not need to be re-computed"
      ],
      "metadata": {
        "id": "dMsl-03abqdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_hist.to_csv('weather-incident-full-join.csv')"
      ],
      "metadata": {
        "id": "9RceAIMwbZSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now we can run queries on the dataset. Here are a few things we would like to determine:\n",
        "\n",
        "1) How many incidents were correlated with precipitation?\n",
        "\n",
        "2) How many incidents correlated with precipitation had freezing temperatures?\n",
        "\n",
        "3) Plot of % of incidents that had precipitation by census\n",
        "\n"
      ],
      "metadata": {
        "id": "jVlt9YQtknBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) How many incidents were correlated with precipitation?"
      ],
      "metadata": {
        "id": "joSnrHwNnhG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_hist['precip'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw_OfuPsmSqx",
        "outputId": "8c8db52d-0057-4028-8788-8b0fecb8a6e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    15859.000000\n",
              "mean         0.208784\n",
              "std          0.755929\n",
              "min          0.000000\n",
              "25%          0.000000\n",
              "50%          0.000000\n",
              "75%          0.000000\n",
              "max         17.000000\n",
              "Name: precip, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nonzero_precip = time_hist['precip'].loc[~(time_hist['precip']==0)]\n",
        "len(nonzero_precip)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycVRPwl7m-pV",
        "outputId": "9686852e-217e-407d-a51d-005da3a395aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2293"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"{:.2f}%\".format((len(nonzero_precip) / len(time_hist['precip']) * 100)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfZ5v7ilntIz",
        "outputId": "145e515e-13dd-45ff-c89a-508e4094ac1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14.46%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that 2,293 incidents had any precipitation involved.\n",
        "\n",
        "This is equivalent to: 14.46%"
      ],
      "metadata": {
        "id": "d6_9gf86nlO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Of those, how many had freezing temperatures?"
      ],
      "metadata": {
        "id": "xvDJXJC3oMjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nonzero_precip_freezing = time_hist[(~(time_hist['precip']==0)) & (time_hist['temp'] <= 32)]\n",
        "len(nonzero_precip_freezing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L-coxBTk2sP",
        "outputId": "00dd90e8-0397-4dd8-fb24-b22108fa91e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"{:.2f}\".format(len(nonzero_precip_freezing) / len(nonzero_precip)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir9Q7Eh5pFaO",
        "outputId": "767458b5-0559-4591-c507-3df244cb02c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_null = nonzero_precip_freezing[nonzero_precip_freezing['temp'].isna()]\n",
        "print(len(check_null))\n",
        "check_null.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "NLLmzzOUp3XY",
        "outputId": "6f7accf2-fdeb-4404-bc76-73e0b03d8182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [time_local, Incident_ID, resp_time, latitude, longitude, ts_loc, station_id, precip, temp]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fc55706a-9baf-4b9b-a8ab-0b21f204bab5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time_local</th>\n",
              "      <th>Incident_ID</th>\n",
              "      <th>resp_time</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>ts_loc</th>\n",
              "      <th>station_id</th>\n",
              "      <th>precip</th>\n",
              "      <th>temp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc55706a-9baf-4b9b-a8ab-0b21f204bab5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fc55706a-9baf-4b9b-a8ab-0b21f204bab5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fc55706a-9baf-4b9b-a8ab-0b21f204bab5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That seems, really off... \n",
        "\n",
        "We're seeing that 98% of incidents with precipitation had freezing temperatures\n",
        "\n",
        "Let's double check that"
      ],
      "metadata": {
        "id": "8hRdk_0wpPtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nonzero_precip_warm = time_hist[(~(time_hist['precip']==0)) & (time_hist['temp'] > 32)]\n",
        "print(len(nonzero_precip_warm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98yaGy6bpRwM",
        "outputId": "d21d034e-d528-454b-b25e-0942a5b67c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This means that we are most likely dealing with temperatures in Celsius...let's check the max temperature to get an indication"
      ],
      "metadata": {
        "id": "AdoWDsAOqUPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(time_hist['temp'].max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv_yzXgDsYDN",
        "outputId": "3a710b75-a233-49a9-9caa-cde9d2050080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definitely in Celsius. Let's re-run what we had above"
      ],
      "metadata": {
        "id": "IIsVvNYis3YG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nonzero_precip_freezing = time_hist[(~(time_hist['precip']==0)) & (time_hist['temp'] <= 0)]\n",
        "len(nonzero_precip_freezing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOOUdZ59s53f",
        "outputId": "0f9dcb55-b73e-40dd-dbef-4277abc77555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"{:.2f}\".format(len(nonzero_precip_freezing) / len(nonzero_precip)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvxpsZZ6tDv4",
        "outputId": "6c89562c-31c0-4729-ec9b-70bddc4ad29e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nonzero_precip_warm = time_hist[(~(time_hist['precip']==0)) & (time_hist['temp'] > 0)]\n",
        "print(len(nonzero_precip_warm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVBPn9_NtHpC",
        "outputId": "c260031a-6949-4037-b783-45f9c12bc695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now see that only 1% of incidents with precipitation have freezing temperatures... this means that ice is likely not a significant factor in crashes in Nashville"
      ],
      "metadata": {
        "id": "Kact3qN_tL8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Last thing to test: Let's see if there's a higher percentage of incidents with precipitation than the percentage of precipitation in weather measurements in general. This will give us a good idea of whether precipitation plays a role in causing incidents"
      ],
      "metadata": {
        "id": "5w31zlEBveTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file localtest2.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('localtest2').getOrCreate()\n",
        "\n",
        "try:\n",
        "  # read in parquet file\n",
        "  # part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n",
        "  incidents_df = spark.read.parquet('weather_tn/weather_tn.parquet')\n",
        "  incidents_df.createOrReplaceTempView(\"weather\")\n",
        "\n",
        "  query=\"\"\"\n",
        "  SELECT MONTH(timestamp_local), DAY(timestamp_local), HOUR(timestamp_local), station_id, precip, temp FROM(\n",
        "    SELECT *, row_number() OVER (\n",
        "      PARTITION BY MONTH(timestamp_local), DAY(timestamp_local), HOUR(timestamp_local)\n",
        "      ORDER BY timestamp_local\n",
        "    ) as rn\n",
        "    FROM weather \n",
        "    WHERE station_id='KBNA') tmp\n",
        "  WHERE rn = 1\n",
        "  \"\"\"\n",
        "\n",
        "  counts = spark.sql(query)\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = counts\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"localtest2.out\")\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass"
      ],
      "metadata": {
        "id": "sCu5g4Byv06o",
        "outputId": "553e23be-0981-454f-f6c4-4ea3f929e24d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting localtest2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 localtest2.py"
      ],
      "metadata": {
        "id": "ZsdCBSGdwqAz",
        "outputId": "6a9d7c72-b3e1-4f4e-a3d4-505415ed9000",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-dfda8513-15a2-4169-816e-b989a053b20a;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 866ms :: artifacts dl 31ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-dfda8513-15a2-4169-816e-b989a053b20a\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/15ms)\n",
            "23/05/01 05:23:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/05/01 05:23:17 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/05/01 05:23:17 INFO ResourceUtils: ==============================================================\n",
            "23/05/01 05:23:17 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/05/01 05:23:17 INFO ResourceUtils: ==============================================================\n",
            "23/05/01 05:23:17 INFO SparkContext: Submitted application: localtest2\n",
            "23/05/01 05:23:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/05/01 05:23:17 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/05/01 05:23:17 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/05/01 05:23:17 INFO SecurityManager: Changing view acls to: root\n",
            "23/05/01 05:23:17 INFO SecurityManager: Changing modify acls to: root\n",
            "23/05/01 05:23:17 INFO SecurityManager: Changing view acls groups to: \n",
            "23/05/01 05:23:17 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/05/01 05:23:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/05/01 05:23:18 INFO Utils: Successfully started service 'sparkDriver' on port 37407.\n",
            "23/05/01 05:23:18 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/05/01 05:23:18 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/05/01 05:23:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/05/01 05:23:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/05/01 05:23:18 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/05/01 05:23:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b810786d-dd81-4c58-8e4c-52df25cc6d0d\n",
            "23/05/01 05:23:18 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/05/01 05:23:18 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/05/01 05:23:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/05/01 05:23:19 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://39049882e5a5:4040\n",
            "23/05/01 05:23:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://39049882e5a5:37407/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://39049882e5a5:37407/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://39049882e5a5:37407/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://39049882e5a5:37407/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://39049882e5a5:37407/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://39049882e5a5:37407/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://39049882e5a5:37407/jars/com.101tec_zkclient-0.3.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://39049882e5a5:37407/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://39049882e5a5:37407/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://39049882e5a5:37407/jars/log4j_log4j-1.2.17.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://39049882e5a5:37407/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://39049882e5a5:37407/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 05:23:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 05:23:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 05:23:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 05:23:19 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 05:23:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 05:23:19 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 05:23:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 05:23:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 05:23:19 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/log4j_log4j-1.2.17.jar\n",
            "23/05/01 05:23:19 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 05:23:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:19 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 05:23:20 INFO Executor: Starting executor ID driver on host 39049882e5a5\n",
            "23/05/01 05:23:20 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:20 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/log4j_log4j-1.2.17.jar\n",
            "23/05/01 05:23:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:20 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 05:23:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:20 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 05:23:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:20 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 05:23:20 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:20 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 05:23:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:20 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 05:23:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:20 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 05:23:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:20 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 05:23:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:20 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 05:23:20 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:20 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 05:23:20 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:20 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 05:23:20 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:20 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 05:23:20 INFO Executor: Fetching spark://39049882e5a5:37407/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:20 INFO TransportClientFactory: Successfully created connection to 39049882e5a5/172.28.0.12:37407 after 177 ms (0 ms spent in bootstraps)\n",
            "23/05/01 05:23:20 INFO Utils: Fetching spark://39049882e5a5:37407/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp6773588940168255312.tmp\n",
            "23/05/01 05:23:21 INFO Utils: /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp6773588940168255312.tmp has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 05:23:21 INFO Executor: Adding file:/tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/05/01 05:23:21 INFO Executor: Fetching spark://39049882e5a5:37407/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:21 INFO Utils: Fetching spark://39049882e5a5:37407/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp1033300833206756620.tmp\n",
            "23/05/01 05:23:21 INFO Utils: /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp1033300833206756620.tmp has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 05:23:21 INFO Executor: Adding file:/tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/05/01 05:23:21 INFO Executor: Fetching spark://39049882e5a5:37407/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:21 INFO Utils: Fetching spark://39049882e5a5:37407/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp7843341820147416758.tmp\n",
            "23/05/01 05:23:21 INFO Utils: /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp7843341820147416758.tmp has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 05:23:21 INFO Executor: Adding file:/tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/05/01 05:23:21 INFO Executor: Fetching spark://39049882e5a5:37407/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:21 INFO Utils: Fetching spark://39049882e5a5:37407/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp4294787326517783278.tmp\n",
            "23/05/01 05:23:21 INFO Utils: /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp4294787326517783278.tmp has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 05:23:21 INFO Executor: Adding file:/tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/05/01 05:23:21 INFO Executor: Fetching spark://39049882e5a5:37407/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:21 INFO Utils: Fetching spark://39049882e5a5:37407/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp2250869153911258708.tmp\n",
            "23/05/01 05:23:21 INFO Utils: /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp2250869153911258708.tmp has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 05:23:21 INFO Executor: Adding file:/tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/05/01 05:23:21 INFO Executor: Fetching spark://39049882e5a5:37407/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:21 INFO Utils: Fetching spark://39049882e5a5:37407/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp1340519600203466868.tmp\n",
            "23/05/01 05:23:21 INFO Utils: /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp1340519600203466868.tmp has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 05:23:21 INFO Executor: Adding file:/tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/05/01 05:23:21 INFO Executor: Fetching spark://39049882e5a5:37407/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:21 INFO Utils: Fetching spark://39049882e5a5:37407/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp6331629475892481752.tmp\n",
            "23/05/01 05:23:21 INFO Utils: /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp6331629475892481752.tmp has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 05:23:21 INFO Executor: Adding file:/tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/05/01 05:23:21 INFO Executor: Fetching spark://39049882e5a5:37407/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:21 INFO Utils: Fetching spark://39049882e5a5:37407/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp4670838944830781785.tmp\n",
            "23/05/01 05:23:21 INFO Utils: /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp4670838944830781785.tmp has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 05:23:21 INFO Executor: Adding file:/tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/05/01 05:23:21 INFO Executor: Fetching spark://39049882e5a5:37407/jars/log4j_log4j-1.2.17.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:21 INFO Utils: Fetching spark://39049882e5a5:37407/jars/log4j_log4j-1.2.17.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp2683635616862474021.tmp\n",
            "23/05/01 05:23:21 INFO Utils: /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp2683635616862474021.tmp has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/log4j_log4j-1.2.17.jar\n",
            "23/05/01 05:23:21 INFO Executor: Adding file:/tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/log4j_log4j-1.2.17.jar to class loader\n",
            "23/05/01 05:23:21 INFO Executor: Fetching spark://39049882e5a5:37407/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:21 INFO Utils: Fetching spark://39049882e5a5:37407/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp2506014526182397583.tmp\n",
            "23/05/01 05:23:21 INFO Utils: /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp2506014526182397583.tmp has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 05:23:21 INFO Executor: Adding file:/tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/05/01 05:23:21 INFO Executor: Fetching spark://39049882e5a5:37407/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:21 INFO Utils: Fetching spark://39049882e5a5:37407/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp3948054798960821166.tmp\n",
            "23/05/01 05:23:21 INFO Utils: /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp3948054798960821166.tmp has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 05:23:21 INFO Executor: Adding file:/tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/05/01 05:23:21 INFO Executor: Fetching spark://39049882e5a5:37407/jars/com.101tec_zkclient-0.3.jar with timestamp 1682918597582\n",
            "23/05/01 05:23:21 INFO Utils: Fetching spark://39049882e5a5:37407/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp6216057029557546118.tmp\n",
            "23/05/01 05:23:21 INFO Utils: /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/fetchFileTemp6216057029557546118.tmp has been previously copied to /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 05:23:21 INFO Executor: Adding file:/tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/userFiles-ebc07ac9-641b-4942-bcae-cfd8878b016d/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/05/01 05:23:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45899.\n",
            "23/05/01 05:23:21 INFO NettyBlockTransferService: Server created on 39049882e5a5:45899\n",
            "23/05/01 05:23:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/05/01 05:23:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 39049882e5a5, 45899, None)\n",
            "23/05/01 05:23:21 INFO BlockManagerMasterEndpoint: Registering block manager 39049882e5a5:45899 with 366.3 MiB RAM, BlockManagerId(driver, 39049882e5a5, 45899, None)\n",
            "23/05/01 05:23:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 39049882e5a5, 45899, None)\n",
            "23/05/01 05:23:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 39049882e5a5, 45899, None)\n",
            "23/05/01 05:23:22 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/05/01 05:23:22 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/05/01 05:23:24 INFO InMemoryFileIndex: It took 197 ms to list leaf files for 1 paths.\n",
            "23/05/01 05:23:25 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 05:23:25 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 05:23:25 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 05:23:25 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/05/01 05:23:25 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 05:23:25 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 05:23:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/05/01 05:23:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/05/01 05:23:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 39049882e5a5:45899 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/05/01 05:23:25 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 05:23:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 05:23:25 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/05/01 05:23:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (39049882e5a5, executor driver, partition 0, PROCESS_LOCAL, 4677 bytes) taskResourceAssignments Map()\n",
            "23/05/01 05:23:26 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/05/01 05:23:27 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3161 bytes result sent to driver\n",
            "23/05/01 05:23:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1581 ms on 39049882e5a5 (executor driver) (1/1)\n",
            "23/05/01 05:23:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/05/01 05:23:27 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.129 s\n",
            "23/05/01 05:23:27 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/05/01 05:23:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/05/01 05:23:27 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.295564 s\n",
            "23/05/01 05:23:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 39049882e5a5:45899 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/05/01 05:23:30 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "23/05/01 05:23:31 INFO DataSourceStrategy: Pruning directories with: \n",
            "23/05/01 05:23:31 INFO FileSourceStrategy: Pushed Filters: IsNotNull(station_id),EqualTo(station_id,KBNA)\n",
            "23/05/01 05:23:31 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(station_id#0),(station_id#0 = KBNA)\n",
            "23/05/01 05:23:31 INFO FileSourceStrategy: Output Data Schema: struct<station_id: string, timestamp_local: timestamp, temp: double, precip: double ... 2 more fields>\n",
            "23/05/01 05:23:32 INFO CodeGenerator: Code generated in 413.129815 ms\n",
            "23/05/01 05:23:32 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 353.0 KiB, free 366.0 MiB)\n",
            "23/05/01 05:23:32 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 365.9 MiB)\n",
            "23/05/01 05:23:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 39049882e5a5:45899 (size: 35.7 KiB, free: 366.3 MiB)\n",
            "23/05/01 05:23:32 INFO SparkContext: Created broadcast 1 from csv at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 05:23:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/05/01 05:23:33 INFO DAGScheduler: Registering RDD 5 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/05/01 05:23:33 INFO DAGScheduler: Got map stage job 1 (csv at NativeMethodAccessorImpl.java:0) with 9 output partitions\n",
            "23/05/01 05:23:33 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 05:23:33 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/05/01 05:23:33 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 05:23:33 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 05:23:33 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 20.2 KiB, free 365.9 MiB)\n",
            "23/05/01 05:23:33 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 365.9 MiB)\n",
            "23/05/01 05:23:33 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 39049882e5a5:45899 (size: 8.5 KiB, free: 366.3 MiB)\n",
            "23/05/01 05:23:33 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 05:23:33 INFO DAGScheduler: Submitting 9 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8))\n",
            "23/05/01 05:23:33 INFO TaskSchedulerImpl: Adding task set 1.0 with 9 tasks resource profile 0\n",
            "23/05/01 05:23:33 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (39049882e5a5, executor driver, partition 0, PROCESS_LOCAL, 7780 bytes) taskResourceAssignments Map()\n",
            "23/05/01 05:23:33 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (39049882e5a5, executor driver, partition 1, PROCESS_LOCAL, 7972 bytes) taskResourceAssignments Map()\n",
            "23/05/01 05:23:33 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/05/01 05:23:33 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
            "23/05/01 05:23:34 INFO CodeGenerator: Code generated in 137.024827 ms\n",
            "23/05/01 05:23:34 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=1/part-00025-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-5557546, partition values: [2016,1]\n",
            "23/05/01 05:23:34 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=7/part-00039-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3951624, partition values: [2014,7]\n",
            "23/05/01 05:23:34 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:34 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:34 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/05/01 05:23:34 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/05/01 05:23:37 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=7/part-00003-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3951093, partition values: [2016,7]\n",
            "23/05/01 05:23:37 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:37 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=5/part-00010-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-4205877, partition values: [2014,5]\n",
            "23/05/01 05:23:37 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:37 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=8/part-00009-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3945089, partition values: [2018,8]\n",
            "23/05/01 05:23:37 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:37 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=2/part-00010-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-4128191, partition values: [2016,2]\n",
            "23/05/01 05:23:37 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:38 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=7/part-00015-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3929948, partition values: [2012,7]\n",
            "23/05/01 05:23:38 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:38 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=7/part-00023-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-4058620, partition values: [2011,7]\n",
            "23/05/01 05:23:38 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:38 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=5/part-00034-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3928208, partition values: [2012,5]\n",
            "23/05/01 05:23:38 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:38 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=7/part-00031-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-4028984, partition values: [2018,7]\n",
            "23/05/01 05:23:38 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:38 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=5/part-00023-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3928147, partition values: [2013,5]\n",
            "23/05/01 05:23:38 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:38 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=5/part-00023-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3924804, partition values: [2011,5]\n",
            "23/05/01 05:23:38 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:38 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=5/part-00018-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-4025263, partition values: [2018,5]\n",
            "23/05/01 05:23:38 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:38 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=5/part-00002-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-4007338, partition values: [2019,5]\n",
            "23/05/01 05:23:38 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=8/part-00029-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3923168, partition values: [2020,8]\n",
            "23/05/01 05:23:38 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:38 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:38 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=7/part-00037-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-4006988, partition values: [2019,7]\n",
            "23/05/01 05:23:38 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:38 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=6/part-00011-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3921171, partition values: [2019,6]\n",
            "23/05/01 05:23:38 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:38 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=5/part-00016-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-4004210, partition values: [2010,5]\n",
            "23/05/01 05:23:38 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:38 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=8/part-00035-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3917866, partition values: [2016,8]\n",
            "23/05/01 05:23:38 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:38 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=7/part-00031-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3996313, partition values: [2020,7]\n",
            "23/05/01 05:23:38 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:38 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=7/part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3915811, partition values: [2015,7]\n",
            "23/05/01 05:23:38 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:38 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=5/part-00038-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3988631, partition values: [2020,5]\n",
            "23/05/01 05:23:38 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:38 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=8/part-00010-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3915434, partition values: [2019,8]\n",
            "23/05/01 05:23:38 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:39 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=5/part-00036-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3979045, partition values: [2017,5]\n",
            "23/05/01 05:23:39 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:39 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=6/part-00002-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3897004, partition values: [2020,6]\n",
            "23/05/01 05:23:39 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:39 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=7/part-00000-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3975611, partition values: [2010,7]\n",
            "23/05/01 05:23:39 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:39 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=8/part-00001-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3894424, partition values: [2017,8]\n",
            "23/05/01 05:23:39 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:39 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=7/part-00011-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3961937, partition values: [2017,7]\n",
            "23/05/01 05:23:39 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:39 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=5/part-00008-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3955624, partition values: [2016,5]\n",
            "23/05/01 05:23:39 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:39 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=8/part-00005-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3879981, partition values: [2012,8]\n",
            "23/05/01 05:23:39 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:39 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=7/part-00020-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3954022, partition values: [2013,7]\n",
            "23/05/01 05:23:39 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:39 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=8/part-00024-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3876431, partition values: [2010,8]\n",
            "23/05/01 05:23:39 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:39 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=5/part-00037-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3875299, partition values: [2015,5]\n",
            "23/05/01 05:23:39 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:39 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2327 bytes result sent to driver\n",
            "23/05/01 05:23:39 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (39049882e5a5, executor driver, partition 2, PROCESS_LOCAL, 7977 bytes) taskResourceAssignments Map()\n",
            "23/05/01 05:23:39 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)\n",
            "23/05/01 05:23:39 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 6101 ms on 39049882e5a5 (executor driver) (1/9)\n",
            "23/05/01 05:23:39 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=6/part-00034-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3874146, partition values: [2018,6]\n",
            "23/05/01 05:23:39 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:39 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2284 bytes result sent to driver\n",
            "23/05/01 05:23:39 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (39049882e5a5, executor driver, partition 3, PROCESS_LOCAL, 7983 bytes) taskResourceAssignments Map()\n",
            "23/05/01 05:23:39 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)\n",
            "23/05/01 05:23:39 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 6244 ms on 39049882e5a5 (executor driver) (2/9)\n",
            "23/05/01 05:23:39 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=3/part-00033-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3795570, partition values: [2012,3]\n",
            "23/05/01 05:23:39 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:39 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=6/part-00022-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3856025, partition values: [2016,6]\n",
            "23/05/01 05:23:39 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:39 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=6/part-00032-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3794690, partition values: [2015,6]\n",
            "23/05/01 05:23:39 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=8/part-00014-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3855225, partition values: [2014,8]\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=3/part-00001-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3793866, partition values: [2017,3]\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=6/part-00038-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3852960, partition values: [2017,6]\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=8/part-00035-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3852867, partition values: [2013,8]\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=4/part-00033-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3792147, partition values: [2010,4]\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=8/part-00019-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3851227, partition values: [2011,8]\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=3/part-00007-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3788827, partition values: [2018,3]\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=6/part-00015-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3842507, partition values: [2010,6]\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=3/part-00005-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3787406, partition values: [2019,3]\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=6/part-00026-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3834361, partition values: [2014,6]\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=4/part-00020-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3785899, partition values: [2020,4]\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=8/part-00016-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3832453, partition values: [2015,8]\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=3/part-00007-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3779093, partition values: [2020,3]\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=6/part-00014-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3829631, partition values: [2013,6]\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=7/part-00019-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3824728, partition values: [2021,7]\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=4/part-00025-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3777885, partition values: [2016,4]\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=4/part-00017-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3764094, partition values: [2011,4]\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=4/part-00026-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3824033, partition values: [2017,4]\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=4/part-00015-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3820655, partition values: [2015,4]\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=10/part-00001-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3759905, partition values: [2019,10]\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=6/part-00026-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3816534, partition values: [2011,6]\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=3/part-00040-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3758371, partition values: [2016,3]\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=6/part-00038-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3816141, partition values: [2012,6]\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=4/part-00026-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3802262, partition values: [2019,4]\n",
            "23/05/01 05:23:40 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=8/part-00003-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3757784, partition values: [2021,8]\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:40 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:41 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=3/part-00014-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3801190, partition values: [2013,3]\n",
            "23/05/01 05:23:41 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=4/part-00001-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3755059, partition values: [2014,4]\n",
            "23/05/01 05:23:41 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:41 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=4/part-00040-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3752250, partition values: [2018,4]\n",
            "23/05/01 05:23:41 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:41 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:41 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=4/part-00021-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3745610, partition values: [2013,4]\n",
            "23/05/01 05:23:41 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:41 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=5/part-00005-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3745255, partition values: [2021,5]\n",
            "23/05/01 05:23:41 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:41 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2284 bytes result sent to driver\n",
            "23/05/01 05:23:41 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (39049882e5a5, executor driver, partition 4, PROCESS_LOCAL, 7983 bytes) taskResourceAssignments Map()\n",
            "23/05/01 05:23:41 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)\n",
            "23/05/01 05:23:41 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 1715 ms on 39049882e5a5 (executor driver) (3/9)\n",
            "23/05/01 05:23:41 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=9/part-00008-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3743670, partition values: [2020,9]\n",
            "23/05/01 05:23:41 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:41 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=9/part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3742820, partition values: [2018,9]\n",
            "23/05/01 05:23:41 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:41 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2284 bytes result sent to driver\n",
            "23/05/01 05:23:41 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (39049882e5a5, executor driver, partition 5, PROCESS_LOCAL, 7978 bytes) taskResourceAssignments Map()\n",
            "23/05/01 05:23:41 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 1809 ms on 39049882e5a5 (executor driver) (4/9)\n",
            "23/05/01 05:23:41 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)\n",
            "23/05/01 05:23:41 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=4/part-00006-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3736359, partition values: [2012,4]\n",
            "23/05/01 05:23:41 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:41 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=10/part-00000-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3680270, partition values: [2012,10]\n",
            "23/05/01 05:23:41 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:41 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=9/part-00016-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3730733, partition values: [2016,9]\n",
            "23/05/01 05:23:41 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:41 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=10/part-00002-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3728411, partition values: [2017,10]\n",
            "23/05/01 05:23:41 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=9/part-00017-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3675914, partition values: [2010,9]\n",
            "23/05/01 05:23:41 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:41 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:41 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=3/part-00028-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3728405, partition values: [2010,3]\n",
            "23/05/01 05:23:41 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:41 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=10/part-00034-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3673428, partition values: [2011,10]\n",
            "23/05/01 05:23:41 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:41 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=10/part-00009-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3723627, partition values: [2013,10]\n",
            "23/05/01 05:23:41 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:41 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=10/part-00002-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3719786, partition values: [2018,10]\n",
            "23/05/01 05:23:41 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:41 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=9/part-00039-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3673335, partition values: [2011,9]\n",
            "23/05/01 05:23:41 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=3/part-00021-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3714074, partition values: [2014,3]\n",
            "23/05/01 05:23:41 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:42 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:42 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=9/part-00015-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3667357, partition values: [2012,9]\n",
            "23/05/01 05:23:42 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=10/part-00032-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3713927, partition values: [2020,10]\n",
            "23/05/01 05:23:42 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:42 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:42 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=9/part-00009-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3666989, partition values: [2013,9]\n",
            "23/05/01 05:23:42 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=9/part-00021-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3712616, partition values: [2017,9]\n",
            "23/05/01 05:23:42 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:42 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:42 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=9/part-00020-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3660994, partition values: [2014,9]\n",
            "23/05/01 05:23:42 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=10/part-00019-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3706093, partition values: [2016,10]\n",
            "23/05/01 05:23:42 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:42 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:42 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=10/part-00027-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3638910, partition values: [2010,10]\n",
            "23/05/01 05:23:42 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:42 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=3/part-00027-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3701137, partition values: [2015,3]\n",
            "23/05/01 05:23:42 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:42 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=9/part-00019-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3621904, partition values: [2015,9]\n",
            "23/05/01 05:23:42 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:42 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=9/part-00029-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3700362, partition values: [2019,9]\n",
            "23/05/01 05:23:42 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=10/part-00008-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3699777, partition values: [2014,10]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=10/part-00021-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3610882, partition values: [2015,10]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=6/part-00004-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3694819, partition values: [2021,6]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=4/part-00039-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3593468, partition values: [2021,4]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=3/part-00013-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3682940, partition values: [2011,3]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=1/part-00039-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3584493, partition values: [2017,1]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2284 bytes result sent to driver\n",
            "23/05/01 05:23:43 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (39049882e5a5, executor driver, partition 6, PROCESS_LOCAL, 7981 bytes) taskResourceAssignments Map()\n",
            "23/05/01 05:23:43 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)\n",
            "23/05/01 05:23:43 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 1921 ms on 39049882e5a5 (executor driver) (5/9)\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=12/part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3542917, partition values: [2019,12]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=12/part-00000-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3583973, partition values: [2010,12]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=1/part-00038-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3578989, partition values: [2010,1]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=10/part-00006-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3558470, partition values: [2021,10]\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=9/part-00012-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3538397, partition values: [2021,9]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=1/part-00007-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3552780, partition values: [2018,1]\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=1/part-00029-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3538017, partition values: [2012,1]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=3/part-00022-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3537995, partition values: [2021,3]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=1/part-00036-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3550882, partition values: [2011,1]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=12/part-00033-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3533037, partition values: [2018,12]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=1/part-00025-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3531889, partition values: [2020,1]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2327 bytes result sent to driver\n",
            "23/05/01 05:23:43 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (39049882e5a5, executor driver, partition 7, PROCESS_LOCAL, 8167 bytes) taskResourceAssignments Map()\n",
            "23/05/01 05:23:43 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)\n",
            "23/05/01 05:23:43 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 2192 ms on 39049882e5a5 (executor driver) (6/9)\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=11/part-00013-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3453230, partition values: [2010,11]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=1/part-00014-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3528153, partition values: [2019,1]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=2/part-00032-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3431993, partition values: [2020,2]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=12/part-00017-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3527889, partition values: [2020,12]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=12/part-00012-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3431383, partition values: [2013,12]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=11/part-00033-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3430659, partition values: [2018,11]\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=1/part-00034-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3521996, partition values: [2013,1]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:43 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2020/month=11/part-00006-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3424084, partition values: [2020,11]\n",
            "23/05/01 05:23:43 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=11/part-00004-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3419463, partition values: [2016,11]\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=12/part-00006-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3519059, partition values: [2012,12]\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=11/part-00035-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3411280, partition values: [2017,11]\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2022/month=1/part-00029-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3411037, partition values: [2022,1]\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=1/part-00012-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3515591, partition values: [2014,1]\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=11/part-00028-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3403323, partition values: [2011,11]\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=11/part-00037-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3502944, partition values: [2019,11]\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=12/part-00003-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3500110, partition values: [2015,12]\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=11/part-00032-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3401421, partition values: [2014,11]\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=12/part-00008-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3394472, partition values: [2014,12]\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2016/month=12/part-00024-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3491506, partition values: [2016,12]\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=11/part-00025-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3392997, partition values: [2012,11]\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=12/part-00020-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3468416, partition values: [2017,12]\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=11/part-00018-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3372381, partition values: [2015,11]\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=12/part-00018-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3468292, partition values: [2011,12]\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=1/part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3369951, partition values: [2021,1]\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=1/part-00036-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3457643, partition values: [2015,1]\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2012/month=2/part-00031-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3368054, partition values: [2012,2]\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=11/part-00022-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3366832, partition values: [2013,11]\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2284 bytes result sent to driver\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=12/part-00018-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3357932, partition values: [2021,12]\n",
            "23/05/01 05:23:44 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9) (39049882e5a5, executor driver, partition 8, PROCESS_LOCAL, 6654 bytes) taskResourceAssignments Map()\n",
            "23/05/01 05:23:44 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 1607 ms on 39049882e5a5 (executor driver) (7/9)\n",
            "23/05/01 05:23:44 INFO Executor: Running task 8.0 in stage 1.0 (TID 9)\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2019/month=2/part-00016-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3334098, partition values: [2019,2]\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2018/month=2/part-00037-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3345692, partition values: [2018,2]\n",
            "23/05/01 05:23:44 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:44 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2017/month=2/part-00035-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3319158, partition values: [2017,2]\n",
            "23/05/01 05:23:45 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:45 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2010/month=2/part-00028-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3282783, partition values: [2010,2]\n",
            "23/05/01 05:23:45 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:45 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2284 bytes result sent to driver\n",
            "23/05/01 05:23:45 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 1349 ms on 39049882e5a5 (executor driver) (8/9)\n",
            "23/05/01 05:23:45 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2011/month=2/part-00017-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3263332, partition values: [2011,2]\n",
            "23/05/01 05:23:45 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:45 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2014/month=2/part-00024-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3255844, partition values: [2014,2]\n",
            "23/05/01 05:23:45 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:45 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2013/month=2/part-00005-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3238792, partition values: [2013,2]\n",
            "23/05/01 05:23:45 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:45 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2015/month=2/part-00000-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3233715, partition values: [2015,2]\n",
            "23/05/01 05:23:45 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:45 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=11/part-00010-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3181347, partition values: [2021,11]\n",
            "23/05/01 05:23:45 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:45 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2021/month=2/part-00023-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-3104775, partition values: [2021,2]\n",
            "23/05/01 05:23:45 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:45 INFO FileScanRDD: Reading File path: file:///content/weather_tn/weather_tn.parquet/year=2022/month=2/part-00009-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet, range: 0-1166956, partition values: [2022,2]\n",
            "23/05/01 05:23:45 INFO FilterCompat: Filtering using predicate: and(noteq(station_id, null), eq(station_id, Binary{\"KBNA\"}))\n",
            "23/05/01 05:23:45 INFO Executor: Finished task 8.0 in stage 1.0 (TID 9). 2284 bytes result sent to driver\n",
            "23/05/01 05:23:45 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 861 ms on 39049882e5a5 (executor driver) (9/9)\n",
            "23/05/01 05:23:45 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/05/01 05:23:45 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 12.412 s\n",
            "23/05/01 05:23:45 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/05/01 05:23:45 INFO DAGScheduler: running: Set()\n",
            "23/05/01 05:23:45 INFO DAGScheduler: waiting: Set()\n",
            "23/05/01 05:23:45 INFO DAGScheduler: failed: Set()\n",
            "23/05/01 05:23:45 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1185141, minimum partition size: 1048576\n",
            "23/05/01 05:23:46 INFO CodeGenerator: Code generated in 112.665973 ms\n",
            "23/05/01 05:23:46 INFO CodeGenerator: Code generated in 75.129591 ms\n",
            "23/05/01 05:23:46 INFO DAGScheduler: Registering RDD 10 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/05/01 05:23:46 INFO DAGScheduler: Got map stage job 2 (csv at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
            "23/05/01 05:23:46 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 05:23:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/05/01 05:23:46 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 05:23:46 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 05:23:46 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 39.0 KiB, free 365.9 MiB)\n",
            "23/05/01 05:23:46 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 18.1 KiB, free 365.8 MiB)\n",
            "23/05/01 05:23:46 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 39049882e5a5:45899 (size: 18.1 KiB, free: 366.2 MiB)\n",
            "23/05/01 05:23:46 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 05:23:46 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/05/01 05:23:46 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0\n",
            "23/05/01 05:23:46 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 10) (39049882e5a5, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/05/01 05:23:46 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 11) (39049882e5a5, executor driver, partition 1, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/05/01 05:23:46 INFO Executor: Running task 0.0 in stage 3.0 (TID 10)\n",
            "23/05/01 05:23:46 INFO Executor: Running task 1.0 in stage 3.0 (TID 11)\n",
            "23/05/01 05:23:47 INFO ShuffleBlockFetcherIterator: Getting 9 (1153.8 KiB) non-empty blocks including 9 (1153.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/05/01 05:23:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 61 ms\n",
            "23/05/01 05:23:47 INFO ShuffleBlockFetcherIterator: Getting 9 (1160.9 KiB) non-empty blocks including 9 (1160.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/05/01 05:23:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 63 ms\n",
            "23/05/01 05:23:47 INFO CodeGenerator: Code generated in 51.391814 ms\n",
            "23/05/01 05:23:47 INFO CodeGenerator: Code generated in 35.930707 ms\n",
            "23/05/01 05:23:47 INFO CodeGenerator: Code generated in 52.394543 ms\n",
            "23/05/01 05:23:47 INFO CodeGenerator: Code generated in 70.375628 ms\n",
            "23/05/01 05:23:48 INFO CodeGenerator: Code generated in 38.815319 ms\n",
            "23/05/01 05:23:48 INFO CodeGenerator: Code generated in 31.002385 ms\n",
            "23/05/01 05:23:48 INFO CodeGenerator: Code generated in 34.625067 ms\n",
            "23/05/01 05:23:48 INFO Executor: Finished task 0.0 in stage 3.0 (TID 10). 3925 bytes result sent to driver\n",
            "23/05/01 05:23:48 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 10) in 1809 ms on 39049882e5a5 (executor driver) (1/2)\n",
            "23/05/01 05:23:48 INFO Executor: Finished task 1.0 in stage 3.0 (TID 11). 3925 bytes result sent to driver\n",
            "23/05/01 05:23:48 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 11) in 1828 ms on 39049882e5a5 (executor driver) (2/2)\n",
            "23/05/01 05:23:48 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/05/01 05:23:48 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 1.890 s\n",
            "23/05/01 05:23:48 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/05/01 05:23:48 INFO DAGScheduler: running: Set()\n",
            "23/05/01 05:23:48 INFO DAGScheduler: waiting: Set()\n",
            "23/05/01 05:23:48 INFO DAGScheduler: failed: Set()\n",
            "23/05/01 05:23:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/05/01 05:23:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/05/01 05:23:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/05/01 05:23:48 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 05:23:48 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 05:23:48 INFO DAGScheduler: Final stage: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 05:23:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
            "23/05/01 05:23:48 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 05:23:48 INFO DAGScheduler: Submitting ResultStage 6 (ShuffledRowRDD[11] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 05:23:48 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 212.0 KiB, free 365.6 MiB)\n",
            "23/05/01 05:23:49 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 75.8 KiB, free 365.6 MiB)\n",
            "23/05/01 05:23:49 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 39049882e5a5:45899 (size: 75.8 KiB, free: 366.2 MiB)\n",
            "23/05/01 05:23:49 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 05:23:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (ShuffledRowRDD[11] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 05:23:49 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "23/05/01 05:23:49 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 12) (39049882e5a5, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/05/01 05:23:49 INFO Executor: Running task 0.0 in stage 6.0 (TID 12)\n",
            "23/05/01 05:23:49 INFO ShuffleBlockFetcherIterator: Getting 2 (118.1 KiB) non-empty blocks including 2 (118.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/05/01 05:23:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\n",
            "23/05/01 05:23:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/05/01 05:23:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/05/01 05:23:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/05/01 05:23:49 INFO FileOutputCommitter: Saved output of task 'attempt_202305010523485387824200966698756_0006_m_000000_12' to file:/content/localtest2.out/_temporary/0/task_202305010523485387824200966698756_0006_m_000000\n",
            "23/05/01 05:23:49 INFO SparkHadoopMapRedUtil: attempt_202305010523485387824200966698756_0006_m_000000_12: Committed\n",
            "23/05/01 05:23:49 INFO Executor: Finished task 0.0 in stage 6.0 (TID 12). 3483 bytes result sent to driver\n",
            "23/05/01 05:23:49 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 12) in 584 ms on 39049882e5a5 (executor driver) (1/1)\n",
            "23/05/01 05:23:49 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "23/05/01 05:23:49 INFO DAGScheduler: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0) finished in 0.676 s\n",
            "23/05/01 05:23:49 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/05/01 05:23:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "23/05/01 05:23:49 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 0.687010 s\n",
            "23/05/01 05:23:49 INFO FileFormatWriter: Start to commit write Job 1c298fa2-b79f-4080-9b63-939d50f09bee.\n",
            "23/05/01 05:23:49 INFO FileFormatWriter: Write Job 1c298fa2-b79f-4080-9b63-939d50f09bee committed. Elapsed time: 31 ms.\n",
            "23/05/01 05:23:49 INFO FileFormatWriter: Finished processing stats for write job 1c298fa2-b79f-4080-9b63-939d50f09bee.\n",
            "23/05/01 05:23:49 INFO SparkUI: Stopped Spark web UI at http://39049882e5a5:4040\n",
            "23/05/01 05:23:49 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/05/01 05:23:49 INFO MemoryStore: MemoryStore cleared\n",
            "23/05/01 05:23:49 INFO BlockManager: BlockManager stopped\n",
            "23/05/01 05:23:49 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/05/01 05:23:49 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/05/01 05:23:49 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/05/01 05:23:50 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/05/01 05:23:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf/pyspark-0805ab0e-718a-40d5-b3d6-05de3b7f7b13\n",
            "23/05/01 05:23:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-ace332b3-0271-4afc-91bb-d8d66c4208ef\n",
            "23/05/01 05:23:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-8b94a4da-c8fd-4faa-8ec9-2066cb1caedf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rename_file()\n",
        "with open('localtest2.out/results.csv') as f:\n",
        "  lines = f.readlines()\n",
        "  weather_df = pd.read_csv('localtest2.out/results.csv', header=None, names=['month', 'day', 'hour', 'station', 'precip', 'temp'])\n",
        "weather_df"
      ],
      "metadata": {
        "id": "rXzd5kY3wlPB",
        "outputId": "5a0350c7-fcc6-4969-ef57-34b188f8bcf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renamed part-00000-12c705d1-f2a8-417f-9b19-ae5495976816-c000.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      month  day  hour station  precip  temp\n",
              "0         1    1     0    KBNA     0.0   3.3\n",
              "1         1    1     1    KBNA     0.0   2.8\n",
              "2         1    1     2    KBNA     0.0   1.7\n",
              "3         1    1     3    KBNA     0.0  -1.0\n",
              "4         1    1     8    KBNA     0.0  -5.6\n",
              "...     ...  ...   ...     ...     ...   ...\n",
              "8779     12   31    13    KBNA     0.0  18.3\n",
              "8780     12   31    16    KBNA     0.0  18.9\n",
              "8781     12   31    17    KBNA     0.0  17.2\n",
              "8782     12   31    21    KBNA     0.5  17.2\n",
              "8783     12   31    22    KBNA     0.5  15.6\n",
              "\n",
              "[8784 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-10a8e0d2-df64-420d-8042-e8b789f232dd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "      <th>station</th>\n",
              "      <th>precip</th>\n",
              "      <th>temp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8779</th>\n",
              "      <td>12</td>\n",
              "      <td>31</td>\n",
              "      <td>13</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8780</th>\n",
              "      <td>12</td>\n",
              "      <td>31</td>\n",
              "      <td>16</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8781</th>\n",
              "      <td>12</td>\n",
              "      <td>31</td>\n",
              "      <td>17</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8782</th>\n",
              "      <td>12</td>\n",
              "      <td>31</td>\n",
              "      <td>21</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.5</td>\n",
              "      <td>17.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8783</th>\n",
              "      <td>12</td>\n",
              "      <td>31</td>\n",
              "      <td>22</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.5</td>\n",
              "      <td>15.6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8784 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-10a8e0d2-df64-420d-8042-e8b789f232dd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-10a8e0d2-df64-420d-8042-e8b789f232dd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-10a8e0d2-df64-420d-8042-e8b789f232dd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nonzero_precip_all = weather_df['precip'].loc[~(weather_df['precip']==0)]\n",
        "len(nonzero_precip_all)"
      ],
      "metadata": {
        "id": "jTI-RwgLxKGw",
        "outputId": "03862cc2-ad0b-4c52-e754-dea34b8e9dff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "723"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The percent of time that there is precipitation\n",
        "print(\"General % of time there is precipitation: {:.2f}%\".format(len(nonzero_precip_all) * 100 / len(weather_df)))"
      ],
      "metadata": {
        "id": "r6hADnMuxYZ_",
        "outputId": "23b49ce9-d294-403f-8434-ae650236d991",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "General % of time there is precipitation: 8.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's see if we can get EMR to work"
      ],
      "metadata": {
        "id": "eQNBral-6SEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file emr-test.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('emr-test').getOrCreate()\n",
        "\n",
        "try:\n",
        "  weather_df = spark.read.parquet('s3://cs4266-finalproject/weather_tn.parquet')\n",
        "  weather_df.createOrReplaceTempView(\"weather\")\n",
        "\n",
        "  incidents_df = spark.read.parquet('s3://cs4266-finalproject/nfd_incidents_xd_seg.parquet')\n",
        "  incidents_df.createOrReplaceTempView(\"incidents\")\n",
        "\n",
        "  incidents_query=\"\"\"\n",
        "  (SELECT time_local, Incident_ID, response_time_sec, latitude, longitude FROM(\n",
        "    SELECT *, row_number() OVER (\n",
        "      PARTITION BY YEAR(time_local), MONTH(time_local), DAY(time_local), HOUR(time_local)\n",
        "      ORDER BY time_local\n",
        "    ) as rn\n",
        "    FROM incidents\n",
        "    WHERE YEAR(time_local) >= 2017 AND YEAR(time_local) <= 2021) tmp\n",
        "  WHERE rn = 1) as i\n",
        "  \"\"\"\n",
        "\n",
        "  weather_query=\"\"\"\n",
        "  (SELECT tmp.timestamp_local, tmp.station_id, tmp.precip, tmp.temp FROM(\n",
        "    SELECT *, row_number() OVER (\n",
        "      PARTITION BY YEAR(timestamp_local), MONTH(timestamp_local), DAY(timestamp_local), HOUR(timestamp_local)\n",
        "      ORDER BY timestamp_local\n",
        "    ) as rn\n",
        "    FROM weather \n",
        "    WHERE station_id='KBNA') tmp\n",
        "  WHERE rn = 1) as w\n",
        "  \"\"\"\n",
        "\n",
        "  full_query = \"SELECT * FROM (\" + incidents_query + \") LEFT JOIN (\" + weather_query + \") ON YEAR(i.time_local) = YEAR(w.timestamp_local) AND MONTH(i.time_local) = MONTH(w.timestamp_local) AND DAY(i.time_local) = DAY(w.timestamp_local) AND HOUR(i.time_local) = HOUR(w.timestamp_local)\"\n",
        "\n",
        "  counts = spark.sql(full_query)\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = counts\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"s3://cs4266-finalproject-spark/emr-logs/emr-test.out\")\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass"
      ],
      "metadata": {
        "id": "Qavvx-Z86oGQ",
        "outputId": "44a0915e-9032-40a7-fda9-ae1b8bbaca60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting emr-test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# replae with your EMR cluster ID\n",
        "CLUSTER_ID = 'j-2VO4MYLJDTO1N'\n",
        "\n",
        "def submit_job(app_name, pyfile_uri):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ],
      "metadata": {
        "id": "sgIi_x0S5zfu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s3.upload_file(Filename='emr-test.py', Bucket='cs4266-finalproject-spark', Key='emr-logs/emr-test.py')"
      ],
      "metadata": {
        "id": "k8Gd0cob5zj9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_job(app_name='emr-test', pyfile_uri='s3://cs4266-finalproject-spark/emr-logs/emr-test.py')"
      ],
      "metadata": {
        "id": "IRc-k6F47XUr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_key = \"emr-logs/emr-test.out/part-00000-3bea3377-8c7b-48d6-9eea-ab47758db901-c000.csv\"\n",
        "lines = s3.get_object(Bucket='cs4266-finalproject-spark', Key=output_key)['Body'].read().decode().splitlines()\n",
        "s3.download_file('cs4266-finalproject-spark', output_key, 'emr-join.csv')"
      ],
      "metadata": {
        "id": "_Ji9_2NO8IiU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "sFf5LeVS_c33"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emr_df = pd.read_csv('emr-join.csv', header=None, names=['time_local', 'Incident_ID', 'resp_time', 'latitude', 'longitude', 'ts_loc', 'station_id', 'precip', 'temp'])\n",
        "emr_df"
      ],
      "metadata": {
        "id": "2ZD4lJhR_QKS",
        "outputId": "3c476509-8ab7-4040-e392-85459f8f087b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     time_local  Incident_ID  resp_time   latitude  longitude  \\\n",
              "0      2017-01-07T11:10:30.000Z          203      567.0  36.054320 -86.988325   \n",
              "1      2017-01-10T16:18:04.000Z          263      227.0  36.181699 -86.796756   \n",
              "2      2017-04-01T11:11:17.000Z         2139      429.0  36.235918 -86.781342   \n",
              "3      2018-11-22T21:09:02.570Z         7406      292.0  36.052902 -86.685688   \n",
              "4      2018-12-18T07:16:29.370Z         7937      290.0  36.044399 -86.647056   \n",
              "...                         ...          ...        ...        ...        ...   \n",
              "15854  2020-06-30T16:14:49.093Z        23746        NaN  36.106218 -86.673067   \n",
              "15855  2020-07-30T21:43:16.510Z        24508      602.0  36.272214 -86.914536   \n",
              "15856  2020-10-03T15:20:31.247Z        26166       -1.0  36.323529 -86.701087   \n",
              "15857  2021-02-13T19:19:29.613Z        29441      193.0  36.262998 -86.680948   \n",
              "15858  2021-02-27T18:11:24.320Z        29725      251.0  36.186880 -86.608581   \n",
              "\n",
              "                         ts_loc station_id  precip  temp  \n",
              "0      2017-01-07T11:00:00.000Z       KBNA     0.0  -7.8  \n",
              "1      2017-01-10T16:00:00.000Z       KBNA     1.0  13.9  \n",
              "2      2017-04-01T11:00:00.000Z       KBNA     0.0  10.0  \n",
              "3      2018-11-22T21:00:00.000Z       KBNA     0.0   6.1  \n",
              "4      2018-12-18T07:00:00.000Z       KBNA     0.0  -0.6  \n",
              "...                         ...        ...     ...   ...  \n",
              "15854  2020-06-30T16:00:00.000Z       KBNA     0.0  26.7  \n",
              "15855  2020-07-30T21:00:00.000Z       KBNA     0.0  26.7  \n",
              "15856  2020-10-03T15:00:00.000Z       KBNA     0.0  21.1  \n",
              "15857  2021-02-13T19:00:00.000Z       KBNA     0.0  -3.3  \n",
              "15858  2021-02-27T18:00:00.000Z       KBNA     0.5  13.9  \n",
              "\n",
              "[15859 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1d612b28-5b06-4f4a-8f14-851deafad154\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time_local</th>\n",
              "      <th>Incident_ID</th>\n",
              "      <th>resp_time</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>ts_loc</th>\n",
              "      <th>station_id</th>\n",
              "      <th>precip</th>\n",
              "      <th>temp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017-01-07T11:10:30.000Z</td>\n",
              "      <td>203</td>\n",
              "      <td>567.0</td>\n",
              "      <td>36.054320</td>\n",
              "      <td>-86.988325</td>\n",
              "      <td>2017-01-07T11:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-7.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017-01-10T16:18:04.000Z</td>\n",
              "      <td>263</td>\n",
              "      <td>227.0</td>\n",
              "      <td>36.181699</td>\n",
              "      <td>-86.796756</td>\n",
              "      <td>2017-01-10T16:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>1.0</td>\n",
              "      <td>13.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017-04-01T11:11:17.000Z</td>\n",
              "      <td>2139</td>\n",
              "      <td>429.0</td>\n",
              "      <td>36.235918</td>\n",
              "      <td>-86.781342</td>\n",
              "      <td>2017-04-01T11:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-11-22T21:09:02.570Z</td>\n",
              "      <td>7406</td>\n",
              "      <td>292.0</td>\n",
              "      <td>36.052902</td>\n",
              "      <td>-86.685688</td>\n",
              "      <td>2018-11-22T21:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-12-18T07:16:29.370Z</td>\n",
              "      <td>7937</td>\n",
              "      <td>290.0</td>\n",
              "      <td>36.044399</td>\n",
              "      <td>-86.647056</td>\n",
              "      <td>2018-12-18T07:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15854</th>\n",
              "      <td>2020-06-30T16:14:49.093Z</td>\n",
              "      <td>23746</td>\n",
              "      <td>NaN</td>\n",
              "      <td>36.106218</td>\n",
              "      <td>-86.673067</td>\n",
              "      <td>2020-06-30T16:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15855</th>\n",
              "      <td>2020-07-30T21:43:16.510Z</td>\n",
              "      <td>24508</td>\n",
              "      <td>602.0</td>\n",
              "      <td>36.272214</td>\n",
              "      <td>-86.914536</td>\n",
              "      <td>2020-07-30T21:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15856</th>\n",
              "      <td>2020-10-03T15:20:31.247Z</td>\n",
              "      <td>26166</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>36.323529</td>\n",
              "      <td>-86.701087</td>\n",
              "      <td>2020-10-03T15:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15857</th>\n",
              "      <td>2021-02-13T19:19:29.613Z</td>\n",
              "      <td>29441</td>\n",
              "      <td>193.0</td>\n",
              "      <td>36.262998</td>\n",
              "      <td>-86.680948</td>\n",
              "      <td>2021-02-13T19:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-3.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15858</th>\n",
              "      <td>2021-02-27T18:11:24.320Z</td>\n",
              "      <td>29725</td>\n",
              "      <td>251.0</td>\n",
              "      <td>36.186880</td>\n",
              "      <td>-86.608581</td>\n",
              "      <td>2021-02-27T18:00:00.000Z</td>\n",
              "      <td>KBNA</td>\n",
              "      <td>0.5</td>\n",
              "      <td>13.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15859 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d612b28-5b06-4f4a-8f14-851deafad154')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1d612b28-5b06-4f4a-8f14-851deafad154 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1d612b28-5b06-4f4a-8f14-851deafad154');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}