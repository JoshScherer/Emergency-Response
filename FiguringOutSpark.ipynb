{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP671sM4kTsMv1gpl3mcWKW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshScherer/Emergency-Response/blob/josh-dev/FiguringOutSpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCEi2D06C0u_"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install spark. we are using the one that uses hadoop as the underlying scheduler.\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!tar xf  spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "\n",
        "#Provides findspark.init() to make pyspark importable as a regular library.\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.2.4-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 pyspark-shell'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBCqGm0GC8y0",
        "outputId": "6d117e04-3d02-45be-cb76-c9e7371d8445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 294140\n",
            "drwxr-xr-x  1 root root      4096 Apr 27 13:35 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Apr  9 21:17 spark-3.2.4-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 301183180 Apr  9 21:35 spark-3.2.4-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip nashville-tweets-2019-01-28.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG1JNMZ3ESBM",
        "outputId": "db57f666-9f22-49e3-e944-ee54f177b2d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  nashville-tweets-2019-01-28.zip\n",
            "  inflating: nashville-tweets-2019-01-28  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -3 nashville-tweets-2019-01-28.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg0WLNbMDVDG",
        "outputId": "8e8420b4-b683-4342-a8e9-d9d14876a5c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PK\u0003\u0004\u0014\u0000\b\u0000\b\u0000\u000bYyR\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000�\u0011~\u0001\u001b\u0000 \u0000nashville-tweets-2019-01-28UT\r\u0000\u0007v�\\`u�\\`��a`ux\u000b\u0000\u0001\u0004�\u0001\u0000\u0000\u0004\u0014\u0000\u0000\u0000�iw�8�.���\u0015HrW�\u0019\u001c�\u0000A\u0002��};�<&�㊝�N��EI�Ę\"\u0015�������\rj )J�dY���T9�\bN�\u0006�<{���� v�����_�\u0012m��l\u0006~\u001c��A\u001c�\u0011|�\u000f<\u000f��Áߴc�\u0005_�m/r�;�\u0007���M�.�پ%�F���EzA�~���aܵI7���n�\u001aW�A�����o�>��\t�p���\t�؎\u0007Q��\u001d:}ox\u001d\u0007���.ޜj\u0016ӄFMiR.uɄ\u000e����ט6�\u001a��2�\u0014\u001a�L8Զo��ͽ���n�:�\b�|9���_\u0005>~�?�L��\f]X\u0012nk�'l�Mu���\u0005(�_uWx�\u0010;e�\u0014�3j���_�v����\u000eÃ�oG����%�&�l���g9��ns���\u001a�l�\"��M?\u000b=�}�H�+���;N���/|�h�k\u0004�zC��kG���L~\u001f��c=���IO9?���rZ�p\u001c_k2�wn\u001c;!\bA��\u0001Z؞W�A�\u001c�W�\u0011�\u0016�W��\r�J��Zn������}�YI_4+Z���\n",
            "�zf�صj�n�oDx�^G]8�s����m9�u�\u001d�\u001a��a��s��K�V��yۯ|�w^f��&�d\u001a�LX#9p�\u0018��g�\u001dG}��O�Ԕ\u0014�q<�����\u000e>s���g���Vh�/7�p�z�I{jd�7à?>\u0001���Wv\u0002�l�\u0018\u0006\n",
            "B<�\u0013԰��O�Tw�\u0007`��k��_���zX�:�D5� l�>,\\���9 p��uO\r:58�)L3���(\u0005��������\u0015:���>~�����;�\u000b�VՅ����_\u0012�eh�\u0007����_����9i\u0007!q/�������e~}[�:��Tjv�g\u0005.�ס�+��\u001f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark pyspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tibabgyADCxv",
        "outputId": "3acd3030-892d-4615-ef2b-1ef27326d9ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOCAL\n",
        "%%file local_1_count.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import json\n",
        "from operator import add\n",
        "#import other things as required\n",
        "\n",
        "#create spark context. This is very important. Do this similarly for the other parts\n",
        "# Note to read a file directly from s3 into an rdd you may have to do something like this\n",
        "\n",
        "def checkjson(entry):\n",
        "  try:\n",
        "    json.loads(entry)\n",
        "    #if load succeeded. We use correct as the key\n",
        "    return \"correct\", 1\n",
        "  except:\n",
        "    #there was an error in loading. We use incorrect as the key\n",
        "    return \"incorrect\", 1\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # replace this line with the s3 pass when testing over EMR\n",
        "  conf = SparkConf().setAppName('local_1_count').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "\n",
        "  try:\n",
        "    #@todo: fix the path as required\n",
        "    tweets=sc.textFile('/content/nashville-tweets-2019-01-28')\n",
        "    # review the page rank example for how to use the map operation\n",
        "    # review word count for reduce and add\n",
        "    # see how we use map to parse each row\n",
        "    counts = tweets.map(lambda tweet: checkjson(tweet)).reduceByKey(add)\n",
        "    print(counts.take(2))\n",
        "\n",
        "    # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "    output = counts.sortBy(lambda a: a[1], ascending=False)\n",
        "\n",
        "    # @todo: the s3 version will have to save it to correct s3 path\n",
        "    output.repartition(1).saveAsTextFile(\"local_1_count.out\")\n",
        "    \n",
        "\n",
        "  finally:\n",
        "    # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "    #finally is used to make sure the context is stopped even with errors\n",
        "    sc.stop()\n",
        "  \n",
        "\n",
        " \n",
        "  \n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6864B_IDQa8",
        "outputId": "8b872fb9-3817-486f-c7ae-454ccce20306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing local_1_count.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 local_1_count.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWGlE464FgVe",
        "outputId": "6ca63577-835a-4650-f70f-c827f436bbe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-dce22bc7-e519-41a8-b87c-92cc415d9e8c;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.4.7/spark-streaming-kafka-0-8_2.11-2.4.7.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7!spark-streaming-kafka-0-8_2.11.jar (60ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.kafka#kafka_2.11;0.8.2.1!kafka_2.11.jar (220ms)\n",
            "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
            "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (11ms)\n",
            "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.2/scala-xml_2.11-1.0.2.jar ...\n",
            "\t[SUCCESSFUL ] org.scala-lang.modules#scala-xml_2.11;1.0.2!scala-xml_2.11.jar(bundle) (54ms)\n",
            "downloading https://repo1.maven.org/maven2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar ...\n",
            "\t[SUCCESSFUL ] com.yammer.metrics#metrics-core;2.2.0!metrics-core.jar (16ms)\n",
            "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar ...\n",
            "\t[SUCCESSFUL ] org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0!scala-parser-combinators_2.11.jar(bundle) (34ms)\n",
            "downloading https://repo1.maven.org/maven2/com/101tec/zkclient/0.3/zkclient-0.3.jar ...\n",
            "\t[SUCCESSFUL ] com.101tec#zkclient;0.3!zkclient.jar (15ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;0.8.2.1!kafka-clients.jar (25ms)\n",
            "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...\n",
            "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (12ms)\n",
            "downloading https://repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar ...\n",
            "\t[SUCCESSFUL ] log4j#log4j;1.2.17!log4j.jar(bundle) (68ms)\n",
            "downloading https://repo1.maven.org/maven2/net/jpountz/lz4/lz4/1.2.0/lz4-1.2.0.jar ...\n",
            "\t[SUCCESSFUL ] net.jpountz.lz4#lz4;1.2.0!lz4.jar (18ms)\n",
            "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.7.5/snappy-java-1.1.7.5.jar ...\n",
            "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.7.5!snappy-java.jar(bundle) (117ms)\n",
            ":: resolution report :: resolve 3486ms :: artifacts dl 669ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   12  |   12  |   0   ||   12  |   12  |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-dce22bc7-e519-41a8-b87c-92cc415d9e8c\n",
            "\tconfs: [default]\n",
            "\t12 artifacts copied, 0 already retrieved (8282kB/59ms)\n",
            "23/04/30 02:55:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/30 02:55:35 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/04/30 02:55:35 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 02:55:35 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/30 02:55:35 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 02:55:35 INFO SparkContext: Submitted application: local_1_count\n",
            "23/04/30 02:55:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/30 02:55:35 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/30 02:55:35 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/30 02:55:35 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/30 02:55:35 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/30 02:55:35 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/30 02:55:35 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/30 02:55:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/30 02:55:36 INFO Utils: Successfully started service 'sparkDriver' on port 42663.\n",
            "23/04/30 02:55:36 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/30 02:55:36 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/30 02:55:36 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/30 02:55:36 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/30 02:55:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/30 02:55:36 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-211e57e0-e4d7-4c6b-aa38-2d6fd4110cfd\n",
            "23/04/30 02:55:36 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/30 02:55:36 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/30 02:55:36 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/30 02:55:37 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://fe7812865801:4040\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://fe7812865801:42663/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://fe7812865801:42663/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://fe7812865801:42663/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://fe7812865801:42663/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://fe7812865801:42663/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://fe7812865801:42663/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://fe7812865801:42663/jars/com.101tec_zkclient-0.3.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://fe7812865801:42663/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://fe7812865801:42663/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://fe7812865801:42663/jars/log4j_log4j-1.2.17.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://fe7812865801:42663/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://fe7812865801:42663/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/log4j_log4j-1.2.17.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 02:55:37 INFO Executor: Starting executor ID driver on host fe7812865801\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/log4j_log4j-1.2.17.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO TransportClientFactory: Successfully created connection to fe7812865801/172.28.0.12:42663 after 41 ms (0 ms spent in bootstraps)\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp454044287111138420.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp454044287111138420.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp3647916134763500826.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp3647916134763500826.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp5471250395108677831.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp5471250395108677831.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp970250618888269534.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp970250618888269534.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp8674847155725214929.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp8674847155725214929.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp7380504686912578160.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp7380504686912578160.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/log4j_log4j-1.2.17.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/log4j_log4j-1.2.17.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp8145504063903493070.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp8145504063903493070.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/log4j_log4j-1.2.17.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp7084789055581084624.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp7084789055581084624.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp2434420460375898848.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp2434420460375898848.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp6250372548388709988.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp6250372548388709988.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/com.101tec_zkclient-0.3.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp3691715182997485759.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp3691715182997485759.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp2049087260221762115.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp2049087260221762115.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/30 02:55:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33917.\n",
            "23/04/30 02:55:37 INFO NettyBlockTransferService: Server created on fe7812865801:33917\n",
            "23/04/30 02:55:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/30 02:55:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fe7812865801, 33917, None)\n",
            "23/04/30 02:55:37 INFO BlockManagerMasterEndpoint: Registering block manager fe7812865801:33917 with 366.3 MiB RAM, BlockManagerId(driver, fe7812865801, 33917, None)\n",
            "23/04/30 02:55:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fe7812865801, 33917, None)\n",
            "23/04/30 02:55:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fe7812865801, 33917, None)\n",
            "23/04/30 02:55:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.3 KiB, free 366.0 MiB)\n",
            "23/04/30 02:55:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fe7812865801:33917 (size: 32.0 KiB, free: 366.3 MiB)\n",
            "23/04/30 02:55:39 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 02:55:40 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/30 02:55:40 INFO SparkContext: Starting job: runJob at PythonRDD.scala:166\n",
            "23/04/30 02:55:40 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /content/local_1_count.py:37) as input to shuffle 0\n",
            "23/04/30 02:55:40 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:166) with 1 output partitions\n",
            "23/04/30 02:55:40 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at PythonRDD.scala:166)\n",
            "23/04/30 02:55:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
            "23/04/30 02:55:40 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
            "23/04/30 02:55:41 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/local_1_count.py:37), which has no missing parents\n",
            "23/04/30 02:55:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.2 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fe7812865801:33917 (size: 7.5 KiB, free: 366.3 MiB)\n",
            "23/04/30 02:55:41 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:41 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/local_1_count.py:37) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/30 02:55:41 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "23/04/30 02:55:41 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (fe7812865801, executor driver, partition 0, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:41 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (fe7812865801, executor driver, partition 1, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:41 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "23/04/30 02:55:41 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/30 02:55:42 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:0+12519625\n",
            "23/04/30 02:55:42 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:12519625+12519626\n",
            "23/04/30 02:55:46 INFO PythonRunner: Times: total = 2872, boot = 1417, init = 578, finish = 877\n",
            "23/04/30 02:55:46 INFO PythonRunner: Times: total = 2821, boot = 1433, init = 591, finish = 797\n",
            "23/04/30 02:55:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1612 bytes result sent to driver\n",
            "23/04/30 02:55:46 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1655 bytes result sent to driver\n",
            "23/04/30 02:55:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5020 ms on fe7812865801 (executor driver) (1/2)\n",
            "23/04/30 02:55:46 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 4958 ms on fe7812865801 (executor driver) (2/2)\n",
            "23/04/30 02:55:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:46 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 52053\n",
            "23/04/30 02:55:46 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /content/local_1_count.py:37) finished in 5.613 s\n",
            "23/04/30 02:55:46 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 02:55:46 INFO DAGScheduler: running: Set()\n",
            "23/04/30 02:55:46 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
            "23/04/30 02:55:46 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 02:55:46 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at RDD at PythonRDD.scala:53), which has no missing parents\n",
            "23/04/30 02:55:46 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.2 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:46 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:46 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on fe7812865801:33917 (size: 6.1 KiB, free: 366.3 MiB)\n",
            "23/04/30 02:55:46 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[6] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 02:55:46 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/04/30 02:55:46 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (fe7812865801, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:46 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)\n",
            "23/04/30 02:55:46 INFO ShuffleBlockFetcherIterator: Getting 2 (160.0 B) non-empty blocks including 2 (160.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 27 ms\n",
            "23/04/30 02:55:46 INFO PythonRunner: Times: total = 76, boot = -955, init = 1031, finish = 0\n",
            "23/04/30 02:55:46 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 1625 bytes result sent to driver\n",
            "23/04/30 02:55:46 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 200 ms on fe7812865801 (executor driver) (1/1)\n",
            "23/04/30 02:55:46 INFO DAGScheduler: ResultStage 1 (runJob at PythonRDD.scala:166) finished in 0.224 s\n",
            "23/04/30 02:55:46 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:46 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 02:55:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "23/04/30 02:55:46 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:166, took 6.105414 s\n",
            "23/04/30 02:55:47 INFO SparkContext: Starting job: runJob at PythonRDD.scala:166\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Got job 1 (runJob at PythonRDD.scala:166) with 1 output partitions\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Final stage: ResultStage 3 (runJob at PythonRDD.scala:166)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[7] at RDD at PythonRDD.scala:53), which has no missing parents\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.2 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:47 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on fe7812865801:33917 (size: 6.1 KiB, free: 366.2 MiB)\n",
            "23/04/30 02:55:47 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (PythonRDD[7] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(1))\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (fe7812865801, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:47 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 02:55:47 INFO PythonRunner: Times: total = 74, boot = -1144, init = 1217, finish = 1\n",
            "23/04/30 02:55:47 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1572 bytes result sent to driver\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 104 ms on fe7812865801 (executor driver) (1/1)\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:47 INFO DAGScheduler: ResultStage 3 (runJob at PythonRDD.scala:166) finished in 0.126 s\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Job 1 finished: runJob at PythonRDD.scala:166, took 0.138441 s\n",
            "[('correct', 6294)]\n",
            "23/04/30 02:55:47 INFO SparkContext: Starting job: sortBy at /content/local_1_count.py:41\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Got job 2 (sortBy at /content/local_1_count.py:41) with 2 output partitions\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Final stage: ResultStage 5 (sortBy at /content/local_1_count.py:41)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting ResultStage 5 (PythonRDD[8] at sortBy at /content/local_1_count.py:41), which has no missing parents\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 11.7 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:47 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on fe7812865801:33917 (size: 6.6 KiB, free: 366.2 MiB)\n",
            "23/04/30 02:55:47 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 5 (PythonRDD[8] at sortBy at /content/local_1_count.py:41) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (fe7812865801, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 5) (fe7812865801, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:47 INFO Executor: Running task 1.0 in stage 5.0 (TID 5)\n",
            "23/04/30 02:55:47 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Getting 2 (160.0 B) non-empty blocks including 2 (160.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms\n",
            "23/04/30 02:55:47 INFO PythonRunner: Times: total = 138, boot = -329, init = 467, finish = 0\n",
            "23/04/30 02:55:47 INFO Executor: Finished task 1.0 in stage 5.0 (TID 5). 1612 bytes result sent to driver\n",
            "23/04/30 02:55:47 INFO PythonRunner: Times: total = 136, boot = -126, init = 262, finish = 0\n",
            "23/04/30 02:55:47 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 1612 bytes result sent to driver\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 5) in 187 ms on fe7812865801 (executor driver) (1/2)\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 188 ms on fe7812865801 (executor driver) (2/2)\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:47 INFO DAGScheduler: ResultStage 5 (sortBy at /content/local_1_count.py:41) finished in 0.211 s\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Job 2 finished: sortBy at /content/local_1_count.py:41, took 0.219877 s\n",
            "23/04/30 02:55:47 INFO SparkContext: Starting job: sortBy at /content/local_1_count.py:41\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Got job 3 (sortBy at /content/local_1_count.py:41) with 2 output partitions\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Final stage: ResultStage 7 (sortBy at /content/local_1_count.py:41)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting ResultStage 7 (PythonRDD[9] at sortBy at /content/local_1_count.py:41), which has no missing parents\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 11.0 KiB, free 365.8 MiB)\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 365.8 MiB)\n",
            "23/04/30 02:55:47 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on fe7812865801:33917 (size: 6.4 KiB, free: 366.2 MiB)\n",
            "23/04/30 02:55:47 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 7 (PythonRDD[9] at sortBy at /content/local_1_count.py:41) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Adding task set 7.0 with 2 tasks resource profile 0\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (fe7812865801, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 7) (fe7812865801, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:47 INFO Executor: Running task 1.0 in stage 7.0 (TID 7)\n",
            "23/04/30 02:55:47 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Getting 2 (160.0 B) non-empty blocks including 2 (160.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms\n",
            "23/04/30 02:55:47 INFO PythonRunner: Times: total = 135, boot = -58, init = 193, finish = 0\n",
            "23/04/30 02:55:47 INFO PythonRunner: Times: total = 128, boot = -84, init = 212, finish = 0\n",
            "23/04/30 02:55:47 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 1613 bytes result sent to driver\n",
            "23/04/30 02:55:47 INFO Executor: Finished task 1.0 in stage 7.0 (TID 7). 1572 bytes result sent to driver\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 181 ms on fe7812865801 (executor driver) (1/2)\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 7) in 182 ms on fe7812865801 (executor driver) (2/2)\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:47 INFO DAGScheduler: ResultStage 7 (sortBy at /content/local_1_count.py:41) finished in 0.197 s\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Job 3 finished: sortBy at /content/local_1_count.py:41, took 0.205789 s\n",
            "23/04/30 02:55:47 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "23/04/30 02:55:47 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/30 02:55:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 02:55:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 02:55:47 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Registering RDD 11 (sortBy at /content/local_1_count.py:41) as input to shuffle 2\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Registering RDD 15 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Got job 4 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Final stage: ResultStage 11 (runJob at SparkHadoopWriter.scala:83)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 10)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting ShuffleMapStage 9 (PairwiseRDD[11] at sortBy at /content/local_1_count.py:41), which has no missing parents\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 11.8 KiB, free 365.8 MiB)\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 365.8 MiB)\n",
            "23/04/30 02:55:47 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on fe7812865801:33917 (size: 7.2 KiB, free: 366.2 MiB)\n",
            "23/04/30 02:55:47 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (PairwiseRDD[11] at sortBy at /content/local_1_count.py:41) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Adding task set 9.0 with 2 tasks resource profile 0\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 8) (fe7812865801, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 9) (fe7812865801, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:47 INFO Executor: Running task 0.0 in stage 9.0 (TID 8)\n",
            "23/04/30 02:55:47 INFO Executor: Running task 1.0 in stage 9.0 (TID 9)\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Getting 2 (160.0 B) non-empty blocks including 2 (160.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms\n",
            "23/04/30 02:55:48 INFO PythonRunner: Times: total = 132, boot = -258, init = 390, finish = 0\n",
            "23/04/30 02:55:48 INFO Executor: Finished task 0.0 in stage 9.0 (TID 8). 1784 bytes result sent to driver\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 8) in 173 ms on fe7812865801 (executor driver) (1/2)\n",
            "23/04/30 02:55:48 INFO PythonRunner: Times: total = 170, boot = -290, init = 460, finish = 0\n",
            "23/04/30 02:55:48 INFO Executor: Finished task 1.0 in stage 9.0 (TID 9). 1655 bytes result sent to driver\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 9) in 214 ms on fe7812865801 (executor driver) (2/2)\n",
            "23/04/30 02:55:48 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:48 INFO DAGScheduler: ShuffleMapStage 9 (sortBy at /content/local_1_count.py:41) finished in 0.229 s\n",
            "23/04/30 02:55:48 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 02:55:48 INFO DAGScheduler: running: Set()\n",
            "23/04/30 02:55:48 INFO DAGScheduler: waiting: Set(ShuffleMapStage 10, ResultStage 11)\n",
            "23/04/30 02:55:48 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 02:55:48 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[15] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 02:55:48 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 10.2 KiB, free 365.8 MiB)\n",
            "23/04/30 02:55:48 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 365.8 MiB)\n",
            "23/04/30 02:55:48 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on fe7812865801:33917 (size: 6.0 KiB, free: 366.2 MiB)\n",
            "23/04/30 02:55:48 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[15] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/30 02:55:48 INFO TaskSchedulerImpl: Adding task set 10.0 with 2 tasks resource profile 0\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 10) (fe7812865801, executor driver, partition 1, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 11) (fe7812865801, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:48 INFO Executor: Running task 1.0 in stage 10.0 (TID 10)\n",
            "23/04/30 02:55:48 INFO Executor: Running task 0.0 in stage 10.0 (TID 11)\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms\n",
            "23/04/30 02:55:48 INFO PythonRunner: Times: total = 129, boot = -107, init = 236, finish = 0\n",
            "23/04/30 02:55:48 INFO Executor: Finished task 0.0 in stage 10.0 (TID 11). 1654 bytes result sent to driver\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 11) in 182 ms on fe7812865801 (executor driver) (1/2)\n",
            "23/04/30 02:55:48 INFO PythonRunner: Times: total = 151, boot = -56, init = 207, finish = 0\n",
            "23/04/30 02:55:48 INFO Executor: Finished task 1.0 in stage 10.0 (TID 10). 1783 bytes result sent to driver\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 10) in 218 ms on fe7812865801 (executor driver) (2/2)\n",
            "23/04/30 02:55:48 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:48 INFO DAGScheduler: ShuffleMapStage 10 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.250 s\n",
            "23/04/30 02:55:48 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 02:55:48 INFO DAGScheduler: running: Set()\n",
            "23/04/30 02:55:48 INFO DAGScheduler: waiting: Set(ResultStage 11)\n",
            "23/04/30 02:55:48 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 02:55:48 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[21] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 02:55:48 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 105.9 KiB, free 365.7 MiB)\n",
            "23/04/30 02:55:48 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 39.7 KiB, free 365.7 MiB)\n",
            "23/04/30 02:55:48 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on fe7812865801:33917 (size: 39.7 KiB, free: 366.2 MiB)\n",
            "23/04/30 02:55:48 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[21] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 02:55:48 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 12) (fe7812865801, executor driver, partition 0, NODE_LOCAL, 4547 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:48 INFO Executor: Running task 0.0 in stage 11.0 (TID 12)\n",
            "23/04/30 02:55:48 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/30 02:55:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 02:55:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms\n",
            "23/04/30 02:55:48 INFO PythonRunner: Times: total = 145, boot = -213, init = 358, finish = 0\n",
            "23/04/30 02:55:48 INFO FileOutputCommitter: Saved output of task 'attempt_202304300255471325538644976917902_0021_m_000000_0' to file:/content/local_1_count.out/_temporary/0/task_202304300255471325538644976917902_0021_m_000000\n",
            "23/04/30 02:55:48 INFO SparkHadoopMapRedUtil: attempt_202304300255471325538644976917902_0021_m_000000_0: Committed\n",
            "23/04/30 02:55:48 INFO Executor: Finished task 0.0 in stage 11.0 (TID 12). 1952 bytes result sent to driver\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 12) in 315 ms on fe7812865801 (executor driver) (1/1)\n",
            "23/04/30 02:55:48 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:48 INFO DAGScheduler: ResultStage 11 (runJob at SparkHadoopWriter.scala:83) finished in 0.369 s\n",
            "23/04/30 02:55:48 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 02:55:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
            "23/04/30 02:55:48 INFO DAGScheduler: Job 4 finished: runJob at SparkHadoopWriter.scala:83, took 0.884339 s\n",
            "23/04/30 02:55:48 INFO SparkHadoopWriter: Start to commit write Job job_202304300255471325538644976917902_0021.\n",
            "23/04/30 02:55:48 INFO SparkHadoopWriter: Write Job job_202304300255471325538644976917902_0021 committed. Elapsed time: 18 ms.\n",
            "23/04/30 02:55:48 INFO SparkUI: Stopped Spark web UI at http://fe7812865801:4040\n",
            "23/04/30 02:55:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/30 02:55:48 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/30 02:55:48 INFO BlockManager: BlockManager stopped\n",
            "23/04/30 02:55:48 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/30 02:55:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/30 02:55:48 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/30 02:55:49 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/30 02:55:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a\n",
            "23/04/30 02:55:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-73213582-d3dc-4f67-b966-404ac352b724\n",
            "23/04/30 02:55:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/pyspark-3bfb6939-d3f0-4839-98e2-c8d3755da6ac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "credentials = {\n",
        "    'region_name': 'us-east-1',\n",
        "    'aws_access_key_id': 'ASIAYELUFHJM2IAO5LOI',\n",
        "    'aws_secret_access_key': 'Kicby5su+q8Io/ZdX25RI/hfAdqsYfGlO4k/maUg',\n",
        "    'aws_session_token': 'FwoGZXIvYXdzEI3//////////wEaDF+YaQijw81lcUGJtyLMAcvxVUB1kRKaxxo1VJbOjaHFR959YX6uaibZ4CbJSBKIOGk8x8PsDIZ5GXCTtJKg5nFpu2YNi1sSGFD68tU1zoq6SkKyTvXfIAm9VE2X9UDg2Tr7a1JgCLLpzRnXYiMslGh1KB/M8olhJrcv5ITXu93fPYzskdebAakk347vxVmfKOsBaOG6iAuCfvKr4dkVu8gTFsWRbmxK/Z+ZQWHkAiB4zcoMx8FdW4Lfi9jB8e2w2nBd23rVM/Ioz6Uc+J5kCG0/9kbzKLiDvHESaSjmubeiBjItWG+zzDJ0zoEl2XMOY1E4oWNE8mZyDqdW42kWIpEqJFzZCfy8ZfNR5iWJo4U4'\n",
        "}"
      ],
      "metadata": {
        "id": "VsrEMSFVGAFq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replae with your EMR cluster ID\n",
        "CLUSTER_ID = 'j-2ELHWKR7TCCMD'\n",
        "\n",
        "def submit_job(app_name, pyfile_uri):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ],
      "metadata": {
        "id": "N7NWQw5jOvTi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3\n",
        "import boto3, json\n",
        "\n",
        "session = boto3.session.Session(**credentials)\n",
        "s3 = session.client('s3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiKCtrACGbX7",
        "outputId": "9d587d92-f6e2-46a3-e197-6216d07a041c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.123-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.123\n",
            "  Downloading botocore-1.29.123-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.123->boto3) (1.26.15)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.123->boto3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.123->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.26.123 botocore-1.29.123 jmespath-1.0.1 s3transfer-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EMR\n",
        "%%file testing.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import json\n",
        "from operator import add\n",
        "#import other things as required\n",
        "\n",
        "#create spark context. This is very important. Do this similarly for the other parts\n",
        "# Note to read a file directly from s3 into an rdd you may have to do something like this\n",
        "\n",
        "def checkjson(entry):\n",
        "  try:\n",
        "    json.loads(entry)\n",
        "    #if load succeeded. We use correct as the key\n",
        "    return \"correct\", 1\n",
        "  except:\n",
        "    #there was an error in loading. We use incorrect as the key\n",
        "    return \"incorrect\", 1\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # replace this line with the s3 pass when testing over EMR\n",
        "  conf = SparkConf().setAppName('testing').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "\n",
        "  try:\n",
        "    #@todo: fix the path as required\n",
        "    tweets=sc.textFile('s3://cs4266-finalproject/nashville-tweets-2019-01-28')\n",
        "    # review the page rank example for how to use the map operation\n",
        "    # review word count for reduce and add\n",
        "    # see how we use map to parse each row\n",
        "    counts = tweets.map(lambda tweet: checkjson(tweet)).reduceByKey(add)\n",
        "    print(counts.take(2))\n",
        "\n",
        "    # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "    output = counts.sortBy(lambda a: a[1], ascending=False)\n",
        "\n",
        "    # @todo: the s3 version will have to save it to correct s3 path\n",
        "    output.repartition(1).saveAsTextFile(\"s3://cs4266-finalproject-spark/emr-logs/testing.out\")\n",
        "    \n",
        "\n",
        "  finally:\n",
        "    # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "    #finally is used to make sure the context is stopped even with errors\n",
        "    sc.stop()\n",
        "  \n",
        "\n",
        " \n",
        "  \n",
        "  pass"
      ],
      "metadata": {
        "id": "eWh1EXj_GmhK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a79bc4b-7981-4ac0-fb64-e2ec83491041"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing testing.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s3.upload_file(Filename='testing.py', Bucket='cs4266-finalproject-spark', Key='emr-logs/testing.py')"
      ],
      "metadata": {
        "id": "R6kXItziJXCT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# submit spark job to emr. Make all the necessary changes to the path\n",
        "submit_job(app_name='testing', pyfile_uri='s3://cs4266-finalproject-spark/emr-logs/testing.py')"
      ],
      "metadata": {
        "id": "KomT1ochOeEo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test1(lines):\n",
        "  if '6294' in lines[0] and 'correct' in lines[0]:\n",
        "    print(\"passed\")\n",
        "  else:\n",
        "    print(\"failed\")\n",
        "\n",
        "# test local execution results\n",
        "with open('local_1_count.out/part-00000') as f:\n",
        "  lines = f.readlines()\n",
        "  print(lines)\n",
        "  test1(lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKQKQItEQRIB",
        "outputId": "ceb843f9-bfc4-47ee-da62-60e5f28af425"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"('correct', 6294)\\n\"]\n",
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test EMR execution results. Once again, make sure that S3 paths are consistent.\n",
        "output_key = \"emr-logs/testing.out/part-00000\"\n",
        "lines = s3.get_object(Bucket='cs4266-finalproject-spark', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test1(lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfjBYpjMOgRE",
        "outputId": "9494d35a-6cad-4baf-a349-6538a190f483"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We have verified that the old homework question works with our EMR and S3 buckets...\n",
        "\n",
        "### Now, let's move on to trying to read in a basic parquet file"
      ],
      "metadata": {
        "id": "k9-oFWK1RHEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EMR\n",
        "%%file testing2.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('testing2').getOrCreate()\n",
        "\n",
        "try:\n",
        "  # read in parquet file\n",
        "  # part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n",
        "  tweets_df = spark.read.parquet('s3://cs4266-finalproject/weather_tn.parquet/year=2021/month=1/part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet')\n",
        "  tweets_df.createOrReplaceTempView(\"tweets\")\n",
        "\n",
        "  # run SQL query\n",
        "  counts = spark.sql('SELECT COUNT(*) FROM tweets')\n",
        "  print(counts.show())\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = counts.sort(\"count\", ascending=False)\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"s3://cs4266-finalproject-spark/emr-logs/testing2.out\")\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzh03JB9OhMM",
        "outputId": "3afaceba-803a-44da-9aa9-4490a1bfcb36"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting testing2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s3.upload_file(Filename='testing2.py', Bucket='cs4266-finalproject-spark', Key='emr-logs/testing2.py')"
      ],
      "metadata": {
        "id": "aJ0Q8xMATJMS"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# submit spark job to emr. Make all the necessary changes to the path\n",
        "submit_job(app_name='testing2', pyfile_uri='s3://cs4266-finalproject-spark/emr-logs/testing2.py')"
      ],
      "metadata": {
        "id": "CVQBTrzgTVBt"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_key = \"emr-logs/testing2.out/part-00000-c588163a-1b46-4c13-a474-0b931eba155a-c000.csv\"\n",
        "lines = s3.get_object(Bucket='cs4266-finalproject-spark', Key=output_key)['Body'].read().decode().splitlines()"
      ],
      "metadata": {
        "id": "i-j-HM4jTcoY"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GWJUXsw9UlR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF77WV81UGuD",
        "outputId": "24236652-1a6a-4665-bbc6-133323f52234"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['29765']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WE GOT 29765 ROWS FOR ONE PARQUET"
      ],
      "metadata": {
        "id": "3tMMUUgdYyDN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fUu_GaZ2deP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8XOyHXi0deSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aJzXv7sXdeUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PFVt7Gf4deWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WKI707gVdeY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRYING A BRAVE NEW ATTEMPT TO LOAD IT IN LOCALLY TO TEST SPARK"
      ],
      "metadata": {
        "id": "S1UZ1DHxde1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_path = 'weather_tn.parquet/year=2021/month=1/part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet'"
      ],
      "metadata": {
        "id": "DBbYYziJdk8k"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s3.download_file('cs4266-finalproject', full_path, 'weather.parquet')"
      ],
      "metadata": {
        "id": "IC7Z7Lfsd4JL"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s3.download_file('cs4266-finalproject', 'nfd_incidents_xd_seg.parquet', 'incidents.parquet')"
      ],
      "metadata": {
        "id": "SNF4bWhNfURx"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file localtest2.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('localtest2').getOrCreate()\n",
        "\n",
        "try:\n",
        "  # read in parquet file\n",
        "  # part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n",
        "  tweets_df = spark.read.parquet('weather.parquet')\n",
        "  tweets_df.createOrReplaceTempView(\"weather\")\n",
        "\n",
        "  # run SQL query\n",
        "  counts = spark.sql('SELECT COUNT(*) FROM weather')\n",
        "  print(counts.show())\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = counts#.sort(\"count\", ascending=False)\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"localtest2.out\")\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-4iudk5fw0e",
        "outputId": "40d70a34-5343-4bf9-e663-259feeb7824d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting localtest2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 localtest2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfyx2q_OgVIb",
        "outputId": "74036211-2560-426a-935d-ff7bc1a1e19c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9ad98076-9171-4319-8414-81f5ed5f0aff;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 761ms :: artifacts dl 32ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-9ad98076-9171-4319-8414-81f5ed5f0aff\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/13ms)\n",
            "23/04/30 04:59:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/30 04:59:15 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/04/30 04:59:15 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 04:59:15 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/30 04:59:16 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 04:59:16 INFO SparkContext: Submitted application: localtest2\n",
            "23/04/30 04:59:16 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/30 04:59:16 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/30 04:59:16 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/30 04:59:16 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/30 04:59:16 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/30 04:59:16 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/30 04:59:16 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/30 04:59:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/30 04:59:16 INFO Utils: Successfully started service 'sparkDriver' on port 41965.\n",
            "23/04/30 04:59:17 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/30 04:59:17 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/30 04:59:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/30 04:59:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/30 04:59:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/30 04:59:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7833ebfd-340c-4ea5-a063-e4ef5c1e48c2\n",
            "23/04/30 04:59:17 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/30 04:59:17 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/30 04:59:17 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/30 04:59:17 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://fe7812865801:4040\n",
            "23/04/30 04:59:17 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://fe7812865801:41965/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://fe7812865801:41965/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://fe7812865801:41965/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://fe7812865801:41965/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://fe7812865801:41965/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://fe7812865801:41965/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://fe7812865801:41965/jars/com.101tec_zkclient-0.3.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://fe7812865801:41965/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://fe7812865801:41965/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://fe7812865801:41965/jars/log4j_log4j-1.2.17.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://fe7812865801:41965/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://fe7812865801:41965/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 04:59:17 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 04:59:17 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 04:59:17 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 04:59:17 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 04:59:17 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 04:59:17 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 04:59:17 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 04:59:17 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 04:59:17 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/log4j_log4j-1.2.17.jar\n",
            "23/04/30 04:59:17 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:17 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 04:59:18 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 04:59:18 INFO Executor: Starting executor ID driver on host fe7812865801\n",
            "23/04/30 04:59:18 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/log4j_log4j-1.2.17.jar\n",
            "23/04/30 04:59:18 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 04:59:18 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 04:59:18 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 04:59:18 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 04:59:18 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 04:59:18 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 04:59:18 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 04:59:18 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 04:59:18 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 04:59:18 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 04:59:18 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 04:59:18 INFO Executor: Fetching spark://fe7812865801:41965/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO TransportClientFactory: Successfully created connection to fe7812865801/172.28.0.12:41965 after 65 ms (0 ms spent in bootstraps)\n",
            "23/04/30 04:59:18 INFO Utils: Fetching spark://fe7812865801:41965/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp6553971203765668274.tmp\n",
            "23/04/30 04:59:18 INFO Utils: /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp6553971203765668274.tmp has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 04:59:18 INFO Executor: Adding file:/tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/30 04:59:18 INFO Executor: Fetching spark://fe7812865801:41965/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO Utils: Fetching spark://fe7812865801:41965/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp6961603494454355360.tmp\n",
            "23/04/30 04:59:18 INFO Utils: /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp6961603494454355360.tmp has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 04:59:18 INFO Executor: Adding file:/tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/30 04:59:18 INFO Executor: Fetching spark://fe7812865801:41965/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO Utils: Fetching spark://fe7812865801:41965/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp7831080075003986848.tmp\n",
            "23/04/30 04:59:18 INFO Utils: /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp7831080075003986848.tmp has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 04:59:18 INFO Executor: Adding file:/tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/30 04:59:18 INFO Executor: Fetching spark://fe7812865801:41965/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO Utils: Fetching spark://fe7812865801:41965/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp5970918687171226789.tmp\n",
            "23/04/30 04:59:18 INFO Utils: /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp5970918687171226789.tmp has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 04:59:18 INFO Executor: Adding file:/tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/30 04:59:18 INFO Executor: Fetching spark://fe7812865801:41965/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO Utils: Fetching spark://fe7812865801:41965/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp3782356831377205989.tmp\n",
            "23/04/30 04:59:18 INFO Utils: /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp3782356831377205989.tmp has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 04:59:18 INFO Executor: Adding file:/tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/30 04:59:18 INFO Executor: Fetching spark://fe7812865801:41965/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO Utils: Fetching spark://fe7812865801:41965/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp2117382966981637659.tmp\n",
            "23/04/30 04:59:18 INFO Utils: /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp2117382966981637659.tmp has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 04:59:18 INFO Executor: Adding file:/tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/30 04:59:18 INFO Executor: Fetching spark://fe7812865801:41965/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO Utils: Fetching spark://fe7812865801:41965/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp5723197867494609297.tmp\n",
            "23/04/30 04:59:18 INFO Utils: /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp5723197867494609297.tmp has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 04:59:18 INFO Executor: Adding file:/tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/30 04:59:18 INFO Executor: Fetching spark://fe7812865801:41965/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO Utils: Fetching spark://fe7812865801:41965/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp5527878154305518913.tmp\n",
            "23/04/30 04:59:18 INFO Utils: /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp5527878154305518913.tmp has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 04:59:18 INFO Executor: Adding file:/tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/30 04:59:18 INFO Executor: Fetching spark://fe7812865801:41965/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:18 INFO Utils: Fetching spark://fe7812865801:41965/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp704200263250209337.tmp\n",
            "23/04/30 04:59:19 INFO Utils: /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp704200263250209337.tmp has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 04:59:19 INFO Executor: Adding file:/tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/30 04:59:19 INFO Executor: Fetching spark://fe7812865801:41965/jars/log4j_log4j-1.2.17.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:19 INFO Utils: Fetching spark://fe7812865801:41965/jars/log4j_log4j-1.2.17.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp2573238380621229123.tmp\n",
            "23/04/30 04:59:19 INFO Utils: /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp2573238380621229123.tmp has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/log4j_log4j-1.2.17.jar\n",
            "23/04/30 04:59:19 INFO Executor: Adding file:/tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/30 04:59:19 INFO Executor: Fetching spark://fe7812865801:41965/jars/com.101tec_zkclient-0.3.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:19 INFO Utils: Fetching spark://fe7812865801:41965/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp4593676100275249819.tmp\n",
            "23/04/30 04:59:19 INFO Utils: /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp4593676100275249819.tmp has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 04:59:19 INFO Executor: Adding file:/tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/30 04:59:19 INFO Executor: Fetching spark://fe7812865801:41965/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682830755875\n",
            "23/04/30 04:59:19 INFO Utils: Fetching spark://fe7812865801:41965/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp7954230970992500451.tmp\n",
            "23/04/30 04:59:19 INFO Utils: /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/fetchFileTemp7954230970992500451.tmp has been previously copied to /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 04:59:19 INFO Executor: Adding file:/tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/userFiles-8ae2016f-150d-454d-beae-f562ba5eac7e/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/30 04:59:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40301.\n",
            "23/04/30 04:59:19 INFO NettyBlockTransferService: Server created on fe7812865801:40301\n",
            "23/04/30 04:59:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/30 04:59:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fe7812865801, 40301, None)\n",
            "23/04/30 04:59:19 INFO BlockManagerMasterEndpoint: Registering block manager fe7812865801:40301 with 366.3 MiB RAM, BlockManagerId(driver, fe7812865801, 40301, None)\n",
            "23/04/30 04:59:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fe7812865801, 40301, None)\n",
            "23/04/30 04:59:19 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fe7812865801, 40301, None)\n",
            "23/04/30 04:59:19 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/04/30 04:59:19 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/04/30 04:59:21 INFO InMemoryFileIndex: It took 48 ms to list leaf files for 1 paths.\n",
            "23/04/30 04:59:21 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 04:59:21 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 04:59:21 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 04:59:21 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 04:59:21 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 04:59:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 04:59:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/04/30 04:59:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/04/30 04:59:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fe7812865801:40301 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 04:59:22 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 04:59:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 04:59:22 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/30 04:59:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (fe7812865801, executor driver, partition 0, PROCESS_LOCAL, 4577 bytes) taskResourceAssignments Map()\n",
            "23/04/30 04:59:22 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/30 04:59:23 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3161 bytes result sent to driver\n",
            "23/04/30 04:59:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1160 ms on fe7812865801 (executor driver) (1/1)\n",
            "23/04/30 04:59:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/30 04:59:23 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.483 s\n",
            "23/04/30 04:59:23 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 04:59:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/30 04:59:23 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.585649 s\n",
            "23/04/30 04:59:24 INFO BlockManagerInfo: Removed broadcast_0_piece0 on fe7812865801:40301 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 04:59:26 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "23/04/30 04:59:27 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/30 04:59:27 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "23/04/30 04:59:27 INFO FileSourceStrategy: Output Data Schema: struct<>\n",
            "23/04/30 04:59:28 INFO CodeGenerator: Code generated in 613.715867 ms\n",
            "23/04/30 04:59:28 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 351.9 KiB, free 366.0 MiB)\n",
            "23/04/30 04:59:28 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.4 KiB, free 365.9 MiB)\n",
            "23/04/30 04:59:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fe7812865801:40301 (size: 35.4 KiB, free: 366.3 MiB)\n",
            "23/04/30 04:59:28 INFO SparkContext: Created broadcast 1 from showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 04:59:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/30 04:59:28 INFO DAGScheduler: Registering RDD 5 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/30 04:59:28 INFO DAGScheduler: Got map stage job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 04:59:28 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 04:59:28 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 04:59:28 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 04:59:28 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 04:59:28 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 15.2 KiB, free 365.9 MiB)\n",
            "23/04/30 04:59:28 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 365.9 MiB)\n",
            "23/04/30 04:59:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on fe7812865801:40301 (size: 7.2 KiB, free: 366.3 MiB)\n",
            "23/04/30 04:59:28 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 04:59:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 04:59:28 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/04/30 04:59:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (fe7812865801, executor driver, partition 0, PROCESS_LOCAL, 4845 bytes) taskResourceAssignments Map()\n",
            "23/04/30 04:59:28 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/30 04:59:29 INFO FileScanRDD: Reading File path: file:///content/weather.parquet, range: 0-3369951, partition values: [empty row]\n",
            "23/04/30 04:59:29 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2167 bytes result sent to driver\n",
            "23/04/30 04:59:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 572 ms on fe7812865801 (executor driver) (1/1)\n",
            "23/04/30 04:59:29 INFO DAGScheduler: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.708 s\n",
            "23/04/30 04:59:29 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 04:59:29 INFO DAGScheduler: running: Set()\n",
            "23/04/30 04:59:29 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 04:59:29 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 04:59:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/30 04:59:29 INFO CodeGenerator: Code generated in 55.108699 ms\n",
            "23/04/30 04:59:29 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 04:59:29 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 04:59:29 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 04:59:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/30 04:59:29 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 04:59:29 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 04:59:29 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.1 KiB, free 365.9 MiB)\n",
            "23/04/30 04:59:29 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 365.9 MiB)\n",
            "23/04/30 04:59:29 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on fe7812865801:40301 (size: 5.5 KiB, free: 366.3 MiB)\n",
            "23/04/30 04:59:29 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 04:59:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 04:59:29 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/30 04:59:29 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (fe7812865801, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/30 04:59:29 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
            "23/04/30 04:59:29 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 04:59:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 34 ms\n",
            "23/04/30 04:59:29 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 2657 bytes result sent to driver\n",
            "23/04/30 04:59:29 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 155 ms on fe7812865801 (executor driver) (1/1)\n",
            "23/04/30 04:59:29 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.216 s\n",
            "23/04/30 04:59:29 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 04:59:29 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/30 04:59:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/30 04:59:29 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.262440 s\n",
            "23/04/30 04:59:30 INFO CodeGenerator: Code generated in 59.340607 ms\n",
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|  144336|\n",
            "+--------+\n",
            "\n",
            "None\n",
            "23/04/30 04:59:30 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/30 04:59:30 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "23/04/30 04:59:30 INFO FileSourceStrategy: Output Data Schema: struct<>\n",
            "23/04/30 04:59:30 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 351.9 KiB, free 365.5 MiB)\n",
            "23/04/30 04:59:30 INFO BlockManagerInfo: Removed broadcast_1_piece0 on fe7812865801:40301 in memory (size: 35.4 KiB, free: 366.3 MiB)\n",
            "23/04/30 04:59:30 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.4 KiB, free 365.9 MiB)\n",
            "23/04/30 04:59:30 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on fe7812865801:40301 (size: 35.4 KiB, free: 366.3 MiB)\n",
            "23/04/30 04:59:30 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 04:59:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/30 04:59:30 INFO BlockManagerInfo: Removed broadcast_2_piece0 on fe7812865801:40301 in memory (size: 7.2 KiB, free: 366.3 MiB)\n",
            "23/04/30 04:59:30 INFO BlockManagerInfo: Removed broadcast_3_piece0 on fe7812865801:40301 in memory (size: 5.5 KiB, free: 366.3 MiB)\n",
            "23/04/30 04:59:30 INFO DAGScheduler: Registering RDD 12 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/30 04:59:30 INFO DAGScheduler: Got map stage job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 04:59:30 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 04:59:30 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 04:59:30 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 04:59:30 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[12] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 04:59:30 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.2 KiB, free 365.9 MiB)\n",
            "23/04/30 04:59:30 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 365.9 MiB)\n",
            "23/04/30 04:59:30 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on fe7812865801:40301 (size: 7.2 KiB, free: 366.3 MiB)\n",
            "23/04/30 04:59:30 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 04:59:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[12] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 04:59:30 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "23/04/30 04:59:30 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (fe7812865801, executor driver, partition 0, PROCESS_LOCAL, 4845 bytes) taskResourceAssignments Map()\n",
            "23/04/30 04:59:30 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)\n",
            "23/04/30 04:59:30 INFO FileScanRDD: Reading File path: file:///content/weather.parquet, range: 0-3369951, partition values: [empty row]\n",
            "23/04/30 04:59:30 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 2124 bytes result sent to driver\n",
            "23/04/30 04:59:30 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 60 ms on fe7812865801 (executor driver) (1/1)\n",
            "23/04/30 04:59:30 INFO DAGScheduler: ShuffleMapStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 0.088 s\n",
            "23/04/30 04:59:30 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 04:59:30 INFO DAGScheduler: running: Set()\n",
            "23/04/30 04:59:30 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 04:59:30 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 04:59:30 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "23/04/30 04:59:31 INFO CodeGenerator: Code generated in 40.443593 ms\n",
            "23/04/30 04:59:31 INFO DAGScheduler: Registering RDD 15 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
            "23/04/30 04:59:31 INFO DAGScheduler: Got map stage job 4 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 04:59:31 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 04:59:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
            "23/04/30 04:59:31 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 04:59:31 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 04:59:31 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 11.8 KiB, free 365.9 MiB)\n",
            "23/04/30 04:59:31 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 365.9 MiB)\n",
            "23/04/30 04:59:31 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on fe7812865801:40301 (size: 6.0 KiB, free: 366.3 MiB)\n",
            "23/04/30 04:59:31 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 04:59:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 04:59:31 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "23/04/30 04:59:31 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (fe7812865801, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/30 04:59:31 INFO Executor: Running task 0.0 in stage 6.0 (TID 4)\n",
            "23/04/30 04:59:31 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 04:59:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
            "23/04/30 04:59:31 INFO Executor: Finished task 0.0 in stage 6.0 (TID 4). 2839 bytes result sent to driver\n",
            "23/04/30 04:59:31 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 51 ms on fe7812865801 (executor driver) (1/1)\n",
            "23/04/30 04:59:31 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "23/04/30 04:59:31 INFO DAGScheduler: ShuffleMapStage 6 (csv at NativeMethodAccessorImpl.java:0) finished in 0.070 s\n",
            "23/04/30 04:59:31 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 04:59:31 INFO DAGScheduler: running: Set()\n",
            "23/04/30 04:59:31 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 04:59:31 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 04:59:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 04:59:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 04:59:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 04:59:31 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 04:59:31 INFO DAGScheduler: Got job 5 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 04:59:31 INFO DAGScheduler: Final stage: ResultStage 9 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 04:59:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
            "23/04/30 04:59:31 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 04:59:31 INFO DAGScheduler: Submitting ResultStage 9 (ShuffledRowRDD[16] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 04:59:31 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 211.2 KiB, free 365.7 MiB)\n",
            "23/04/30 04:59:31 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 75.5 KiB, free 365.6 MiB)\n",
            "23/04/30 04:59:31 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on fe7812865801:40301 (size: 75.5 KiB, free: 366.2 MiB)\n",
            "23/04/30 04:59:31 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 04:59:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (ShuffledRowRDD[16] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 04:59:31 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "23/04/30 04:59:31 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 5) (fe7812865801, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/30 04:59:31 INFO Executor: Running task 0.0 in stage 9.0 (TID 5)\n",
            "23/04/30 04:59:31 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 04:59:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms\n",
            "23/04/30 04:59:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 04:59:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 04:59:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 04:59:31 INFO FileOutputCommitter: Saved output of task 'attempt_202304300459313452429895371023818_0009_m_000000_5' to file:/content/localtest2.out/_temporary/0/task_202304300459313452429895371023818_0009_m_000000\n",
            "23/04/30 04:59:31 INFO SparkHadoopMapRedUtil: attempt_202304300459313452429895371023818_0009_m_000000_5: Committed\n",
            "23/04/30 04:59:31 INFO Executor: Finished task 0.0 in stage 9.0 (TID 5). 3483 bytes result sent to driver\n",
            "23/04/30 04:59:31 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 5) in 304 ms on fe7812865801 (executor driver) (1/1)\n",
            "23/04/30 04:59:31 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "23/04/30 04:59:31 INFO DAGScheduler: ResultStage 9 (csv at NativeMethodAccessorImpl.java:0) finished in 0.372 s\n",
            "23/04/30 04:59:31 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 04:59:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "23/04/30 04:59:31 INFO DAGScheduler: Job 5 finished: csv at NativeMethodAccessorImpl.java:0, took 0.384275 s\n",
            "23/04/30 04:59:31 INFO FileFormatWriter: Start to commit write Job 9b17ff43-5ea2-40ec-9ee8-249bdfd31dab.\n",
            "23/04/30 04:59:31 INFO FileFormatWriter: Write Job 9b17ff43-5ea2-40ec-9ee8-249bdfd31dab committed. Elapsed time: 23 ms.\n",
            "23/04/30 04:59:31 INFO FileFormatWriter: Finished processing stats for write job 9b17ff43-5ea2-40ec-9ee8-249bdfd31dab.\n",
            "23/04/30 04:59:31 INFO SparkUI: Stopped Spark web UI at http://fe7812865801:4040\n",
            "23/04/30 04:59:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/30 04:59:31 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/30 04:59:31 INFO BlockManager: BlockManager stopped\n",
            "23/04/30 04:59:31 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/30 04:59:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/30 04:59:31 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/30 04:59:32 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/30 04:59:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-446de7a7-2647-4501-92b4-e39cfa9724b6\n",
            "23/04/30 04:59:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be\n",
            "23/04/30 04:59:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-450d7e9e-6fa2-493a-be10-34a2c744c0be/pyspark-26e7ad6c-5787-4558-80d0-57b4e3f52e9b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('localtest2.out/part-00000-d87ee3a4-9e6a-4b96-98ee-f4fc22acc9cc-c000.csv') as f:\n",
        "  lines = f.readlines()\n",
        "  print(lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9W_fBPihihW",
        "outputId": "84facf08-a6b7-4f8c-beef-9893bc34f869"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['144336\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOCALTEST2 WORKED... LET'S TRY TO SEND IT TO EMR!"
      ],
      "metadata": {
        "id": "r5OJ4Ks0kGpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file emrtest3.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('emrtest3').getOrCreate()\n",
        "\n",
        "try:\n",
        "  # read in parquet file\n",
        "  # part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n",
        "  tweets_df = spark.read.parquet('s3://weather_tn.parquet:*/*/*.parquet')\n",
        "  tweets_df.createOrReplaceTempView(\"weather\")\n",
        "\n",
        "  # run SQL query\n",
        "  counts = spark.sql('SELECT COUNT(*) FROM weather')\n",
        "  print(counts.show())\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = counts#.sort(\"count\", ascending=False)\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"emrtest3.out\")\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnOxyqX1j_f2",
        "outputId": "f78c8e8a-781d-4fd0-a76d-ae6dd3780e14"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting emrtest3.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s3.upload_file(Filename='emrtest3.py', Bucket='cs4266-finalproject-spark', Key='emr-logs/emrtest3.py')"
      ],
      "metadata": {
        "id": "hh52-EX7kkFA"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# submit spark job to emr. Make all the necessary changes to the path\n",
        "submit_job(app_name='emrtest3', pyfile_uri='s3://cs4266-finalproject-spark/emr-logs/emrtest3.py')"
      ],
      "metadata": {
        "id": "rlzi2Gy4kwDb"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# That failed miserably... let's try some other approaches"
      ],
      "metadata": {
        "id": "YSR6e2iIoXQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file emrtest3.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('emrtest3').getOrCreate()\n",
        "\n",
        "try:\n",
        "  # read in parquet file\n",
        "  # part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n",
        "  df = spark.read.option(\"mergeSchema\", \"true\").parquet(\"s3://weather_tn.parquet\")\n",
        "  df.createOrReplaceTempView(\"weather\")\n",
        "\n",
        "  # run SQL query\n",
        "  results = spark.sql('SELECT COUNT(*) FROM weather WHERE year = \"2021\" AND month = \"1\"')\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = results\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"emrtest3.out\")\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "sales_data = spark.read.format('parquet').load('s3://bucket/sales_data')\n",
        "\n",
        "# Create a temporary view\n",
        "sales_data.createOrReplaceTempView('sales')\n",
        "\n",
        "# Run a SQL query on the partitioned data\n",
        "results = spark.sql('SELECT SUM(sales_amount) FROM sales WHERE year = 2021 AND month = 01')\n",
        "  \"\"\"\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJTw98T0oaX-",
        "outputId": "46baf099-fd0c-481c-b5b5-fabe1fa7463a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting emrtest3.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s3.upload_file(Filename='emrtest3.py', Bucket='cs4266-finalproject-spark', Key='emr-logs/emrtest3.py')"
      ],
      "metadata": {
        "id": "9g5S5l2PqQuM"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# submit spark job to emr. Make all the necessary changes to the path\n",
        "submit_job(app_name='emrtest3', pyfile_uri='s3://cs4266-finalproject-spark/emr-logs/emrtest3.py')"
      ],
      "metadata": {
        "id": "0Awcjw4zqSBx"
      },
      "execution_count": 83,
      "outputs": []
    }
  ]
}