{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9lmrY/7OOUt7jPWH1BDiM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshScherer/Emergency-Response/blob/josh-dev/FiguringOutSpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kCEi2D06C0u_"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install spark. we are using the one that uses hadoop as the underlying scheduler.\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!tar xf  spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "\n",
        "#Provides findspark.init() to make pyspark importable as a regular library.\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.2.4-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 pyspark-shell'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBCqGm0GC8y0",
        "outputId": "7159e5cb-72e5-4543-8455-5cb833ab41f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 299364\n",
            "-rw-r--r--  1 root root   3369951 Apr 30 18:40 2021-1-weather.parquet\n",
            "-rw-r--r--  1 root root   1975841 Apr 30 18:42 incidents.parquet\n",
            "drwxr-xr-x  1 root root      4096 Apr 27 13:35 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Apr  9 21:17 spark-3.2.4-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 301183180 Apr  9 21:35 spark-3.2.4-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip nashville-tweets-2019-01-28.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG1JNMZ3ESBM",
        "outputId": "db57f666-9f22-49e3-e944-ee54f177b2d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  nashville-tweets-2019-01-28.zip\n",
            "  inflating: nashville-tweets-2019-01-28  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -3 nashville-tweets-2019-01-28.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg0WLNbMDVDG",
        "outputId": "8e8420b4-b683-4342-a8e9-d9d14876a5c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PK\u0003\u0004\u0014\u0000\b\u0000\b\u0000\u000bYyR\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000�\u0011~\u0001\u001b\u0000 \u0000nashville-tweets-2019-01-28UT\r\u0000\u0007v�\\`u�\\`��a`ux\u000b\u0000\u0001\u0004�\u0001\u0000\u0000\u0004\u0014\u0000\u0000\u0000�iw�8�.���\u0015HrW�\u0019\u001c�\u0000A\u0002��};�<&�㊝�N��EI�Ę\"\u0015�������\rj )J�dY���T9�\bN�\u0006�<{���� v�����_�\u0012m��l\u0006~\u001c��A\u001c�\u0011|�\u000f<\u000f��Áߴc�\u0005_�m/r�;�\u0007���M�.�پ%�F���EzA�~���aܵI7���n�\u001aW�A�����o�>��\t�p���\t�؎\u0007Q��\u001d:}ox\u001d\u0007���.ޜj\u0016ӄFMiR.uɄ\u000e����ט6�\u001a��2�\u0014\u001a�L8Զo��ͽ���n�:�\b�|9���_\u0005>~�?�L��\f]X\u0012nk�'l�Mu���\u0005(�_uWx�\u0010;e�\u0014�3j���_�v����\u000eÃ�oG����%�&�l���g9��ns���\u001a�l�\"��M?\u000b=�}�H�+���;N���/|�h�k\u0004�zC��kG���L~\u001f��c=���IO9?���rZ�p\u001c_k2�wn\u001c;!\bA��\u0001Z؞W�A�\u001c�W�\u0011�\u0016�W��\r�J��Zn������}�YI_4+Z���\n",
            "�zf�صj�n�oDx�^G]8�s����m9�u�\u001d�\u001a��a��s��K�V��yۯ|�w^f��&�d\u001a�LX#9p�\u0018��g�\u001dG}��O�Ԕ\u0014�q<�����\u000e>s���g���Vh�/7�p�z�I{jd�7à?>\u0001���Wv\u0002�l�\u0018\u0006\n",
            "B<�\u0013԰��O�Tw�\u0007`��k��_���zX�:�D5� l�>,\\���9 p��uO\r:58�)L3���(\u0005��������\u0015:���>~�����;�\u000b�VՅ����_\u0012�eh�\u0007����_����9i\u0007!q/�������e~}[�:��Tjv�g\u0005.�ס�+��\u001f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark pyspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tibabgyADCxv",
        "outputId": "d3c00e0c-1146-4941-9af6-fe110cad2acc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOCAL\n",
        "%%file local_1_count.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import json\n",
        "from operator import add\n",
        "#import other things as required\n",
        "\n",
        "#create spark context. This is very important. Do this similarly for the other parts\n",
        "# Note to read a file directly from s3 into an rdd you may have to do something like this\n",
        "\n",
        "def checkjson(entry):\n",
        "  try:\n",
        "    json.loads(entry)\n",
        "    #if load succeeded. We use correct as the key\n",
        "    return \"correct\", 1\n",
        "  except:\n",
        "    #there was an error in loading. We use incorrect as the key\n",
        "    return \"incorrect\", 1\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # replace this line with the s3 pass when testing over EMR\n",
        "  conf = SparkConf().setAppName('local_1_count').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "\n",
        "  try:\n",
        "    #@todo: fix the path as required\n",
        "    tweets=sc.textFile('/content/nashville-tweets-2019-01-28')\n",
        "    # review the page rank example for how to use the map operation\n",
        "    # review word count for reduce and add\n",
        "    # see how we use map to parse each row\n",
        "    counts = tweets.map(lambda tweet: checkjson(tweet)).reduceByKey(add)\n",
        "    print(counts.take(2))\n",
        "\n",
        "    # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "    output = counts.sortBy(lambda a: a[1], ascending=False)\n",
        "\n",
        "    # @todo: the s3 version will have to save it to correct s3 path\n",
        "    output.repartition(1).saveAsTextFile(\"local_1_count.out\")\n",
        "    \n",
        "\n",
        "  finally:\n",
        "    # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "    #finally is used to make sure the context is stopped even with errors\n",
        "    sc.stop()\n",
        "  \n",
        "\n",
        " \n",
        "  \n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6864B_IDQa8",
        "outputId": "8b872fb9-3817-486f-c7ae-454ccce20306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing local_1_count.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 local_1_count.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWGlE464FgVe",
        "outputId": "6ca63577-835a-4650-f70f-c827f436bbe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-dce22bc7-e519-41a8-b87c-92cc415d9e8c;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.4.7/spark-streaming-kafka-0-8_2.11-2.4.7.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7!spark-streaming-kafka-0-8_2.11.jar (60ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.kafka#kafka_2.11;0.8.2.1!kafka_2.11.jar (220ms)\n",
            "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
            "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (11ms)\n",
            "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.2/scala-xml_2.11-1.0.2.jar ...\n",
            "\t[SUCCESSFUL ] org.scala-lang.modules#scala-xml_2.11;1.0.2!scala-xml_2.11.jar(bundle) (54ms)\n",
            "downloading https://repo1.maven.org/maven2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar ...\n",
            "\t[SUCCESSFUL ] com.yammer.metrics#metrics-core;2.2.0!metrics-core.jar (16ms)\n",
            "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar ...\n",
            "\t[SUCCESSFUL ] org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0!scala-parser-combinators_2.11.jar(bundle) (34ms)\n",
            "downloading https://repo1.maven.org/maven2/com/101tec/zkclient/0.3/zkclient-0.3.jar ...\n",
            "\t[SUCCESSFUL ] com.101tec#zkclient;0.3!zkclient.jar (15ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;0.8.2.1!kafka-clients.jar (25ms)\n",
            "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...\n",
            "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (12ms)\n",
            "downloading https://repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar ...\n",
            "\t[SUCCESSFUL ] log4j#log4j;1.2.17!log4j.jar(bundle) (68ms)\n",
            "downloading https://repo1.maven.org/maven2/net/jpountz/lz4/lz4/1.2.0/lz4-1.2.0.jar ...\n",
            "\t[SUCCESSFUL ] net.jpountz.lz4#lz4;1.2.0!lz4.jar (18ms)\n",
            "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.7.5/snappy-java-1.1.7.5.jar ...\n",
            "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.7.5!snappy-java.jar(bundle) (117ms)\n",
            ":: resolution report :: resolve 3486ms :: artifacts dl 669ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   12  |   12  |   0   ||   12  |   12  |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-dce22bc7-e519-41a8-b87c-92cc415d9e8c\n",
            "\tconfs: [default]\n",
            "\t12 artifacts copied, 0 already retrieved (8282kB/59ms)\n",
            "23/04/30 02:55:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/30 02:55:35 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/04/30 02:55:35 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 02:55:35 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/30 02:55:35 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 02:55:35 INFO SparkContext: Submitted application: local_1_count\n",
            "23/04/30 02:55:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/30 02:55:35 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/30 02:55:35 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/30 02:55:35 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/30 02:55:35 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/30 02:55:35 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/30 02:55:35 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/30 02:55:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/30 02:55:36 INFO Utils: Successfully started service 'sparkDriver' on port 42663.\n",
            "23/04/30 02:55:36 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/30 02:55:36 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/30 02:55:36 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/30 02:55:36 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/30 02:55:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/30 02:55:36 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-211e57e0-e4d7-4c6b-aa38-2d6fd4110cfd\n",
            "23/04/30 02:55:36 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/30 02:55:36 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/30 02:55:36 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/30 02:55:37 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://fe7812865801:4040\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://fe7812865801:42663/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://fe7812865801:42663/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://fe7812865801:42663/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://fe7812865801:42663/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://fe7812865801:42663/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://fe7812865801:42663/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://fe7812865801:42663/jars/com.101tec_zkclient-0.3.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://fe7812865801:42663/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://fe7812865801:42663/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://fe7812865801:42663/jars/log4j_log4j-1.2.17.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://fe7812865801:42663/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://fe7812865801:42663/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/log4j_log4j-1.2.17.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 02:55:37 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 02:55:37 INFO Executor: Starting executor ID driver on host fe7812865801\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/log4j_log4j-1.2.17.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO TransportClientFactory: Successfully created connection to fe7812865801/172.28.0.12:42663 after 41 ms (0 ms spent in bootstraps)\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp454044287111138420.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp454044287111138420.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp3647916134763500826.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp3647916134763500826.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp5471250395108677831.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp5471250395108677831.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp970250618888269534.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp970250618888269534.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp8674847155725214929.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp8674847155725214929.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp7380504686912578160.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp7380504686912578160.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/log4j_log4j-1.2.17.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/log4j_log4j-1.2.17.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp8145504063903493070.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp8145504063903493070.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/log4j_log4j-1.2.17.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp7084789055581084624.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp7084789055581084624.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp2434420460375898848.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp2434420460375898848.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp6250372548388709988.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp6250372548388709988.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/com.101tec_zkclient-0.3.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp3691715182997485759.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp3691715182997485759.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/30 02:55:37 INFO Executor: Fetching spark://fe7812865801:42663/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682823335564\n",
            "23/04/30 02:55:37 INFO Utils: Fetching spark://fe7812865801:42663/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp2049087260221762115.tmp\n",
            "23/04/30 02:55:37 INFO Utils: /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/fetchFileTemp2049087260221762115.tmp has been previously copied to /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 02:55:37 INFO Executor: Adding file:/tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/userFiles-b818b593-4397-4041-b960-8328f8c20eae/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/30 02:55:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33917.\n",
            "23/04/30 02:55:37 INFO NettyBlockTransferService: Server created on fe7812865801:33917\n",
            "23/04/30 02:55:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/30 02:55:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fe7812865801, 33917, None)\n",
            "23/04/30 02:55:37 INFO BlockManagerMasterEndpoint: Registering block manager fe7812865801:33917 with 366.3 MiB RAM, BlockManagerId(driver, fe7812865801, 33917, None)\n",
            "23/04/30 02:55:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fe7812865801, 33917, None)\n",
            "23/04/30 02:55:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fe7812865801, 33917, None)\n",
            "23/04/30 02:55:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.3 KiB, free 366.0 MiB)\n",
            "23/04/30 02:55:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fe7812865801:33917 (size: 32.0 KiB, free: 366.3 MiB)\n",
            "23/04/30 02:55:39 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 02:55:40 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/30 02:55:40 INFO SparkContext: Starting job: runJob at PythonRDD.scala:166\n",
            "23/04/30 02:55:40 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /content/local_1_count.py:37) as input to shuffle 0\n",
            "23/04/30 02:55:40 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:166) with 1 output partitions\n",
            "23/04/30 02:55:40 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at PythonRDD.scala:166)\n",
            "23/04/30 02:55:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
            "23/04/30 02:55:40 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
            "23/04/30 02:55:41 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/local_1_count.py:37), which has no missing parents\n",
            "23/04/30 02:55:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.2 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fe7812865801:33917 (size: 7.5 KiB, free: 366.3 MiB)\n",
            "23/04/30 02:55:41 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:41 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/local_1_count.py:37) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/30 02:55:41 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "23/04/30 02:55:41 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (fe7812865801, executor driver, partition 0, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:41 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (fe7812865801, executor driver, partition 1, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:41 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "23/04/30 02:55:41 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/30 02:55:42 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:0+12519625\n",
            "23/04/30 02:55:42 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:12519625+12519626\n",
            "23/04/30 02:55:46 INFO PythonRunner: Times: total = 2872, boot = 1417, init = 578, finish = 877\n",
            "23/04/30 02:55:46 INFO PythonRunner: Times: total = 2821, boot = 1433, init = 591, finish = 797\n",
            "23/04/30 02:55:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1612 bytes result sent to driver\n",
            "23/04/30 02:55:46 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1655 bytes result sent to driver\n",
            "23/04/30 02:55:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5020 ms on fe7812865801 (executor driver) (1/2)\n",
            "23/04/30 02:55:46 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 4958 ms on fe7812865801 (executor driver) (2/2)\n",
            "23/04/30 02:55:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:46 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 52053\n",
            "23/04/30 02:55:46 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /content/local_1_count.py:37) finished in 5.613 s\n",
            "23/04/30 02:55:46 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 02:55:46 INFO DAGScheduler: running: Set()\n",
            "23/04/30 02:55:46 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
            "23/04/30 02:55:46 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 02:55:46 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at RDD at PythonRDD.scala:53), which has no missing parents\n",
            "23/04/30 02:55:46 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.2 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:46 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:46 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on fe7812865801:33917 (size: 6.1 KiB, free: 366.3 MiB)\n",
            "23/04/30 02:55:46 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[6] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 02:55:46 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/04/30 02:55:46 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (fe7812865801, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:46 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)\n",
            "23/04/30 02:55:46 INFO ShuffleBlockFetcherIterator: Getting 2 (160.0 B) non-empty blocks including 2 (160.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 27 ms\n",
            "23/04/30 02:55:46 INFO PythonRunner: Times: total = 76, boot = -955, init = 1031, finish = 0\n",
            "23/04/30 02:55:46 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 1625 bytes result sent to driver\n",
            "23/04/30 02:55:46 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 200 ms on fe7812865801 (executor driver) (1/1)\n",
            "23/04/30 02:55:46 INFO DAGScheduler: ResultStage 1 (runJob at PythonRDD.scala:166) finished in 0.224 s\n",
            "23/04/30 02:55:46 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:46 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 02:55:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "23/04/30 02:55:46 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:166, took 6.105414 s\n",
            "23/04/30 02:55:47 INFO SparkContext: Starting job: runJob at PythonRDD.scala:166\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Got job 1 (runJob at PythonRDD.scala:166) with 1 output partitions\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Final stage: ResultStage 3 (runJob at PythonRDD.scala:166)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[7] at RDD at PythonRDD.scala:53), which has no missing parents\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.2 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:47 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on fe7812865801:33917 (size: 6.1 KiB, free: 366.2 MiB)\n",
            "23/04/30 02:55:47 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (PythonRDD[7] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(1))\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (fe7812865801, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:47 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 02:55:47 INFO PythonRunner: Times: total = 74, boot = -1144, init = 1217, finish = 1\n",
            "23/04/30 02:55:47 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1572 bytes result sent to driver\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 104 ms on fe7812865801 (executor driver) (1/1)\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:47 INFO DAGScheduler: ResultStage 3 (runJob at PythonRDD.scala:166) finished in 0.126 s\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Job 1 finished: runJob at PythonRDD.scala:166, took 0.138441 s\n",
            "[('correct', 6294)]\n",
            "23/04/30 02:55:47 INFO SparkContext: Starting job: sortBy at /content/local_1_count.py:41\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Got job 2 (sortBy at /content/local_1_count.py:41) with 2 output partitions\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Final stage: ResultStage 5 (sortBy at /content/local_1_count.py:41)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting ResultStage 5 (PythonRDD[8] at sortBy at /content/local_1_count.py:41), which has no missing parents\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 11.7 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 365.9 MiB)\n",
            "23/04/30 02:55:47 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on fe7812865801:33917 (size: 6.6 KiB, free: 366.2 MiB)\n",
            "23/04/30 02:55:47 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 5 (PythonRDD[8] at sortBy at /content/local_1_count.py:41) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (fe7812865801, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 5) (fe7812865801, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:47 INFO Executor: Running task 1.0 in stage 5.0 (TID 5)\n",
            "23/04/30 02:55:47 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Getting 2 (160.0 B) non-empty blocks including 2 (160.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms\n",
            "23/04/30 02:55:47 INFO PythonRunner: Times: total = 138, boot = -329, init = 467, finish = 0\n",
            "23/04/30 02:55:47 INFO Executor: Finished task 1.0 in stage 5.0 (TID 5). 1612 bytes result sent to driver\n",
            "23/04/30 02:55:47 INFO PythonRunner: Times: total = 136, boot = -126, init = 262, finish = 0\n",
            "23/04/30 02:55:47 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 1612 bytes result sent to driver\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 5) in 187 ms on fe7812865801 (executor driver) (1/2)\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 188 ms on fe7812865801 (executor driver) (2/2)\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:47 INFO DAGScheduler: ResultStage 5 (sortBy at /content/local_1_count.py:41) finished in 0.211 s\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Job 2 finished: sortBy at /content/local_1_count.py:41, took 0.219877 s\n",
            "23/04/30 02:55:47 INFO SparkContext: Starting job: sortBy at /content/local_1_count.py:41\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Got job 3 (sortBy at /content/local_1_count.py:41) with 2 output partitions\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Final stage: ResultStage 7 (sortBy at /content/local_1_count.py:41)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting ResultStage 7 (PythonRDD[9] at sortBy at /content/local_1_count.py:41), which has no missing parents\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 11.0 KiB, free 365.8 MiB)\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 365.8 MiB)\n",
            "23/04/30 02:55:47 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on fe7812865801:33917 (size: 6.4 KiB, free: 366.2 MiB)\n",
            "23/04/30 02:55:47 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 7 (PythonRDD[9] at sortBy at /content/local_1_count.py:41) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Adding task set 7.0 with 2 tasks resource profile 0\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (fe7812865801, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 7) (fe7812865801, executor driver, partition 1, PROCESS_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:47 INFO Executor: Running task 1.0 in stage 7.0 (TID 7)\n",
            "23/04/30 02:55:47 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Getting 2 (160.0 B) non-empty blocks including 2 (160.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms\n",
            "23/04/30 02:55:47 INFO PythonRunner: Times: total = 135, boot = -58, init = 193, finish = 0\n",
            "23/04/30 02:55:47 INFO PythonRunner: Times: total = 128, boot = -84, init = 212, finish = 0\n",
            "23/04/30 02:55:47 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 1613 bytes result sent to driver\n",
            "23/04/30 02:55:47 INFO Executor: Finished task 1.0 in stage 7.0 (TID 7). 1572 bytes result sent to driver\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 181 ms on fe7812865801 (executor driver) (1/2)\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 7) in 182 ms on fe7812865801 (executor driver) (2/2)\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:47 INFO DAGScheduler: ResultStage 7 (sortBy at /content/local_1_count.py:41) finished in 0.197 s\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Job 3 finished: sortBy at /content/local_1_count.py:41, took 0.205789 s\n",
            "23/04/30 02:55:47 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "23/04/30 02:55:47 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/30 02:55:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 02:55:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 02:55:47 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Registering RDD 11 (sortBy at /content/local_1_count.py:41) as input to shuffle 2\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Registering RDD 15 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Got job 4 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Final stage: ResultStage 11 (runJob at SparkHadoopWriter.scala:83)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 10)\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting ShuffleMapStage 9 (PairwiseRDD[11] at sortBy at /content/local_1_count.py:41), which has no missing parents\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 11.8 KiB, free 365.8 MiB)\n",
            "23/04/30 02:55:47 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 365.8 MiB)\n",
            "23/04/30 02:55:47 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on fe7812865801:33917 (size: 7.2 KiB, free: 366.2 MiB)\n",
            "23/04/30 02:55:47 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:47 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (PairwiseRDD[11] at sortBy at /content/local_1_count.py:41) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/30 02:55:47 INFO TaskSchedulerImpl: Adding task set 9.0 with 2 tasks resource profile 0\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 8) (fe7812865801, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:47 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 9) (fe7812865801, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:47 INFO Executor: Running task 0.0 in stage 9.0 (TID 8)\n",
            "23/04/30 02:55:47 INFO Executor: Running task 1.0 in stage 9.0 (TID 9)\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Getting 2 (160.0 B) non-empty blocks including 2 (160.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms\n",
            "23/04/30 02:55:48 INFO PythonRunner: Times: total = 132, boot = -258, init = 390, finish = 0\n",
            "23/04/30 02:55:48 INFO Executor: Finished task 0.0 in stage 9.0 (TID 8). 1784 bytes result sent to driver\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 8) in 173 ms on fe7812865801 (executor driver) (1/2)\n",
            "23/04/30 02:55:48 INFO PythonRunner: Times: total = 170, boot = -290, init = 460, finish = 0\n",
            "23/04/30 02:55:48 INFO Executor: Finished task 1.0 in stage 9.0 (TID 9). 1655 bytes result sent to driver\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 9) in 214 ms on fe7812865801 (executor driver) (2/2)\n",
            "23/04/30 02:55:48 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:48 INFO DAGScheduler: ShuffleMapStage 9 (sortBy at /content/local_1_count.py:41) finished in 0.229 s\n",
            "23/04/30 02:55:48 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 02:55:48 INFO DAGScheduler: running: Set()\n",
            "23/04/30 02:55:48 INFO DAGScheduler: waiting: Set(ShuffleMapStage 10, ResultStage 11)\n",
            "23/04/30 02:55:48 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 02:55:48 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[15] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 02:55:48 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 10.2 KiB, free 365.8 MiB)\n",
            "23/04/30 02:55:48 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 365.8 MiB)\n",
            "23/04/30 02:55:48 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on fe7812865801:33917 (size: 6.0 KiB, free: 366.2 MiB)\n",
            "23/04/30 02:55:48 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[15] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/30 02:55:48 INFO TaskSchedulerImpl: Adding task set 10.0 with 2 tasks resource profile 0\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 10) (fe7812865801, executor driver, partition 1, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 11) (fe7812865801, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:48 INFO Executor: Running task 1.0 in stage 10.0 (TID 10)\n",
            "23/04/30 02:55:48 INFO Executor: Running task 0.0 in stage 10.0 (TID 11)\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms\n",
            "23/04/30 02:55:48 INFO PythonRunner: Times: total = 129, boot = -107, init = 236, finish = 0\n",
            "23/04/30 02:55:48 INFO Executor: Finished task 0.0 in stage 10.0 (TID 11). 1654 bytes result sent to driver\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 11) in 182 ms on fe7812865801 (executor driver) (1/2)\n",
            "23/04/30 02:55:48 INFO PythonRunner: Times: total = 151, boot = -56, init = 207, finish = 0\n",
            "23/04/30 02:55:48 INFO Executor: Finished task 1.0 in stage 10.0 (TID 10). 1783 bytes result sent to driver\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 10) in 218 ms on fe7812865801 (executor driver) (2/2)\n",
            "23/04/30 02:55:48 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:48 INFO DAGScheduler: ShuffleMapStage 10 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.250 s\n",
            "23/04/30 02:55:48 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 02:55:48 INFO DAGScheduler: running: Set()\n",
            "23/04/30 02:55:48 INFO DAGScheduler: waiting: Set(ResultStage 11)\n",
            "23/04/30 02:55:48 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 02:55:48 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[21] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 02:55:48 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 105.9 KiB, free 365.7 MiB)\n",
            "23/04/30 02:55:48 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 39.7 KiB, free 365.7 MiB)\n",
            "23/04/30 02:55:48 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on fe7812865801:33917 (size: 39.7 KiB, free: 366.2 MiB)\n",
            "23/04/30 02:55:48 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 02:55:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[21] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 02:55:48 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 12) (fe7812865801, executor driver, partition 0, NODE_LOCAL, 4547 bytes) taskResourceAssignments Map()\n",
            "23/04/30 02:55:48 INFO Executor: Running task 0.0 in stage 11.0 (TID 12)\n",
            "23/04/30 02:55:48 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/30 02:55:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 02:55:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 02:55:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms\n",
            "23/04/30 02:55:48 INFO PythonRunner: Times: total = 145, boot = -213, init = 358, finish = 0\n",
            "23/04/30 02:55:48 INFO FileOutputCommitter: Saved output of task 'attempt_202304300255471325538644976917902_0021_m_000000_0' to file:/content/local_1_count.out/_temporary/0/task_202304300255471325538644976917902_0021_m_000000\n",
            "23/04/30 02:55:48 INFO SparkHadoopMapRedUtil: attempt_202304300255471325538644976917902_0021_m_000000_0: Committed\n",
            "23/04/30 02:55:48 INFO Executor: Finished task 0.0 in stage 11.0 (TID 12). 1952 bytes result sent to driver\n",
            "23/04/30 02:55:48 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 12) in 315 ms on fe7812865801 (executor driver) (1/1)\n",
            "23/04/30 02:55:48 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
            "23/04/30 02:55:48 INFO DAGScheduler: ResultStage 11 (runJob at SparkHadoopWriter.scala:83) finished in 0.369 s\n",
            "23/04/30 02:55:48 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 02:55:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
            "23/04/30 02:55:48 INFO DAGScheduler: Job 4 finished: runJob at SparkHadoopWriter.scala:83, took 0.884339 s\n",
            "23/04/30 02:55:48 INFO SparkHadoopWriter: Start to commit write Job job_202304300255471325538644976917902_0021.\n",
            "23/04/30 02:55:48 INFO SparkHadoopWriter: Write Job job_202304300255471325538644976917902_0021 committed. Elapsed time: 18 ms.\n",
            "23/04/30 02:55:48 INFO SparkUI: Stopped Spark web UI at http://fe7812865801:4040\n",
            "23/04/30 02:55:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/30 02:55:48 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/30 02:55:48 INFO BlockManager: BlockManager stopped\n",
            "23/04/30 02:55:48 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/30 02:55:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/30 02:55:48 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/30 02:55:49 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/30 02:55:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a\n",
            "23/04/30 02:55:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-73213582-d3dc-4f67-b966-404ac352b724\n",
            "23/04/30 02:55:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-b66d46dc-2a53-456d-ac6d-a73a0d8a521a/pyspark-3bfb6939-d3f0-4839-98e2-c8d3755da6ac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test1(lines):\n",
        "  if '6294' in lines[0] and 'correct' in lines[0]:\n",
        "    print(\"passed\")\n",
        "  else:\n",
        "    print(\"failed\")\n",
        "\n",
        "# test local execution results\n",
        "with open('local_1_count.out/part-00000') as f:\n",
        "  lines = f.readlines()\n",
        "  print(lines)\n",
        "  test1(lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKQKQItEQRIB",
        "outputId": "ceb843f9-bfc4-47ee-da62-60e5f28af425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"('correct', 6294)\\n\"]\n",
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We have verified that the old homework question works with our EMR and S3 buckets...\n",
        "\n",
        "### Now, let's move on to trying to read in a basic parquet file"
      ],
      "metadata": {
        "id": "k9-oFWK1RHEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New day, new attempts\n",
        "\n",
        "THESE ARE STARTING FROM SCRATCH... NO PREVIOUS CODE WAS USED"
      ],
      "metadata": {
        "id": "iyKGPQMNUbls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3\n",
        "import boto3, json\n",
        "\n",
        "credentials = {\n",
        "    'region_name': 'us-east-1',\n",
        "    'aws_access_key_id': 'ASIAYELUFHJMQOGUHUKQ',\n",
        "    'aws_secret_access_key': 'blG5GvO6Pr30vKdqgmdjK9cvRurGiJY+xIv183Vr',\n",
        "    'aws_session_token': 'FwoGZXIvYXdzEJv//////////wEaDE/qPtJe0PYOzk6GoCLMAbgdYvgPC9ekgfACzT1ArxtDulNLEHxrHlcPRvBNTATNCPLnz2Vg9+UXHdlrzF27jKnOL4gTXwXQVrjNMTqv9dhnAOnW8xRC+/mFJF5Up7BlsUdoSGxHr/fxCIcBk2XtD1hocdPFAyVefOYj9NOzyS0IbaPzPPM4pJLrV/yt8xazR4UYEuCU8111iGeWK/XT+sICzA1HBXSMoayJFlQpsxaWSn072bmEVT0iL5NcX9vZyMQMuAP7OGYOjNoJ+jwy973FtGW1j089/9y4Uij807qiBjItAbZPOBiV9zi2qrS1OKi1nIuC1ZMX1381JBxZXswBGa4gJLR732084sFZCZZE'\n",
        "}\n",
        "\n",
        "session = boto3.session.Session(**credentials)\n",
        "s3 = session.client('s3')"
      ],
      "metadata": {
        "id": "zlkj5_A4WXUN",
        "outputId": "7b48e67a-23de-4960-da56-14583bead723",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.123-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.123\n",
            "  Downloading botocore-1.29.123-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.123->boto3) (1.26.15)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.123->boto3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.123->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.26.123 botocore-1.29.123 jmespath-1.0.1 s3transfer-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The following works, meaning that we can store everything locally"
      ],
      "metadata": {
        "id": "QcUZkznPdiBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First I want to run a test to see if we can download all of the data locally\n",
        "num_years = 13\n",
        "num_months = 12\n",
        "\n",
        "years_list = [2010 + i for i in range(num_years)]\n",
        "months_list = [i for i in range(1, 13)]\n",
        "print(years_list)\n",
        "print(months_list)\n",
        "\n",
        "full_path = 'weather_tn.parquet/year=2021/month=1/part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet'\n",
        "for year in years_list:\n",
        "  for month in months_list:\n",
        "    local_name =  str(year) + '-' + str(month) +'-weather.parquet'\n",
        "    s3.download_file('cs4266-finalproject', full_path, local_name)\n",
        "    print(\"Downloaded\", local_name)"
      ],
      "metadata": {
        "id": "EzCYUgMvUdsW",
        "outputId": "e41525cc-15fa-42ec-88b8-01601205175c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
            "Downloaded 2010-1-weather.parquet\n",
            "Downloaded 2010-2-weather.parquet\n",
            "Downloaded 2010-3-weather.parquet\n",
            "Downloaded 2010-4-weather.parquet\n",
            "Downloaded 2010-5-weather.parquet\n",
            "Downloaded 2010-6-weather.parquet\n",
            "Downloaded 2010-7-weather.parquet\n",
            "Downloaded 2010-8-weather.parquet\n",
            "Downloaded 2010-9-weather.parquet\n",
            "Downloaded 2010-10-weather.parquet\n",
            "Downloaded 2010-11-weather.parquet\n",
            "Downloaded 2010-12-weather.parquet\n",
            "Downloaded 2011-1-weather.parquet\n",
            "Downloaded 2011-2-weather.parquet\n",
            "Downloaded 2011-3-weather.parquet\n",
            "Downloaded 2011-4-weather.parquet\n",
            "Downloaded 2011-5-weather.parquet\n",
            "Downloaded 2011-6-weather.parquet\n",
            "Downloaded 2011-7-weather.parquet\n",
            "Downloaded 2011-8-weather.parquet\n",
            "Downloaded 2011-9-weather.parquet\n",
            "Downloaded 2011-10-weather.parquet\n",
            "Downloaded 2011-11-weather.parquet\n",
            "Downloaded 2011-12-weather.parquet\n",
            "Downloaded 2012-1-weather.parquet\n",
            "Downloaded 2012-2-weather.parquet\n",
            "Downloaded 2012-3-weather.parquet\n",
            "Downloaded 2012-4-weather.parquet\n",
            "Downloaded 2012-5-weather.parquet\n",
            "Downloaded 2012-6-weather.parquet\n",
            "Downloaded 2012-7-weather.parquet\n",
            "Downloaded 2012-8-weather.parquet\n",
            "Downloaded 2012-9-weather.parquet\n",
            "Downloaded 2012-10-weather.parquet\n",
            "Downloaded 2012-11-weather.parquet\n",
            "Downloaded 2012-12-weather.parquet\n",
            "Downloaded 2013-1-weather.parquet\n",
            "Downloaded 2013-2-weather.parquet\n",
            "Downloaded 2013-3-weather.parquet\n",
            "Downloaded 2013-4-weather.parquet\n",
            "Downloaded 2013-5-weather.parquet\n",
            "Downloaded 2013-6-weather.parquet\n",
            "Downloaded 2013-7-weather.parquet\n",
            "Downloaded 2013-8-weather.parquet\n",
            "Downloaded 2013-9-weather.parquet\n",
            "Downloaded 2013-10-weather.parquet\n",
            "Downloaded 2013-11-weather.parquet\n",
            "Downloaded 2013-12-weather.parquet\n",
            "Downloaded 2014-1-weather.parquet\n",
            "Downloaded 2014-2-weather.parquet\n",
            "Downloaded 2014-3-weather.parquet\n",
            "Downloaded 2014-4-weather.parquet\n",
            "Downloaded 2014-5-weather.parquet\n",
            "Downloaded 2014-6-weather.parquet\n",
            "Downloaded 2014-7-weather.parquet\n",
            "Downloaded 2014-8-weather.parquet\n",
            "Downloaded 2014-9-weather.parquet\n",
            "Downloaded 2014-10-weather.parquet\n",
            "Downloaded 2014-11-weather.parquet\n",
            "Downloaded 2014-12-weather.parquet\n",
            "Downloaded 2015-1-weather.parquet\n",
            "Downloaded 2015-2-weather.parquet\n",
            "Downloaded 2015-3-weather.parquet\n",
            "Downloaded 2015-4-weather.parquet\n",
            "Downloaded 2015-5-weather.parquet\n",
            "Downloaded 2015-6-weather.parquet\n",
            "Downloaded 2015-7-weather.parquet\n",
            "Downloaded 2015-8-weather.parquet\n",
            "Downloaded 2015-9-weather.parquet\n",
            "Downloaded 2015-10-weather.parquet\n",
            "Downloaded 2015-11-weather.parquet\n",
            "Downloaded 2015-12-weather.parquet\n",
            "Downloaded 2016-1-weather.parquet\n",
            "Downloaded 2016-2-weather.parquet\n",
            "Downloaded 2016-3-weather.parquet\n",
            "Downloaded 2016-4-weather.parquet\n",
            "Downloaded 2016-5-weather.parquet\n",
            "Downloaded 2016-6-weather.parquet\n",
            "Downloaded 2016-7-weather.parquet\n",
            "Downloaded 2016-8-weather.parquet\n",
            "Downloaded 2016-9-weather.parquet\n",
            "Downloaded 2016-10-weather.parquet\n",
            "Downloaded 2016-11-weather.parquet\n",
            "Downloaded 2016-12-weather.parquet\n",
            "Downloaded 2017-1-weather.parquet\n",
            "Downloaded 2017-2-weather.parquet\n",
            "Downloaded 2017-3-weather.parquet\n",
            "Downloaded 2017-4-weather.parquet\n",
            "Downloaded 2017-5-weather.parquet\n",
            "Downloaded 2017-6-weather.parquet\n",
            "Downloaded 2017-7-weather.parquet\n",
            "Downloaded 2017-8-weather.parquet\n",
            "Downloaded 2017-9-weather.parquet\n",
            "Downloaded 2017-10-weather.parquet\n",
            "Downloaded 2017-11-weather.parquet\n",
            "Downloaded 2017-12-weather.parquet\n",
            "Downloaded 2018-1-weather.parquet\n",
            "Downloaded 2018-2-weather.parquet\n",
            "Downloaded 2018-3-weather.parquet\n",
            "Downloaded 2018-4-weather.parquet\n",
            "Downloaded 2018-5-weather.parquet\n",
            "Downloaded 2018-6-weather.parquet\n",
            "Downloaded 2018-7-weather.parquet\n",
            "Downloaded 2018-8-weather.parquet\n",
            "Downloaded 2018-9-weather.parquet\n",
            "Downloaded 2018-10-weather.parquet\n",
            "Downloaded 2018-11-weather.parquet\n",
            "Downloaded 2018-12-weather.parquet\n",
            "Downloaded 2019-1-weather.parquet\n",
            "Downloaded 2019-2-weather.parquet\n",
            "Downloaded 2019-3-weather.parquet\n",
            "Downloaded 2019-4-weather.parquet\n",
            "Downloaded 2019-5-weather.parquet\n",
            "Downloaded 2019-6-weather.parquet\n",
            "Downloaded 2019-7-weather.parquet\n",
            "Downloaded 2019-8-weather.parquet\n",
            "Downloaded 2019-9-weather.parquet\n",
            "Downloaded 2019-10-weather.parquet\n",
            "Downloaded 2019-11-weather.parquet\n",
            "Downloaded 2019-12-weather.parquet\n",
            "Downloaded 2020-1-weather.parquet\n",
            "Downloaded 2020-2-weather.parquet\n",
            "Downloaded 2020-3-weather.parquet\n",
            "Downloaded 2020-4-weather.parquet\n",
            "Downloaded 2020-5-weather.parquet\n",
            "Downloaded 2020-6-weather.parquet\n",
            "Downloaded 2020-7-weather.parquet\n",
            "Downloaded 2020-8-weather.parquet\n",
            "Downloaded 2020-9-weather.parquet\n",
            "Downloaded 2020-10-weather.parquet\n",
            "Downloaded 2020-11-weather.parquet\n",
            "Downloaded 2020-12-weather.parquet\n",
            "Downloaded 2021-1-weather.parquet\n",
            "Downloaded 2021-2-weather.parquet\n",
            "Downloaded 2021-3-weather.parquet\n",
            "Downloaded 2021-4-weather.parquet\n",
            "Downloaded 2021-5-weather.parquet\n",
            "Downloaded 2021-6-weather.parquet\n",
            "Downloaded 2021-7-weather.parquet\n",
            "Downloaded 2021-8-weather.parquet\n",
            "Downloaded 2021-9-weather.parquet\n",
            "Downloaded 2021-10-weather.parquet\n",
            "Downloaded 2021-11-weather.parquet\n",
            "Downloaded 2021-12-weather.parquet\n",
            "Downloaded 2022-1-weather.parquet\n",
            "Downloaded 2022-2-weather.parquet\n",
            "Downloaded 2022-3-weather.parquet\n",
            "Downloaded 2022-4-weather.parquet\n",
            "Downloaded 2022-5-weather.parquet\n",
            "Downloaded 2022-6-weather.parquet\n",
            "Downloaded 2022-7-weather.parquet\n",
            "Downloaded 2022-8-weather.parquet\n",
            "Downloaded 2022-9-weather.parquet\n",
            "Downloaded 2022-10-weather.parquet\n",
            "Downloaded 2022-11-weather.parquet\n",
            "Downloaded 2022-12-weather.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now we want to see if we can join incidents and weather bucketed by hour\n",
        "\n",
        "Start by downloading weather for January of 2021"
      ],
      "metadata": {
        "id": "ku7acXZ0d7ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_path = 'weather_tn.parquet/year=2021/month=1/part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet'\n",
        "\n",
        "local_name = '2021-1-weather.parquet'\n",
        "s3.download_file('cs4266-finalproject', full_path, local_name)\n",
        "print(\"Downloaded\", local_name)"
      ],
      "metadata": {
        "id": "qNVlAxdUZMw3",
        "outputId": "7116e7ee-6ae2-44eb-9cd5-835268ab25d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 2021-1-weather.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now download the incidents file"
      ],
      "metadata": {
        "id": "ZIcGeigNeDDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_path = 'nfd_incidents_xd_seg.parquet'\n",
        "\n",
        "local_name = 'incidents.parquet'\n",
        "s3.download_file('cs4266-finalproject', full_path, local_name)\n",
        "print(\"Downloaded\", local_name)"
      ],
      "metadata": {
        "id": "ZMz3sS3fd51H",
        "outputId": "c8b07dc2-9cb1-4b87-ebfc-3195a7372008",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded incidents.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "608AcUfueRZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple test on weather data"
      ],
      "metadata": {
        "id": "6_ngEQwyjFIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file localtest2.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('localtest2').getOrCreate()\n",
        "\n",
        "try:\n",
        "  # read in parquet file\n",
        "  # part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n",
        "  tweets_df = spark.read.parquet('2021-1-weather.parquet')\n",
        "  tweets_df.createOrReplaceTempView(\"weather\")\n",
        "\n",
        "  # run SQL query\n",
        "  counts = spark.sql('SELECT COUNT(*) FROM weather')\n",
        "  print(counts.show())\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = counts#.sort(\"count\", ascending=False)\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"localtest2.out\")\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass"
      ],
      "metadata": {
        "id": "BmFaqk7feiba",
        "outputId": "5ceca35e-7128-47df-d6c0-71b05985e565",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting localtest2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 localtest2.py"
      ],
      "metadata": {
        "id": "dNrEC-J6fBqY",
        "outputId": "93ed157b-778b-49b4-807f-a78069f7a684",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c35c0565-c994-46ae-8a08-7cd28938a725;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 914ms :: artifacts dl 59ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-c35c0565-c994-46ae-8a08-7cd28938a725\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/26ms)\n",
            "23/04/30 19:01:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/30 19:01:56 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/04/30 19:01:56 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 19:01:56 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/30 19:01:56 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 19:01:56 INFO SparkContext: Submitted application: localtest2\n",
            "23/04/30 19:01:56 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/30 19:01:56 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/30 19:01:56 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/30 19:01:56 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/30 19:01:56 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/30 19:01:56 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/30 19:01:56 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/30 19:01:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/30 19:01:57 INFO Utils: Successfully started service 'sparkDriver' on port 38317.\n",
            "23/04/30 19:01:57 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/30 19:01:57 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/30 19:01:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/30 19:01:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/30 19:01:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/30 19:01:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1aa6f82a-d68e-4561-9899-c586822bb6b8\n",
            "23/04/30 19:01:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/30 19:01:57 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/30 19:01:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/30 19:01:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://145f628fbc4b:4040\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://145f628fbc4b:38317/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://145f628fbc4b:38317/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://145f628fbc4b:38317/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://145f628fbc4b:38317/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://145f628fbc4b:38317/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://145f628fbc4b:38317/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://145f628fbc4b:38317/jars/com.101tec_zkclient-0.3.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://145f628fbc4b:38317/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://145f628fbc4b:38317/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://145f628fbc4b:38317/jars/log4j_log4j-1.2.17.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://145f628fbc4b:38317/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://145f628fbc4b:38317/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/log4j_log4j-1.2.17.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 19:01:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 19:01:58 INFO Executor: Starting executor ID driver on host 145f628fbc4b\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/log4j_log4j-1.2.17.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:58 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 19:01:58 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO TransportClientFactory: Successfully created connection to 145f628fbc4b/172.28.0.12:38317 after 71 ms (0 ms spent in bootstraps)\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp7540423681872739871.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp7540423681872739871.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp6755531432535173981.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp6755531432535173981.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/com.101tec_zkclient-0.3.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp5743219548581484238.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp5743219548581484238.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp4272695134485183491.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp4272695134485183491.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp5634625261822589726.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp5634625261822589726.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp407738289051001962.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp407738289051001962.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp595870644550565025.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp595870644550565025.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp3507976221332203627.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp3507976221332203627.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp5911555324453383978.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp5911555324453383978.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/log4j_log4j-1.2.17.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/log4j_log4j-1.2.17.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp1765298295228190302.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp1765298295228190302.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/log4j_log4j-1.2.17.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp7904445938667965601.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp7904445938667965601.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/30 19:01:59 INFO Executor: Fetching spark://145f628fbc4b:38317/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682881316594\n",
            "23/04/30 19:01:59 INFO Utils: Fetching spark://145f628fbc4b:38317/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp384757528685471959.tmp\n",
            "23/04/30 19:01:59 INFO Utils: /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/fetchFileTemp384757528685471959.tmp has been previously copied to /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 19:01:59 INFO Executor: Adding file:/tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/userFiles-3a991d95-238e-4d09-b95a-cfcaddd8be70/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/30 19:01:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46421.\n",
            "23/04/30 19:01:59 INFO NettyBlockTransferService: Server created on 145f628fbc4b:46421\n",
            "23/04/30 19:01:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/30 19:01:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 145f628fbc4b, 46421, None)\n",
            "23/04/30 19:01:59 INFO BlockManagerMasterEndpoint: Registering block manager 145f628fbc4b:46421 with 366.3 MiB RAM, BlockManagerId(driver, 145f628fbc4b, 46421, None)\n",
            "23/04/30 19:01:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 145f628fbc4b, 46421, None)\n",
            "23/04/30 19:01:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 145f628fbc4b, 46421, None)\n",
            "23/04/30 19:02:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/04/30 19:02:00 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/04/30 19:02:02 INFO InMemoryFileIndex: It took 56 ms to list leaf files for 1 paths.\n",
            "23/04/30 19:02:03 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 19:02:03 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:02:03 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:02:03 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 19:02:03 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:02:03 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:02:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/04/30 19:02:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/04/30 19:02:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 145f628fbc4b:46421 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 19:02:03 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:02:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:02:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:02:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4584 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:02:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/30 19:02:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3118 bytes result sent to driver\n",
            "23/04/30 19:02:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1114 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:02:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:02:05 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.460 s\n",
            "23/04/30 19:02:05 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 19:02:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/30 19:02:05 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.551314 s\n",
            "23/04/30 19:02:05 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 145f628fbc4b:46421 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 19:02:08 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "23/04/30 19:02:09 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/30 19:02:09 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "23/04/30 19:02:09 INFO FileSourceStrategy: Output Data Schema: struct<>\n",
            "23/04/30 19:02:09 INFO CodeGenerator: Code generated in 251.496979 ms\n",
            "23/04/30 19:02:09 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 351.9 KiB, free 366.0 MiB)\n",
            "23/04/30 19:02:09 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.5 KiB, free 365.9 MiB)\n",
            "23/04/30 19:02:09 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 145f628fbc4b:46421 (size: 35.5 KiB, free: 366.3 MiB)\n",
            "23/04/30 19:02:09 INFO SparkContext: Created broadcast 1 from showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 19:02:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/30 19:02:09 INFO DAGScheduler: Registering RDD 5 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/30 19:02:09 INFO DAGScheduler: Got map stage job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:02:09 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:02:09 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 19:02:09 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:02:09 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:02:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 15.2 KiB, free 365.9 MiB)\n",
            "23/04/30 19:02:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 365.9 MiB)\n",
            "23/04/30 19:02:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 145f628fbc4b:46421 (size: 7.2 KiB, free: 366.3 MiB)\n",
            "23/04/30 19:02:09 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:02:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:02:09 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:02:09 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4852 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:02:09 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/30 19:02:10 INFO FileScanRDD: Reading File path: file:///content/2021-1-weather.parquet, range: 0-3369951, partition values: [empty row]\n",
            "23/04/30 19:02:10 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2167 bytes result sent to driver\n",
            "23/04/30 19:02:10 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 430 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:02:10 INFO DAGScheduler: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.535 s\n",
            "23/04/30 19:02:10 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 19:02:10 INFO DAGScheduler: running: Set()\n",
            "23/04/30 19:02:10 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 19:02:10 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 19:02:10 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:02:10 INFO CodeGenerator: Code generated in 51.919664 ms\n",
            "23/04/30 19:02:10 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 19:02:10 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:02:10 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:02:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/30 19:02:10 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:02:10 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:02:10 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.1 KiB, free 365.9 MiB)\n",
            "23/04/30 19:02:10 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 365.9 MiB)\n",
            "23/04/30 19:02:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 145f628fbc4b:46421 (size: 5.5 KiB, free: 366.3 MiB)\n",
            "23/04/30 19:02:10 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:02:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:02:10 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:02:10 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:02:10 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
            "23/04/30 19:02:10 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 19:02:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 22 ms\n",
            "23/04/30 19:02:10 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 2657 bytes result sent to driver\n",
            "23/04/30 19:02:10 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 104 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:02:10 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:02:10 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.138 s\n",
            "23/04/30 19:02:10 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 19:02:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/30 19:02:10 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.164881 s\n",
            "23/04/30 19:02:10 INFO CodeGenerator: Code generated in 33.796646 ms\n",
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|  144336|\n",
            "+--------+\n",
            "\n",
            "None\n",
            "23/04/30 19:02:10 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/30 19:02:10 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "23/04/30 19:02:10 INFO FileSourceStrategy: Output Data Schema: struct<>\n",
            "23/04/30 19:02:11 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 351.9 KiB, free 365.5 MiB)\n",
            "23/04/30 19:02:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.5 KiB, free 365.5 MiB)\n",
            "23/04/30 19:02:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 145f628fbc4b:46421 (size: 35.5 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:02:11 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 19:02:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Registering RDD 12 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Got map stage job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[12] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:02:11 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.2 KiB, free 365.5 MiB)\n",
            "23/04/30 19:02:11 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 365.5 MiB)\n",
            "23/04/30 19:02:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 145f628fbc4b:46421 (size: 7.2 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:02:11 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[12] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:02:11 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:02:11 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4852 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:02:11 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)\n",
            "23/04/30 19:02:11 INFO FileScanRDD: Reading File path: file:///content/2021-1-weather.parquet, range: 0-3369951, partition values: [empty row]\n",
            "23/04/30 19:02:11 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 2124 bytes result sent to driver\n",
            "23/04/30 19:02:11 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 64 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:02:11 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:02:11 INFO DAGScheduler: ShuffleMapStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 0.086 s\n",
            "23/04/30 19:02:11 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 19:02:11 INFO DAGScheduler: running: Set()\n",
            "23/04/30 19:02:11 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 19:02:11 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 19:02:11 INFO CodeGenerator: Code generated in 36.552302 ms\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Registering RDD 15 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Got map stage job 4 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:02:11 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 11.8 KiB, free 365.5 MiB)\n",
            "23/04/30 19:02:11 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 365.5 MiB)\n",
            "23/04/30 19:02:11 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 145f628fbc4b:46421 (size: 6.0 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:02:11 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:02:11 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:02:11 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:02:11 INFO Executor: Running task 0.0 in stage 6.0 (TID 4)\n",
            "23/04/30 19:02:11 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 19:02:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms\n",
            "23/04/30 19:02:11 INFO Executor: Finished task 0.0 in stage 6.0 (TID 4). 2839 bytes result sent to driver\n",
            "23/04/30 19:02:11 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 24 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:02:11 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:02:11 INFO DAGScheduler: ShuffleMapStage 6 (csv at NativeMethodAccessorImpl.java:0) finished in 0.049 s\n",
            "23/04/30 19:02:11 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 19:02:11 INFO DAGScheduler: running: Set()\n",
            "23/04/30 19:02:11 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 19:02:11 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 19:02:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 19:02:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 19:02:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 19:02:11 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Got job 5 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Final stage: ResultStage 9 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Submitting ResultStage 9 (ShuffledRowRDD[16] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:02:11 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 211.2 KiB, free 365.3 MiB)\n",
            "23/04/30 19:02:11 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 75.5 KiB, free 365.2 MiB)\n",
            "23/04/30 19:02:11 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 145f628fbc4b:46421 (size: 75.5 KiB, free: 366.1 MiB)\n",
            "23/04/30 19:02:11 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:02:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (ShuffledRowRDD[16] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:02:11 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:02:11 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 5) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:02:11 INFO Executor: Running task 0.0 in stage 9.0 (TID 5)\n",
            "23/04/30 19:02:11 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 19:02:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 19:02:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 19:02:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 19:02:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 19:02:11 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 145f628fbc4b:46421 in memory (size: 6.0 KiB, free: 366.1 MiB)\n",
            "23/04/30 19:02:11 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 145f628fbc4b:46421 in memory (size: 5.5 KiB, free: 366.1 MiB)\n",
            "23/04/30 19:02:11 INFO FileOutputCommitter: Saved output of task 'attempt_202304301902111307220367332770486_0009_m_000000_5' to file:/content/localtest2.out/_temporary/0/task_202304301902111307220367332770486_0009_m_000000\n",
            "23/04/30 19:02:11 INFO SparkHadoopMapRedUtil: attempt_202304301902111307220367332770486_0009_m_000000_5: Committed\n",
            "23/04/30 19:02:12 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 145f628fbc4b:46421 in memory (size: 35.5 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:02:12 INFO Executor: Finished task 0.0 in stage 9.0 (TID 5). 3526 bytes result sent to driver\n",
            "23/04/30 19:02:12 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 5) in 464 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:02:12 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:02:12 INFO DAGScheduler: ResultStage 9 (csv at NativeMethodAccessorImpl.java:0) finished in 0.534 s\n",
            "23/04/30 19:02:12 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 19:02:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "23/04/30 19:02:12 INFO DAGScheduler: Job 5 finished: csv at NativeMethodAccessorImpl.java:0, took 0.542768 s\n",
            "23/04/30 19:02:12 INFO FileFormatWriter: Start to commit write Job e7e557cd-c9d8-4cb4-b347-da4455ba4b84.\n",
            "23/04/30 19:02:12 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 145f628fbc4b:46421 in memory (size: 7.2 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:02:12 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 145f628fbc4b:46421 in memory (size: 7.2 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:02:12 INFO FileFormatWriter: Write Job e7e557cd-c9d8-4cb4-b347-da4455ba4b84 committed. Elapsed time: 42 ms.\n",
            "23/04/30 19:02:12 INFO FileFormatWriter: Finished processing stats for write job e7e557cd-c9d8-4cb4-b347-da4455ba4b84.\n",
            "23/04/30 19:02:12 INFO SparkUI: Stopped Spark web UI at http://145f628fbc4b:4040\n",
            "23/04/30 19:02:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/30 19:02:12 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/30 19:02:12 INFO BlockManager: BlockManager stopped\n",
            "23/04/30 19:02:12 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/30 19:02:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/30 19:02:12 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/30 19:02:12 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/30 19:02:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc/pyspark-68eb5c2d-aeae-4ab8-b3ad-78a509093c84\n",
            "23/04/30 19:02:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-910119cc-f1c7-4539-81b4-966157ff2d23\n",
            "23/04/30 19:02:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-177b175e-dcd8-49f6-98c0-181ba7214bbc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('localtest2.out/part-00000-053821a9-4ee2-439d-8640-ddc2605a4b05-c000.csv') as f:\n",
        "  lines = f.readlines()\n",
        "  print(lines)"
      ],
      "metadata": {
        "id": "XIzcQg0meuwG",
        "outputId": "94de2dd6-e1db-4675-9e98-3cc72b81829c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['144336\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r localtest2.out"
      ],
      "metadata": {
        "id": "8cVLNIWlevZq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now let's test a basic query on incidents with spark"
      ],
      "metadata": {
        "id": "dd_Xa6EDjhzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file localtest2.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('localtest2').getOrCreate()\n",
        "\n",
        "try:\n",
        "  # read in parquet file\n",
        "  # part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n",
        "  incidents_df = spark.read.parquet('incidents.parquet')\n",
        "  incidents_df.createOrReplaceTempView(\"incidents\")\n",
        "\n",
        "  # run SQL query\n",
        "  counts = spark.sql('SELECT AVG(incidents.latitude), AVG(incidents.longitude) FROM incidents')\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = counts\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"localtest2.out\")\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass"
      ],
      "metadata": {
        "id": "NaDRnhPgjgTM",
        "outputId": "f6bfb7d9-d23b-4700-82ea-8366503e0916",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting localtest2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 localtest2.py"
      ],
      "metadata": {
        "id": "6Viyg67znfmH",
        "outputId": "30ae21a8-f873-4e45-866e-789a7e4e6d1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2ececbd4-d2c7-4511-8a6e-c4ba0a6f61ae;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 873ms :: artifacts dl 55ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-2ececbd4-d2c7-4511-8a6e-c4ba0a6f61ae\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/24ms)\n",
            "23/04/30 19:27:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/30 19:27:52 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/04/30 19:27:52 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 19:27:52 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/30 19:27:52 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 19:27:52 INFO SparkContext: Submitted application: localtest2\n",
            "23/04/30 19:27:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/30 19:27:52 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/30 19:27:52 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/30 19:27:52 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/30 19:27:52 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/30 19:27:52 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/30 19:27:52 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/30 19:27:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/30 19:27:52 INFO Utils: Successfully started service 'sparkDriver' on port 41867.\n",
            "23/04/30 19:27:52 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/30 19:27:53 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/30 19:27:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/30 19:27:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/30 19:27:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/30 19:27:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-83a718a4-7ca2-4aa7-8e4a-c6b69ddf4507\n",
            "23/04/30 19:27:53 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/30 19:27:53 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/30 19:27:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/30 19:27:53 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://145f628fbc4b:4040\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://145f628fbc4b:41867/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://145f628fbc4b:41867/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://145f628fbc4b:41867/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://145f628fbc4b:41867/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://145f628fbc4b:41867/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://145f628fbc4b:41867/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://145f628fbc4b:41867/jars/com.101tec_zkclient-0.3.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://145f628fbc4b:41867/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://145f628fbc4b:41867/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://145f628fbc4b:41867/jars/log4j_log4j-1.2.17.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://145f628fbc4b:41867/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://145f628fbc4b:41867/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/log4j_log4j-1.2.17.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 19:27:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:53 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 19:27:54 INFO Executor: Starting executor ID driver on host 145f628fbc4b\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/log4j_log4j-1.2.17.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO TransportClientFactory: Successfully created connection to 145f628fbc4b/172.28.0.12:41867 after 78 ms (0 ms spent in bootstraps)\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp7141776552641971921.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp7141776552641971921.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp4148912846098409607.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp4148912846098409607.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp1950556471998117991.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp1950556471998117991.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp827791910842979561.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp827791910842979561.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp8628034061007495248.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp8628034061007495248.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp980947152333789592.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp980947152333789592.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/com.101tec_zkclient-0.3.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp9215188553534959149.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp9215188553534959149.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp1892133265186533642.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp1892133265186533642.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp993965001569242677.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp993965001569242677.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp1300540902745574788.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp1300540902745574788.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp584258419599847395.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp584258419599847395.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/30 19:27:54 INFO Executor: Fetching spark://145f628fbc4b:41867/jars/log4j_log4j-1.2.17.jar with timestamp 1682882872164\n",
            "23/04/30 19:27:54 INFO Utils: Fetching spark://145f628fbc4b:41867/jars/log4j_log4j-1.2.17.jar to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp5166325385106947539.tmp\n",
            "23/04/30 19:27:54 INFO Utils: /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/fetchFileTemp5166325385106947539.tmp has been previously copied to /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/log4j_log4j-1.2.17.jar\n",
            "23/04/30 19:27:54 INFO Executor: Adding file:/tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/userFiles-613a5bcf-4760-4de5-924f-ad033969b194/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/30 19:27:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40335.\n",
            "23/04/30 19:27:54 INFO NettyBlockTransferService: Server created on 145f628fbc4b:40335\n",
            "23/04/30 19:27:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/30 19:27:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 145f628fbc4b, 40335, None)\n",
            "23/04/30 19:27:54 INFO BlockManagerMasterEndpoint: Registering block manager 145f628fbc4b:40335 with 366.3 MiB RAM, BlockManagerId(driver, 145f628fbc4b, 40335, None)\n",
            "23/04/30 19:27:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 145f628fbc4b, 40335, None)\n",
            "23/04/30 19:27:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 145f628fbc4b, 40335, None)\n",
            "23/04/30 19:27:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/04/30 19:27:55 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/04/30 19:27:57 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.\n",
            "23/04/30 19:27:58 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 19:27:58 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:27:58 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:27:58 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 19:27:58 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:27:58 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:27:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/04/30 19:27:59 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/04/30 19:27:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 145f628fbc4b:40335 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 19:27:59 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:27:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:27:59 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:27:59 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4579 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:27:59 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/30 19:28:01 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2231 bytes result sent to driver\n",
            "23/04/30 19:28:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2186 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:28:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:28:01 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.787 s\n",
            "23/04/30 19:28:01 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 19:28:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/30 19:28:01 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.997514 s\n",
            "23/04/30 19:28:02 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 145f628fbc4b:40335 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 19:28:05 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/30 19:28:05 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "23/04/30 19:28:05 INFO FileSourceStrategy: Output Data Schema: struct<latitude: double, longitude: double>\n",
            "23/04/30 19:28:06 INFO CodeGenerator: Code generated in 336.123365 ms\n",
            "23/04/30 19:28:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 352.5 KiB, free 366.0 MiB)\n",
            "23/04/30 19:28:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.5 KiB, free 365.9 MiB)\n",
            "23/04/30 19:28:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 145f628fbc4b:40335 (size: 35.5 KiB, free: 366.3 MiB)\n",
            "23/04/30 19:28:06 INFO SparkContext: Created broadcast 1 from csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 19:28:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/30 19:28:06 INFO DAGScheduler: Registering RDD 5 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/30 19:28:06 INFO DAGScheduler: Got map stage job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:28:06 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:28:06 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 19:28:06 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:28:06 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:28:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 20.1 KiB, free 365.9 MiB)\n",
            "23/04/30 19:28:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 365.9 MiB)\n",
            "23/04/30 19:28:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 145f628fbc4b:40335 (size: 8.8 KiB, free: 366.3 MiB)\n",
            "23/04/30 19:28:07 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:28:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:28:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:28:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4847 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:28:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/30 19:28:07 INFO FileScanRDD: Reading File path: file:///content/incidents.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/30 19:28:07 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/04/30 19:28:07 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2167 bytes result sent to driver\n",
            "23/04/30 19:28:07 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 740 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:28:07 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:28:07 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 0.830 s\n",
            "23/04/30 19:28:07 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 19:28:07 INFO DAGScheduler: running: Set()\n",
            "23/04/30 19:28:07 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 19:28:07 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 19:28:07 INFO CodeGenerator: Code generated in 43.53524 ms\n",
            "23/04/30 19:28:07 INFO DAGScheduler: Registering RDD 8 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/30 19:28:07 INFO DAGScheduler: Got map stage job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:28:07 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:28:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/30 19:28:07 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:28:07 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[8] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:28:07 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 365.9 MiB)\n",
            "23/04/30 19:28:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 365.9 MiB)\n",
            "23/04/30 19:28:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 145f628fbc4b:40335 (size: 7.3 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:28:07 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:28:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[8] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:28:07 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:28:07 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:28:07 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
            "23/04/30 19:28:08 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 19:28:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 34 ms\n",
            "23/04/30 19:28:08 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 2839 bytes result sent to driver\n",
            "23/04/30 19:28:08 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 115 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:28:08 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:28:08 INFO DAGScheduler: ShuffleMapStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 0.170 s\n",
            "23/04/30 19:28:08 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 19:28:08 INFO DAGScheduler: running: Set()\n",
            "23/04/30 19:28:08 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 19:28:08 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 19:28:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 19:28:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 19:28:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 19:28:08 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 19:28:08 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:28:08 INFO DAGScheduler: Final stage: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:28:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
            "23/04/30 19:28:08 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:28:08 INFO DAGScheduler: Submitting ResultStage 6 (ShuffledRowRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:28:08 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)\n",
            "23/04/30 19:28:08 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 75.5 KiB, free 365.6 MiB)\n",
            "23/04/30 19:28:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 145f628fbc4b:40335 (size: 75.5 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:28:08 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:28:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (ShuffledRowRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:28:08 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:28:08 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 3) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:28:08 INFO Executor: Running task 0.0 in stage 6.0 (TID 3)\n",
            "23/04/30 19:28:08 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 19:28:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "23/04/30 19:28:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 19:28:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 19:28:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 19:28:08 INFO FileOutputCommitter: Saved output of task 'attempt_202304301928086813574559748560989_0006_m_000000_3' to file:/content/localtest2.out/_temporary/0/task_202304301928086813574559748560989_0006_m_000000\n",
            "23/04/30 19:28:08 INFO SparkHadoopMapRedUtil: attempt_202304301928086813574559748560989_0006_m_000000_3: Committed\n",
            "23/04/30 19:28:08 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 145f628fbc4b:40335 in memory (size: 8.8 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:28:08 INFO Executor: Finished task 0.0 in stage 6.0 (TID 3). 3526 bytes result sent to driver\n",
            "23/04/30 19:28:08 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 3) in 319 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:28:08 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:28:08 INFO DAGScheduler: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0) finished in 0.364 s\n",
            "23/04/30 19:28:08 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 19:28:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "23/04/30 19:28:08 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 0.382930 s\n",
            "23/04/30 19:28:08 INFO FileFormatWriter: Start to commit write Job b598bb27-ce27-47b4-9a67-20e7977b1788.\n",
            "23/04/30 19:28:08 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 145f628fbc4b:40335 in memory (size: 7.3 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:28:08 INFO FileFormatWriter: Write Job b598bb27-ce27-47b4-9a67-20e7977b1788 committed. Elapsed time: 27 ms.\n",
            "23/04/30 19:28:08 INFO FileFormatWriter: Finished processing stats for write job b598bb27-ce27-47b4-9a67-20e7977b1788.\n",
            "23/04/30 19:28:08 INFO SparkUI: Stopped Spark web UI at http://145f628fbc4b:4040\n",
            "23/04/30 19:28:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/30 19:28:08 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/30 19:28:08 INFO BlockManager: BlockManager stopped\n",
            "23/04/30 19:28:08 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/30 19:28:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/30 19:28:08 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/30 19:28:08 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/30 19:28:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2\n",
            "23/04/30 19:28:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-6235c06d-b91a-47fc-9741-ef3df38ff3a2/pyspark-cd5ee225-d03c-4b41-bf5a-ce528258209c\n",
            "23/04/30 19:28:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-15ca1184-f4d2-4383-9cc5-adc8e07f6b43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/localtest2.out/part-00000-fd44653f-34a7-4bd8-837e-c374b301f17e-c000.csv') as f:\n",
        "  lines = f.readlines()\n",
        "  print(lines)"
      ],
      "metadata": {
        "id": "M8eqcQAKnIRu",
        "outputId": "0041aa9b-7b05-4b00-a52a-b843dddbaae8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['36.151854061679494,-86.74292086389275\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r localtest2.out"
      ],
      "metadata": {
        "id": "96oBkNtNoHSa"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now we know that both of our datasets can interact with Spark\n",
        "\n",
        "### Let's now try to extract the hour from the incident data to make sure that we can do it in Spark"
      ],
      "metadata": {
        "id": "PqFst6cAnwmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file localtest2.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "#create spark session\n",
        "spark = SparkSession.builder.appName('localtest2').getOrCreate()\n",
        "\n",
        "try:\n",
        "  # read in parquet file\n",
        "  # part-00030-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\n",
        "  incidents_df = spark.read.parquet('incidents.parquet')\n",
        "  incidents_df.createOrReplaceTempView(\"incidents\")\n",
        "\n",
        "  # run SQL query\n",
        "  # OLD QUERY: 'SELECT HOUR(FROM_UNIXTIME(time_utc)), COUNT(incident_id) FROM \"incidents\" GROUP BY HOUR(FROM_UNIXTIME(time_utc)) ORDER BY COUNT(incident_id) DESC;'\n",
        "  counts = spark.sql('SELECT HOUR(time_local), COUNT(incident_id) FROM incidents GROUP BY HOUR(time_local) ORDER BY COUNT(incident_id) DESC;')\n",
        "\n",
        "  # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "  output = counts\n",
        "\n",
        "  # @todo: the s3 version will have to save it to correct s3 path\n",
        "  output.repartition(1).write.mode('overwrite').csv(\"localtest2.out\")\n",
        "    \n",
        "\n",
        "finally:\n",
        "  # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "  #finally is used to make sure the context is stopped even with errors\n",
        "  spark.stop()\n",
        "pass"
      ],
      "metadata": {
        "id": "prZo9ownoB3J",
        "outputId": "ce9c2b7e-c469-4c70-bd1e-078476729bfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting localtest2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 localtest2.py"
      ],
      "metadata": {
        "id": "F0tWfj7Hqrx8",
        "outputId": "885d7bbd-569a-4051-ff27-e5cba4a5a9c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ec1abbda-08cb-402e-b5d3-2404c0413a16;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 1366ms :: artifacts dl 75ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-ec1abbda-08cb-402e-b5d3-2404c0413a16\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/39ms)\n",
            "23/04/30 19:51:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/30 19:51:55 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/04/30 19:51:55 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 19:51:55 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/30 19:51:55 INFO ResourceUtils: ==============================================================\n",
            "23/04/30 19:51:55 INFO SparkContext: Submitted application: localtest2\n",
            "23/04/30 19:51:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/30 19:51:55 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/30 19:51:55 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/30 19:51:55 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/30 19:51:55 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/30 19:51:55 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/30 19:51:55 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/30 19:51:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/30 19:51:56 INFO Utils: Successfully started service 'sparkDriver' on port 32779.\n",
            "23/04/30 19:51:56 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/30 19:51:56 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/30 19:51:56 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/30 19:51:56 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/30 19:51:56 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/30 19:51:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a923b2b9-8043-454b-9cf5-8cf153d5bd49\n",
            "23/04/30 19:51:56 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/30 19:51:56 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/30 19:51:56 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/30 19:51:56 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://145f628fbc4b:4040\n",
            "23/04/30 19:51:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://145f628fbc4b:32779/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://145f628fbc4b:32779/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://145f628fbc4b:32779/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://145f628fbc4b:32779/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://145f628fbc4b:32779/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://145f628fbc4b:32779/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://145f628fbc4b:32779/jars/com.101tec_zkclient-0.3.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://145f628fbc4b:32779/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://145f628fbc4b:32779/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://145f628fbc4b:32779/jars/log4j_log4j-1.2.17.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://145f628fbc4b:32779/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://145f628fbc4b:32779/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 19:51:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 19:51:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 19:51:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 19:51:57 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 19:51:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 19:51:57 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 19:51:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 19:51:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 19:51:57 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/log4j_log4j-1.2.17.jar\n",
            "23/04/30 19:51:57 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 19:51:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 19:51:57 INFO Executor: Starting executor ID driver on host 145f628fbc4b\n",
            "23/04/30 19:51:57 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/log4j_log4j-1.2.17.jar\n",
            "23/04/30 19:51:57 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 19:51:57 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 19:51:57 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 19:51:57 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 19:51:57 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 19:51:57 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 19:51:57 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 19:51:57 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 19:51:57 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 19:51:57 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 19:51:57 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 19:51:57 INFO Executor: Fetching spark://145f628fbc4b:32779/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO TransportClientFactory: Successfully created connection to 145f628fbc4b/172.28.0.12:32779 after 40 ms (0 ms spent in bootstraps)\n",
            "23/04/30 19:51:57 INFO Utils: Fetching spark://145f628fbc4b:32779/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp5062425502437160783.tmp\n",
            "23/04/30 19:51:57 INFO Utils: /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp5062425502437160783.tmp has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/30 19:51:57 INFO Executor: Adding file:/tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/30 19:51:57 INFO Executor: Fetching spark://145f628fbc4b:32779/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Fetching spark://145f628fbc4b:32779/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp8153290382339953047.tmp\n",
            "23/04/30 19:51:57 INFO Utils: /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp8153290382339953047.tmp has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/30 19:51:57 INFO Executor: Adding file:/tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/30 19:51:57 INFO Executor: Fetching spark://145f628fbc4b:32779/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Fetching spark://145f628fbc4b:32779/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp5622055507494859927.tmp\n",
            "23/04/30 19:51:57 INFO Utils: /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp5622055507494859927.tmp has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/30 19:51:57 INFO Executor: Adding file:/tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/30 19:51:57 INFO Executor: Fetching spark://145f628fbc4b:32779/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Fetching spark://145f628fbc4b:32779/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp1574500739412798270.tmp\n",
            "23/04/30 19:51:57 INFO Utils: /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp1574500739412798270.tmp has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/30 19:51:57 INFO Executor: Adding file:/tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/30 19:51:57 INFO Executor: Fetching spark://145f628fbc4b:32779/jars/log4j_log4j-1.2.17.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Fetching spark://145f628fbc4b:32779/jars/log4j_log4j-1.2.17.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp1517770919684166944.tmp\n",
            "23/04/30 19:51:57 INFO Utils: /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp1517770919684166944.tmp has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/log4j_log4j-1.2.17.jar\n",
            "23/04/30 19:51:57 INFO Executor: Adding file:/tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/30 19:51:57 INFO Executor: Fetching spark://145f628fbc4b:32779/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Fetching spark://145f628fbc4b:32779/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp7965072675497052558.tmp\n",
            "23/04/30 19:51:57 INFO Utils: /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp7965072675497052558.tmp has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/30 19:51:57 INFO Executor: Adding file:/tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/30 19:51:57 INFO Executor: Fetching spark://145f628fbc4b:32779/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Fetching spark://145f628fbc4b:32779/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp3216989853213363319.tmp\n",
            "23/04/30 19:51:57 INFO Utils: /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp3216989853213363319.tmp has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/30 19:51:57 INFO Executor: Adding file:/tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/30 19:51:57 INFO Executor: Fetching spark://145f628fbc4b:32779/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Fetching spark://145f628fbc4b:32779/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp5462332403532578842.tmp\n",
            "23/04/30 19:51:57 INFO Utils: /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp5462332403532578842.tmp has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/30 19:51:57 INFO Executor: Adding file:/tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/30 19:51:57 INFO Executor: Fetching spark://145f628fbc4b:32779/jars/com.101tec_zkclient-0.3.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Fetching spark://145f628fbc4b:32779/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp6215052055486720949.tmp\n",
            "23/04/30 19:51:57 INFO Utils: /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp6215052055486720949.tmp has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/com.101tec_zkclient-0.3.jar\n",
            "23/04/30 19:51:57 INFO Executor: Adding file:/tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/30 19:51:57 INFO Executor: Fetching spark://145f628fbc4b:32779/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Fetching spark://145f628fbc4b:32779/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp4467743408880913800.tmp\n",
            "23/04/30 19:51:57 INFO Utils: /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp4467743408880913800.tmp has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/30 19:51:57 INFO Executor: Adding file:/tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/30 19:51:57 INFO Executor: Fetching spark://145f628fbc4b:32779/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Fetching spark://145f628fbc4b:32779/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp8099173832072516354.tmp\n",
            "23/04/30 19:51:57 INFO Utils: /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp8099173832072516354.tmp has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/30 19:51:57 INFO Executor: Adding file:/tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/30 19:51:57 INFO Executor: Fetching spark://145f628fbc4b:32779/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682884315441\n",
            "23/04/30 19:51:57 INFO Utils: Fetching spark://145f628fbc4b:32779/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp6889605975375202581.tmp\n",
            "23/04/30 19:51:57 INFO Utils: /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/fetchFileTemp6889605975375202581.tmp has been previously copied to /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/30 19:51:57 INFO Executor: Adding file:/tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/userFiles-14243949-9c16-4266-b10f-52cd92cc6fa0/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/30 19:51:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39601.\n",
            "23/04/30 19:51:57 INFO NettyBlockTransferService: Server created on 145f628fbc4b:39601\n",
            "23/04/30 19:51:57 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/30 19:51:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 145f628fbc4b, 39601, None)\n",
            "23/04/30 19:51:57 INFO BlockManagerMasterEndpoint: Registering block manager 145f628fbc4b:39601 with 366.3 MiB RAM, BlockManagerId(driver, 145f628fbc4b, 39601, None)\n",
            "23/04/30 19:51:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 145f628fbc4b, 39601, None)\n",
            "23/04/30 19:51:57 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 145f628fbc4b, 39601, None)\n",
            "23/04/30 19:51:58 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/04/30 19:51:58 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/04/30 19:51:59 INFO InMemoryFileIndex: It took 59 ms to list leaf files for 1 paths.\n",
            "23/04/30 19:52:00 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 19:52:00 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:52:00 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:52:00 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 19:52:00 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:52:00 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:52:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/04/30 19:52:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/04/30 19:52:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 145f628fbc4b:39601 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 19:52:00 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:52:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:52:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:52:01 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4579 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:52:01 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/30 19:52:02 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2274 bytes result sent to driver\n",
            "23/04/30 19:52:02 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1144 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:52:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:52:02 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.540 s\n",
            "23/04/30 19:52:02 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 19:52:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/30 19:52:02 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.647966 s\n",
            "23/04/30 19:52:03 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 145f628fbc4b:39601 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 19:52:06 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/30 19:52:07 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "23/04/30 19:52:07 INFO FileSourceStrategy: Output Data Schema: struct<time_local: timestamp, Incident_ID: int>\n",
            "23/04/30 19:52:08 INFO CodeGenerator: Code generated in 795.108155 ms\n",
            "23/04/30 19:52:08 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 352.5 KiB, free 366.0 MiB)\n",
            "23/04/30 19:52:08 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 365.9 MiB)\n",
            "23/04/30 19:52:08 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 145f628fbc4b:39601 (size: 35.6 KiB, free: 366.3 MiB)\n",
            "23/04/30 19:52:08 INFO SparkContext: Created broadcast 1 from csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 19:52:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/30 19:52:09 INFO DAGScheduler: Registering RDD 5 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/30 19:52:09 INFO DAGScheduler: Got map stage job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:52:09 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:52:09 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/30 19:52:09 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:52:09 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:52:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 37.4 KiB, free 365.9 MiB)\n",
            "23/04/30 19:52:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 365.9 MiB)\n",
            "23/04/30 19:52:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 145f628fbc4b:39601 (size: 17.1 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:52:09 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:52:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:52:09 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:52:09 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (145f628fbc4b, executor driver, partition 0, PROCESS_LOCAL, 4847 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:52:09 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/30 19:52:09 INFO CodeGenerator: Code generated in 49.454076 ms\n",
            "23/04/30 19:52:09 INFO CodeGenerator: Code generated in 19.328018 ms\n",
            "23/04/30 19:52:09 INFO CodeGenerator: Code generated in 28.528279 ms\n",
            "23/04/30 19:52:09 INFO CodeGenerator: Code generated in 29.549774 ms\n",
            "23/04/30 19:52:09 INFO FileScanRDD: Reading File path: file:///content/incidents.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/30 19:52:09 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/04/30 19:52:10 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 3013 bytes result sent to driver\n",
            "23/04/30 19:52:10 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1393 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:52:10 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:52:10 INFO DAGScheduler: ShuffleMapStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 1.436 s\n",
            "23/04/30 19:52:10 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 19:52:10 INFO DAGScheduler: running: Set()\n",
            "23/04/30 19:52:10 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 19:52:10 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 19:52:10 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/30 19:52:10 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/30 19:52:10 INFO CodeGenerator: Code generated in 45.664999 ms\n",
            "23/04/30 19:52:10 INFO CodeGenerator: Code generated in 35.936017 ms\n",
            "23/04/30 19:52:11 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:52:11 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 36.8 KiB, free 365.8 MiB)\n",
            "23/04/30 19:52:11 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 365.8 MiB)\n",
            "23/04/30 19:52:11 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 145f628fbc4b:39601 (size: 17.4 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:52:11 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:52:11 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:52:11 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:52:11 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
            "23/04/30 19:52:11 INFO ShuffleBlockFetcherIterator: Getting 1 (1474.0 B) non-empty blocks including 1 (1474.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 19:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms\n",
            "23/04/30 19:52:11 INFO CodeGenerator: Code generated in 10.04482 ms\n",
            "23/04/30 19:52:11 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 4979 bytes result sent to driver\n",
            "23/04/30 19:52:11 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 152 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:52:11 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:52:11 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 0.179 s\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 19:52:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.214612 s\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Registering RDD 11 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Got map stage job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[11] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:52:11 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 37.7 KiB, free 365.8 MiB)\n",
            "23/04/30 19:52:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 17.7 KiB, free 365.8 MiB)\n",
            "23/04/30 19:52:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 145f628fbc4b:39601 (size: 17.7 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:52:11 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[11] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:52:11 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:52:11 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:52:11 INFO Executor: Running task 0.0 in stage 5.0 (TID 3)\n",
            "23/04/30 19:52:11 INFO ShuffleBlockFetcherIterator: Getting 1 (1474.0 B) non-empty blocks including 1 (1474.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 19:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 19:52:11 INFO Executor: Finished task 0.0 in stage 5.0 (TID 3). 4199 bytes result sent to driver\n",
            "23/04/30 19:52:11 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 96 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:52:11 INFO DAGScheduler: ShuffleMapStage 5 (csv at NativeMethodAccessorImpl.java:0) finished in 0.128 s\n",
            "23/04/30 19:52:11 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 19:52:11 INFO DAGScheduler: running: Set()\n",
            "23/04/30 19:52:11 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 19:52:11 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 19:52:11 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:52:11 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/30 19:52:11 INFO CodeGenerator: Code generated in 18.281593 ms\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Registering RDD 14 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Got map stage job 4 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[14] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:52:11 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 35.3 KiB, free 365.7 MiB)\n",
            "23/04/30 19:52:11 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 365.7 MiB)\n",
            "23/04/30 19:52:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 145f628fbc4b:39601 (size: 16.9 KiB, free: 366.2 MiB)\n",
            "23/04/30 19:52:11 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[14] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:52:11 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:52:11 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 4) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:52:11 INFO Executor: Running task 0.0 in stage 8.0 (TID 4)\n",
            "23/04/30 19:52:11 INFO ShuffleBlockFetcherIterator: Getting 1 (1578.0 B) non-empty blocks including 1 (1578.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 19:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 19:52:11 INFO CodeGenerator: Code generated in 16.108451 ms\n",
            "23/04/30 19:52:11 INFO CodeGenerator: Code generated in 21.837569 ms\n",
            "23/04/30 19:52:11 INFO Executor: Finished task 0.0 in stage 8.0 (TID 4). 5085 bytes result sent to driver\n",
            "23/04/30 19:52:11 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 4) in 125 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:52:11 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:52:11 INFO DAGScheduler: ShuffleMapStage 8 (csv at NativeMethodAccessorImpl.java:0) finished in 0.146 s\n",
            "23/04/30 19:52:11 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/30 19:52:11 INFO DAGScheduler: running: Set()\n",
            "23/04/30 19:52:11 INFO DAGScheduler: waiting: Set()\n",
            "23/04/30 19:52:11 INFO DAGScheduler: failed: Set()\n",
            "23/04/30 19:52:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 19:52:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 19:52:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 19:52:11 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Got job 5 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Final stage: ResultStage 12 (csv at NativeMethodAccessorImpl.java:0)\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Submitting ResultStage 12 (ShuffledRowRDD[15] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/30 19:52:11 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 211.3 KiB, free 365.5 MiB)\n",
            "23/04/30 19:52:11 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 75.6 KiB, free 365.4 MiB)\n",
            "23/04/30 19:52:11 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 145f628fbc4b:39601 (size: 75.6 KiB, free: 366.1 MiB)\n",
            "23/04/30 19:52:11 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (ShuffledRowRDD[15] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/30 19:52:11 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
            "23/04/30 19:52:11 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 5) (145f628fbc4b, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/30 19:52:11 INFO Executor: Running task 0.0 in stage 12.0 (TID 5)\n",
            "23/04/30 19:52:11 INFO ShuffleBlockFetcherIterator: Getting 1 (368.0 B) non-empty blocks including 1 (368.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/30 19:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/30 19:52:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/30 19:52:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/30 19:52:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "23/04/30 19:52:11 INFO FileOutputCommitter: Saved output of task 'attempt_20230430195211134888678967660593_0012_m_000000_5' to file:/content/localtest2.out/_temporary/0/task_20230430195211134888678967660593_0012_m_000000\n",
            "23/04/30 19:52:11 INFO SparkHadoopMapRedUtil: attempt_20230430195211134888678967660593_0012_m_000000_5: Committed\n",
            "23/04/30 19:52:11 INFO Executor: Finished task 0.0 in stage 12.0 (TID 5). 3483 bytes result sent to driver\n",
            "23/04/30 19:52:11 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 5) in 151 ms on 145f628fbc4b (executor driver) (1/1)\n",
            "23/04/30 19:52:11 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
            "23/04/30 19:52:11 INFO DAGScheduler: ResultStage 12 (csv at NativeMethodAccessorImpl.java:0) finished in 0.201 s\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/30 19:52:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
            "23/04/30 19:52:11 INFO DAGScheduler: Job 5 finished: csv at NativeMethodAccessorImpl.java:0, took 0.223198 s\n",
            "23/04/30 19:52:11 INFO FileFormatWriter: Start to commit write Job 841c7eec-0722-4e8d-aad5-cdf4737ec400.\n",
            "23/04/30 19:52:12 INFO FileFormatWriter: Write Job 841c7eec-0722-4e8d-aad5-cdf4737ec400 committed. Elapsed time: 26 ms.\n",
            "23/04/30 19:52:12 INFO FileFormatWriter: Finished processing stats for write job 841c7eec-0722-4e8d-aad5-cdf4737ec400.\n",
            "23/04/30 19:52:12 INFO SparkUI: Stopped Spark web UI at http://145f628fbc4b:4040\n",
            "23/04/30 19:52:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/30 19:52:12 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/30 19:52:12 INFO BlockManager: BlockManager stopped\n",
            "23/04/30 19:52:12 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/30 19:52:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/30 19:52:12 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/30 19:52:12 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/30 19:52:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d\n",
            "23/04/30 19:52:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-9ecac7ac-c77f-487c-bcdf-b8c5a3ad490d/pyspark-51e7450c-9a28-4488-a991-0f230ad09301\n",
            "23/04/30 19:52:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-97893754-57ae-4d75-9600-e2b0070c69da\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/localtest2.out/part-00000-459606a9-8928-48b9-a4ef-115272ffd60d-c000.csv') as f:\n",
        "  lines = f.readlines()\n",
        "  print(lines)"
      ],
      "metadata": {
        "id": "Dbhvt5LpsFaa",
        "outputId": "66cee304-cf85-4e01-bd77-e4cf85756ccf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['17,2303\\n', '16,2081\\n', '15,1984\\n', '18,1874\\n', '14,1670\\n', '13,1613\\n', '12,1609\\n', '19,1554\\n', '11,1403\\n', '20,1307\\n', '10,1231\\n', '21,1197\\n', '7,1190\\n', '8,1163\\n', '9,1105\\n', '22,1032\\n', '6,931\\n', '23,878\\n', '0,661\\n', '5,659\\n', '2,638\\n', '1,631\\n', '3,582\\n', '4,469\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set(style='ticks')\n",
        "time_hist = pd.read_csv('/content/localtest2.out/part-00000-378dc295-0745-4e27-a568-11b57731c299-c000.csv', header=None, names=['_col0', '_col1'])\n",
        "time_hist"
      ],
      "metadata": {
        "id": "ECZZVI64sRH2",
        "outputId": "cc9dffb4-5359-4b6b-b487-5c0f914206c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    _col0  _col1\n",
              "0      17   2303\n",
              "1      16   2081\n",
              "2      15   1984\n",
              "3      18   1874\n",
              "4      14   1670\n",
              "5      13   1613\n",
              "6      12   1609\n",
              "7      19   1554\n",
              "8      11   1403\n",
              "9      20   1307\n",
              "10     10   1231\n",
              "11     21   1197\n",
              "12      7   1190\n",
              "13      8   1163\n",
              "14      9   1105\n",
              "15     22   1032\n",
              "16      6    931\n",
              "17     23    878\n",
              "18      0    661\n",
              "19      5    659\n",
              "20      2    638\n",
              "21      1    631\n",
              "22      3    582\n",
              "23      4    469"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-90400269-d6af-4ba2-b366-39162ee6bcd6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_col0</th>\n",
              "      <th>_col1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17</td>\n",
              "      <td>2303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16</td>\n",
              "      <td>2081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>15</td>\n",
              "      <td>1984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18</td>\n",
              "      <td>1874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14</td>\n",
              "      <td>1670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>13</td>\n",
              "      <td>1613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>12</td>\n",
              "      <td>1609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>19</td>\n",
              "      <td>1554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>11</td>\n",
              "      <td>1403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>20</td>\n",
              "      <td>1307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>1231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>21</td>\n",
              "      <td>1197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>7</td>\n",
              "      <td>1190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>8</td>\n",
              "      <td>1163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>9</td>\n",
              "      <td>1105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>22</td>\n",
              "      <td>1032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6</td>\n",
              "      <td>931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>23</td>\n",
              "      <td>878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>5</td>\n",
              "      <td>659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2</td>\n",
              "      <td>638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1</td>\n",
              "      <td>631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3</td>\n",
              "      <td>582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>4</td>\n",
              "      <td>469</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90400269-d6af-4ba2-b366-39162ee6bcd6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-90400269-d6af-4ba2-b366-39162ee6bcd6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-90400269-d6af-4ba2-b366-39162ee6bcd6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "# the size of A4 paper\n",
        "fig.set_size_inches(11.7, 8.27)\n",
        "sns.barplot(data=time_hist, x=\"_col0\", y=\"_col1\", order=list(range(0, 24))).set(\n",
        "    title='Distribution of Incidents by Hour of Day', xlabel='Hour of Day', ylabel='Total Count')"
      ],
      "metadata": {
        "id": "tFW5Jivfsqdv",
        "outputId": "13eba597-0d98-4ca2-be04-84036a1158df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0.5, 1.0, 'Distribution of Incidents by Hour of Day'),\n",
              " Text(0.5, 0, 'Hour of Day'),\n",
              " Text(0, 0.5, 'Total Count')]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1170x827 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Great, we can clearly extract the hours from each...\n",
        "\n",
        "Now, we're going to need to extract ONLY ONE element of the weather from a given bucket... let's see if we can grab the first weather measurement from January 2021, since that should be easier and we'll know we did it correctly if we get a single result"
      ],
      "metadata": {
        "id": "iVpkGFUVu6py"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fv7GH9sDuXv0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}